<!doctype html>
<!--
  Minimal Mistakes Jekyll Theme 4.15.1 by Michael Rose
  Copyright 2013-2019 Michael Rose - mademistakes.com | @mmistakes
  Free for personal and commercial use under the MIT license
  https://github.com/mmistakes/minimal-mistakes/blob/master/LICENSE
-->
<html lang="en" class="no-js">
  <head>
    <meta charset="utf-8">

<!-- begin _includes/seo.html --><title>Google Machine Learning Crash Course | Aditya Jaishankar</title>
<meta name="description" content="Definitions and Framing">



<meta property="og:type" content="article">
<meta property="og:locale" content="en_US">
<meta property="og:site_name" content="Aditya Jaishankar">
<meta property="og:title" content="Google Machine Learning Crash Course">
<meta property="og:url" content="http://localhost:4000/coursenotes/googlemachinelearning/">


  <meta property="og:description" content="Definitions and Framing">







  <meta property="article:published_time" content="2019-12-22T12:17:42-05:00">





  

  


<link rel="canonical" href="http://localhost:4000/coursenotes/googlemachinelearning/">







  <script type="application/ld+json">
    {
      "@context": "https://schema.org",
      "@type": "Person",
      "name": "Aditya Jaishankar",
      "url": "http://localhost:4000",
      "sameAs": null
    }
  </script>







<!-- end _includes/seo.html -->


<link href="/feed.xml" type="application/atom+xml" rel="alternate" title="Aditya Jaishankar Feed">

<!-- https://t.co/dKP3o1e -->
<meta name="viewport" content="width=device-width, initial-scale=1.0">

<script>
  document.documentElement.className = document.documentElement.className.replace(/\bno-js\b/g, '') + ' js ';
</script>

<!-- For all browsers -->
<link rel="stylesheet" href="/assets/css/main.css">

<!--[if IE ]>
  <style>
    /* old IE unsupported flexbox fixes */
    .greedy-nav .site-title {
      padding-right: 3em;
    }
    .greedy-nav button {
      position: absolute;
      top: 0;
      right: 0;
      height: 100%;
    }
  </style>
<![endif]-->



    <!-- start custom head snippets -->

<!-- insert favicons. use https://realfavicongenerator.net/ -->







<link href="https://fonts.googleapis.com/css?family=Crimson+Text" rel="stylesheet">





<!-- end custom head snippets -->

  </head>

  <body class="layout--single">

    <!--[if lt IE 9]>
<div class="notice--danger align-center" style="margin: 0;">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience.</div>
<![endif]-->

    

<div class="masthead">
  <div class="masthead__inner-wrap">
    <div class="masthead__menu">
      <nav id="site-nav" class="greedy-nav">
        
        <a class="site-title" href="/"> </a>
        <ul class="visible-links"><li class="masthead__menu-item">
              <a href="/" >Home</a>
            </li><li class="masthead__menu-item">
              <a href="/coursework/" >ML/AI Coursework</a>
            </li><li class="masthead__menu-item">
              <a href="/projects/" >Projects</a>
            </li><li class="masthead__menu-item">
              <a href="/research/" >Ph.D. Research</a>
            </li><li class="masthead__menu-item">
              <a href="/publications/" >Publications</a>
            </li></ul>
        
        <button class="search__toggle" type="button">
          <span class="visually-hidden">Toggle search</span>
          <svg class="icon" width="16" height="16" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 15.99 16">
            <path d="M15.5,13.12L13.19,10.8a1.69,1.69,0,0,0-1.28-.55l-0.06-.06A6.5,6.5,0,0,0,5.77,0,6.5,6.5,0,0,0,2.46,11.59a6.47,6.47,0,0,0,7.74.26l0.05,0.05a1.65,1.65,0,0,0,.5,1.24l2.38,2.38A1.68,1.68,0,0,0,15.5,13.12ZM6.4,2A4.41,4.41,0,1,1,2,6.4,4.43,4.43,0,0,1,6.4,2Z" transform="translate(-.01)"></path>
          </svg>
        </button>
        
        <button class="greedy-nav__toggle hidden" type="button">
          <span class="visually-hidden">Toggle menu</span>
          <div class="navicon"></div>
        </button>
        <ul class="hidden-links hidden"></ul>
      </nav>
    </div>
  </div>
</div>


    <div class="initial-content">
      


  
    



<nav class="breadcrumbs">
  <ol itemscope itemtype="https://schema.org/BreadcrumbList">
    
    
    
      
        <li itemprop="itemListElement" itemscope itemtype="https://schema.org/ListItem">
          <a href="http://localhost:4000/" itemprop="item"><span itemprop="name">Home</span></a>
          <meta itemprop="position" content="1" />
        </li>
        <span class="sep">/</span>
      
      
        
        <li itemprop="itemListElement" itemscope itemtype="https://schema.org/ListItem">
          <a href="/coursenotes" itemprop="item"><span itemprop="name">Coursenotes</span></a>
          <meta itemprop="position" content="2" />
        </li>
        <span class="sep">/</span>
      
    
      
      
        <li class="current">Google Machine Learning Crash Course</li>
      
    
  </ol>
</nav>

  


<div id="main" role="main">
  
  <div class="sidebar sticky">
  


<div itemscope itemtype="https://schema.org/Person">

  
    <div class="author__avatar">
      

      
        <img src="/assets/images/profile.jpg" alt="Aditya Jaishankar" itemprop="image">
      
    </div>
  

  <div class="author__content">
    
      <h3 class="author__name" itemprop="name">Aditya Jaishankar</h3>
    
    
      <p class="author__bio" itemprop="description">
        <b>Ph.D., Massachusetts Institute of Technology.</b><br>Soft Condensed Matter Physicist and Surface Scientist.
      </p>
    
  </div>

  <div class="author__urls-wrapper">
    <button class="btn btn--inverse">Follow</button>
    <ul class="author__urls social-icons">
      
        <li itemprop="homeLocation" itemscope itemtype="https://schema.org/Place">
          <i class="fas fa-fw fa-map-marker-alt" aria-hidden="true"></i> <span itemprop="name">New Jersey, USA</span>
        </li>
      

      
        
          
        
          
            <li><a href="https://www.github.com/aditya-jaishankar/" rel="nofollow noopener noreferrer"><i class="fab fa-fw fa-github" aria-hidden="true"></i> GitHub</a></li>
          
        
          
            <li><a href="https://www.linkedin.com/in/adityajaishankar" rel="nofollow noopener noreferrer"><i class="fab fa-linkedin" aria-hidden="true"></i> LinkedIn</a></li>
          
        
          
            <li><a href="/assets/misc/Resume_Jaishankar_191220.pdf" rel="nofollow noopener noreferrer"><i class="fas fa-file-alt" aria-hidden="true"></i> Resume</a></li>
          
        
      

      

      
        <li>
          <a href="mailto:aditya1642@gmail.com">
            <meta itemprop="email" content="aditya1642@gmail.com" />
            <i class="fas fa-fw fa-envelope-square" aria-hidden="true"></i> Email
          </a>
        </li>
      

      

      

      

      

      

      

      

      

      

      

      

      

      

      

      

      

      

      

      

      

      

      

      

      <!--
  <li>
    <a href="http://link-to-whatever-social-network.com/user/" itemprop="sameAs" rel="nofollow noopener noreferrer">
      <i class="fas fa-fw" aria-hidden="true"></i> Custom Social Profile Link
    </a>
  </li>
-->
    </ul>
  </div>
</div>

  
  </div>


  <article class="page" itemscope itemtype="https://schema.org/CreativeWork">
    <meta itemprop="headline" content="Google Machine Learning Crash Course">
    <meta itemprop="description" content="Definitions and Framing">
    
    

    <div class="page__inner-wrap">
      
        <header>
          <h1 id="page-title" class="page__title" itemprop="headline">Google Machine Learning Crash Course
</h1>
          
            <p class="page__meta"><i class="far fa-clock" aria-hidden="true"></i> 




  20 minute read
</p>
          
        </header>
      

      <section class="page__content" itemprop="text">
        
          <aside class="sidebar__right ">
            <nav class="toc">
              <header><h4 class="nav__title"><i class="fas fa-file-alt"></i> On this page</h4></header>
              <ul class="toc__menu">
  <li><a href="#definitions-and-framing">Definitions and Framing</a></li>
  <li><a href="#descending-into-ml">Descending into ML</a></li>
  <li><a href="#reducing-loss">Reducing Loss</a></li>
  <li><a href="#first-steps-with-tensorflow">First steps with TensorFlow</a>
    <ul>
      <li><a href="#tfestimator-api">tf.estimator API</a></li>
      <li><a href="#step-0-setup-imports-loading-and-inspecting-the-data">Step 0: Setup, imports, loading and inspecting the data</a></li>
      <li><a href="#step-1-defining-the-features">Step 1: Defining the features</a></li>
      <li><a href="#step-2-defining-the-target">Step 2: Defining the target</a></li>
      <li><a href="#step-3-configure-the-linearregressor">Step 3: Configure the LinearRegressor</a></li>
      <li><a href="#step-4-define-the-input-function">Step 4: Define the input function</a></li>
      <li><a href="#step-5-train-the-model">Step 5: Train the model</a></li>
      <li><a href="#evaluate-the-model">Evaluate the model</a></li>
      <li><a href="#aside-synthetics-features-and-outliers">Aside: Synthetics features and outliers</a>
        <ul>
          <li><a href="#task-1-creating-my-own-synthetic-feature">Task 1: Creating my own synthetic feature</a></li>
          <li><a href="#task-2-identify-outliers">Task 2: Identify outliers</a></li>
        </ul>
      </li>
    </ul>
  </li>
</ul>
            </nav>
          </aside>
        
        <h2 id="definitions-and-framing">Definitions and Framing</h2>

<ul>
  <li>
    <p><strong>Supervised Machine Learning:</strong> 
  Using known data to generate some useful predictions of on unseen data</p>

    <p>Label $y$: The target variable that we are trying to predict, for example ‘spam’ or ‘not spam’.</p>

    <p>Features: Something about the data that is used to represent the data, that is later fed into a model. Complicated machine learning models can have multiple features $x_1, x_2, \ldots, x_n$. So this is a vector $\mathbf{x}$.</p>

    <p>Model: Maps unseen data to predictions of label $y$. It is defined by internal parameters that are learned using a training set of labeled data</p>

    <p>Labeled data can be represented as $(\mathbf{x}, y)$. In unlabeled data, $y$ is not known and is predicted by the model.</p>
  </li>
  <li>
    <p><strong>Models</strong></p>

    <p>A model defines the relationship between the features and the label. There are two key phases in the life of a model: <br /></p>

    <p>Training: The phase where you the model is trained or learned. You show the model a number of examples of labeled data, and allow the model to learn the relationship between the features and the label. In other words, it is learning the values of the parameters in the model. These parameters in the model are often called hyperparameters.</p>

    <p>Inference: The phase where the model is used to generate labels $y’$ given features $\mathbf{x}$.</p>
  </li>
  <li>
    <p><strong>Regression vs. Classification</strong></p>

    <p>A regression predicts continuous values while a classification predicts discrete values.</p>
  </li>
</ul>

<h2 id="descending-into-ml">Descending into ML</h2>

<p>Topics covered: Linear Regression, Training and Loss</p>

<ul>
  <li>The $L_2$ loss is defined as</li>
</ul>

<script type="math/tex; mode=display">\begin{align}
L_2 \textrm{ Loss } = \sum\limits_i (y_i - prediction_i(\mathbf{x}))^2 
\end{align}</script>

<p>where we sum over all data points in the training set $i$.</p>

<ul>
  <li>
    <p>The convention in machine learning is to represent the linear model as $y’ = w_1 x_1 + b$ instead of the more traditional $y = mx + b$. We could easily generalize this regression from one feature to multiple features and the prediction would be given by $y’ = w_1 x_1 + w_2 x_2 + w_3 x_3 + b$, where all the $w_i$’s are weights and all the $x_i$’s are features. The process of fitting the model is called training and the process of using the trained model to make a prediction is called inference.</p>
  </li>
  <li>
    <p>A commonly used loss function is the mean squared error (MSE), but this is neither the only loss function nor the best or most practical loss function for all cases.</p>
  </li>
</ul>

<h2 id="reducing-loss">Reducing Loss</h2>

<ul>
  <li>
    <p>One of the popular ways to find the minimum in the loss function is to use Gradient Descent. We calculate a gradient at each point and move in the direction of decreasing gradient. The step size that we use as we advance in the direction of decreasing gradient is called the learning rate. This has to be chosen carefully: in multi-dimensional problems, too large a learning rate can cause the code to become unstable.</p>
  </li>
  <li>
    <p>We can also get trapped in local minima if the space is <em>not-convex</em>. There is a whole sprawling field of non-convex optimization.</p>
  </li>
  <li>
    <p>There are two important flavors of gradient descent:</p>

    <p><strong>Stochastic Gradient Descent:</strong></p>

    <p>In theory, while using gradient descent, we need to calculate the gradient of the loss function considering all examples. However in practice, this is found to be excessive and computationally expensive. We therefore select only one example at random and calculate the gradient of the loss function considering only that one example. This is called stochastic gradient descent. Although this might require more steps to reach the optimum, overall there is usually less computation when dealing with very large data sets. Gradient calculations can be very expensive.</p>

    <p>Mathematically, we want to calculate</p>

    <script type="math/tex; mode=display">\begin{align}
  w_{n+1} = w_n -\eta Q(w)
  \end{align}</script>

    <p>where $\eta$ is the learning rate and $Q(w) = \frac{1}{N}\sum_i Q_i(w)$ is the $L_2$ loss for the $i$-th example. What we do instead, in stochastic gradient descent is to first randomnly pick a particular example $k$ and calculate the loss function $Q_k(w)$ and then update $w$ using:</p>

    <script type="math/tex; mode=display">\begin{align}
  w_{n+1} = w_n -\eta Q_k(w)
  \end{align}</script>

    <p>and perform this iteratively until the minimum criterion is reached.</p>

    <p>When there are multiple parameters to optimize, the gradient is of course a vector, and so is $\mathbf{w} = (w_1, w_2, \ldots)$ and we proceed in the direction of steepest gradient in steps $\eta$ large.</p>

    <p><strong>Mini-batch Gradient Descent</strong></p>

    <p>This is very similar to stochastic gradient descent except that instead of taking only one data point, we take batches of 10 or 100 of them.  Especially with datasets that contain duplicates, enormous datasets do not contain any further information than very large datasets. Mini-batch gradient descent exploits this fact and works very well in practice. Typical values of batch-size range between 10 and 1000. Stochastic gradient descent is an extreme example of mini-batch gradient descent with batch size 1.</p>
  </li>
  <li>
    <p>The algorithm to train the model in this case is iterative: we start with some intial guesses for the parameters, compute the loss function for those values, update the values of the parameters through some scheme (with the goal of moving in the direction of lower loss), calculate the loss for the updated values of the parameters, and proceed iteratively until we achieve convergence. Convergence is usually assumed when the loss becomes below some threshold value, or if the loss function starts changing extremely slowly.</p>
  </li>
</ul>

<h2 id="first-steps-with-tensorflow">First steps with TensorFlow</h2>

<ul>
  <li>
    <p>TensnsorFlow is a computational framework that allows you to build machine learning models. We can use both lower level APIs by defining models based on a series of mathematical operations or we could use predefined higher level APIs such as <code class="highlighter-rouge">tf.estimator</code>. These architectures can be linear regressors or neural networks.</p>
  </li>
  <li>
    <p>TensorFlow consists of the following two components:</p>

    <ol>
      <li>A graph protocol buffer</li>
      <li>A runtime that executes the distributed graph</li>
    </ol>
  </li>
  <li>
    <p>The <strong>graph protocol buffer</strong> or protobuf takes data structures written in a text file and then generates classes in Python (or other language) that allows you to load, save and interact with the data in a user friendly way. In this sense the protobuf and the runtime are akin to Python code and a Python interpreter.</p>
  </li>
  <li>
    <p>Because TensorFlow is built using APIs of various levels of abstraction, we have a choice of level. In general, I should chose the layer that offers the highest level of abstraction. Of course, the highest layers are also less flexible, so for some special modeling cases if I need more flexibility, I can just drop one run lower in the level of API.</p>
  </li>
</ul>

<h3 id="tfestimator-api"><code class="highlighter-rouge">tf.estimator</code> API</h3>

<p>The <code class="highlighter-rouge">tf.estimator</code> API is one of the highest level APIs that has a lot of prepackaged tools of use. Below we show some code to exemplify its use. In the example below, we are going to estimate the median housing pricebased on just one input feature. The data is from the 1990 California housing census data. Available <a href="https://developers.google.com/machine-learning/crash-course/california-housing-data-description">here</a>. First we perform some imports:</p>

<h3 id="step-0-setup-imports-loading-and-inspecting-the-data">Step 0: Setup, imports, loading and inspecting the data</h3>

<p><strong>Note:</strong> <code class="highlighter-rouge">tensorflow</code> does not work with <code class="highlighter-rouge">python 3.7.x</code> yet. I had <code class="highlighter-rouge">python 3.7.2</code> loaded on my machine, so this was a good exercise in understanding <code class="highlighter-rouge">conda environments</code>. It is recommended that each project have ita own <em>environment</em>, which consists of a python ditribution (which can be versioned) and a series of packages (which can also be versioned). we first use</p>

<p><code class="highlighter-rouge">conda env --name &lt;envname&gt;</code></p>

<p><code class="highlighter-rouge">-n</code> can be used instead of <code class="highlighter-rouge">--name</code>. The environment can be created with all the required packages in one line as follows (recommended):</p>

<p><code class="highlighter-rouge">conda env -n tensorflowenv python=3.6 scipy numpy matplotlib scikit-learn</code></p>

<p>Here <code class="highlighter-rouge">tensorflowenv</code> is the environment name that I choose to give. I then go into the new environment using</p>

<p><code class="highlighter-rouge">conda activate tensorflowenv</code></p>

<p>Now <code class="highlighter-rouge">jupyter</code> doesn’t yet recognize this new kernel, so I need to manually go and install it (for each environment I create) using the following (see <a href="https://stackoverflow.com/questions/39604271/conda-environments-not-showing-up-in-jupyter-notebook?rq=1">here</a> for more details:)</p>

<p><code class="highlighter-rouge">python -m ipykernel install --name tensorflowenv --display-name "Python (tendorflowenv)"</code></p>

<p>Now when I invoke <code class="highlighter-rouge">jupyter notebook</code> in the activate environment, I can use the dropdown menu to activate the kernel with the specific python version and packages that I desire. Also note that invoking <code class="highlighter-rouge">conda install pkgname</code> in the activated environment only installs the <code class="highlighter-rouge">pkgname</code> for that particular environment.</p>

<p>For more information regarding environments, see the <a href="https://conda.io/projects/conda/en/latest/user-guide/tasks/manage-environments.html">docs</a></p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="nn">math</span>
<span class="kn">from</span> <span class="nn">matplotlib</span> <span class="kn">import</span> <span class="n">cm</span>
<span class="kn">from</span> <span class="nn">matplotlib</span> <span class="kn">import</span> <span class="n">gridspec</span>
<span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="n">plt</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="n">np</span>
<span class="kn">import</span> <span class="nn">pandas</span> <span class="k">as</span> <span class="n">pd</span>
<span class="kn">from</span> <span class="nn">sklearn</span> <span class="kn">import</span> <span class="n">metrics</span>
<span class="kn">import</span> <span class="nn">tensorflow</span> <span class="k">as</span> <span class="n">tf</span>

<span class="kn">from</span> <span class="nn">tensorflow.python.data</span> <span class="kn">import</span> <span class="n">Dataset</span>
<span class="n">tf</span><span class="o">.</span><span class="n">logging</span><span class="o">.</span><span class="n">set_verbosity</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">logging</span><span class="o">.</span><span class="n">ERROR</span><span class="p">)</span>
<span class="n">pd</span><span class="o">.</span><span class="n">options</span><span class="o">.</span><span class="n">display</span><span class="o">.</span><span class="n">max_rows</span> <span class="o">=</span> <span class="mi">10</span>
<span class="n">pd</span><span class="o">.</span><span class="n">options</span><span class="o">.</span><span class="n">display</span><span class="o">.</span><span class="n">float_format</span> <span class="o">=</span> <span class="s">'{:.1f}'</span><span class="o">.</span><span class="n">format</span>
</code></pre></div></div>

<p>We next load the data</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">california_housing_dataframe</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">read_csv</span><span class="p">(</span><span class="s">'https://download.mlcc.google.com/mledu-datasets/california_housing_train.csv'</span><span class="p">,</span> <span class="n">sep</span><span class="o">=</span><span class="s">','</span><span class="p">)</span>

<span class="n">california_housing_dataframe</span> <span class="o">=</span> <span class="n">california_housing_dataframe</span><span class="o">.</span><span class="n">reindex</span><span class="p">(</span>
    <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">permutation</span><span class="p">(</span><span class="n">california_housing_dataframe</span><span class="o">.</span><span class="n">index</span><span class="p">))</span>
<span class="n">california_housing_dataframe</span><span class="p">[</span><span class="s">'median_house_value'</span><span class="p">]</span> <span class="o">/=</span> <span class="mi">1000</span>
</code></pre></div></div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">california_housing_dataframe</span><span class="o">.</span><span class="n">head</span><span class="p">()</span>
</code></pre></div></div>

<div>
<style scoped="">
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>longitude</th>
      <th>latitude</th>
      <th>housing_median_age</th>
      <th>total_rooms</th>
      <th>total_bedrooms</th>
      <th>population</th>
      <th>households</th>
      <th>median_income</th>
      <th>median_house_value</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>4689</th>
      <td>-118.1</td>
      <td>34.1</td>
      <td>28.0</td>
      <td>238.0</td>
      <td>58.0</td>
      <td>142.0</td>
      <td>31.0</td>
      <td>0.5</td>
      <td>500.0</td>
    </tr>
    <tr>
      <th>7444</th>
      <td>-118.3</td>
      <td>33.9</td>
      <td>13.0</td>
      <td>2887.0</td>
      <td>853.0</td>
      <td>2197.0</td>
      <td>800.0</td>
      <td>2.9</td>
      <td>207.9</td>
    </tr>
    <tr>
      <th>14389</th>
      <td>-122.1</td>
      <td>38.0</td>
      <td>42.0</td>
      <td>2225.0</td>
      <td>367.0</td>
      <td>864.0</td>
      <td>381.0</td>
      <td>4.1</td>
      <td>172.4</td>
    </tr>
    <tr>
      <th>9089</th>
      <td>-119.0</td>
      <td>36.1</td>
      <td>20.0</td>
      <td>1042.0</td>
      <td>183.0</td>
      <td>509.0</td>
      <td>175.0</td>
      <td>3.0</td>
      <td>73.0</td>
    </tr>
    <tr>
      <th>8388</th>
      <td>-118.5</td>
      <td>34.3</td>
      <td>33.0</td>
      <td>1549.0</td>
      <td>264.0</td>
      <td>881.0</td>
      <td>289.0</td>
      <td>5.1</td>
      <td>222.9</td>
    </tr>
  </tbody>
</table>
</div>

<p>We are now going to use <code class="highlighter-rouge">total_rooms</code> as an input feature to predict our target (or label) <code class="highlighter-rouge">median_house_value</code>.  Note that the data is at the city block level, so the feature represents the total number of rooms in the block. We are going to use the <code class="highlighter-rouge">tf.estimator</code> API to implement a linear regressor to model the data. The API already implements a lot of the low-level nuts and bolts of the regression (or in general, other models) so we can focus on the training, evaluating, and visualizing aspects of the process. The specific class we will use is the <code class="highlighter-rouge">tf.estimator.LinearRegressor</code> class.</p>

<h3 id="step-1-defining-the-features">Step 1: Defining the features</h3>

<p>There are two main classes of features:</p>

<ul>
  <li>
    <p><strong>categorical:</strong> a feature that can take on discrete values, like spam/not spam, sunny/rainy/cloudy, etc.</p>
  </li>
  <li>
    <p><strong>neumerical:</strong> a feature that can take on continuous or a large number of discrete values, like price, temperature, etc. It seems that these are features that we can do arithmetic on.</p>
  </li>
</ul>

<p>In TensorFlow, we need to define what kind of feature we are working with. In our case, because we are working with <code class="highlighter-rouge">total_rooms</code> in this example, it is a numerical feature. This identification of the type of feature is done using <code class="highlighter-rouge">tf.feature_column</code> module, which has various functions built into it. See <a href="https://www.tensorflow.org/guide/feature_columns">here</a> for documentation. Feature columns only contain a description of the data and not the data itself</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">my_feature</span> <span class="o">=</span> <span class="n">california_housing_dataframe</span><span class="p">[[</span><span class="s">'total_rooms'</span><span class="p">]]</span>
<span class="n">feature_columns</span> <span class="o">=</span> <span class="p">[</span><span class="n">tf</span><span class="o">.</span><span class="n">feature_column</span><span class="o">.</span><span class="n">numeric_column</span><span class="p">(</span><span class="s">'total_rooms'</span><span class="p">)]</span>
</code></pre></div></div>

<h3 id="step-2-defining-the-target">Step 2: Defining the target</h3>

<p>We want to predict the meadian house value given the number of rooms, so this is our target</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">targets</span> <span class="o">=</span> <span class="n">california_housing_dataframe</span><span class="p">[</span><span class="s">'median_house_value'</span><span class="p">]</span>
</code></pre></div></div>

<h3 id="step-3-configure-the-linearregressor">Step 3: Configure the LinearRegressor</h3>

<p>We now configure a linear regression model using <code class="highlighter-rouge">LinearRegressor</code>. The optimization itself is carried out using a built-in implementation of mini-batch stochastic gradient descent. We also use the <code class="highlighter-rouge">tf.contrib.estimator.clip_gradients_by_norm</code> functions to impose a cut off on the step size so that the gradient descent doesn’t become unstable.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">my_optimizer</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">train</span><span class="o">.</span><span class="n">GradientDescentOptimizer</span><span class="p">(</span><span class="n">learning_rate</span><span class="o">=</span><span class="mf">0.0000001</span><span class="p">)</span>
<span class="n">my_optimizer</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">contrib</span><span class="o">.</span><span class="n">estimator</span><span class="o">.</span><span class="n">clip_gradients_by_norm</span><span class="p">(</span><span class="n">my_optimizer</span><span class="p">,</span> <span class="mf">5.0</span><span class="p">)</span>
</code></pre></div></div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">linear_regressor</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">estimator</span><span class="o">.</span><span class="n">LinearRegressor</span><span class="p">(</span><span class="n">feature_columns</span><span class="o">=</span><span class="n">feature_columns</span><span class="p">,</span>
                                                <span class="n">optimizer</span><span class="o">=</span><span class="n">my_optimizer</span><span class="p">)</span>
</code></pre></div></div>

<h3 id="step-4-define-the-input-function">Step 4: Define the input function</h3>

<p>We now need to define an input function which tells tensorflow how to preprocess the data as well as how to batch, shuffle, and repeat it during model training. This function is a bit of a jump from what we have seen so far, so we will explore it step by step.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">my_input_fn</span><span class="p">(</span><span class="n">features</span><span class="p">,</span> <span class="n">targets</span><span class="p">,</span> <span class="n">batch_size</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">shuffle</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span> <span class="n">num_epochs</span><span class="o">=</span><span class="bp">None</span><span class="p">):</span>
    <span class="s">"""Trains a linear regression model with one feature
    Inputs: 
        features: a pandas dataframe of features
        targets: a pandas dataframe of targets
        batch_size: size of batches to be passed to the model
        shuffle: Boolean, whether to shuffle the training data or not
        num_epochs: number of epochs for which the data should be repeated. None means repeat indefinitely
    Returns:
        Tuple of (features, label) for the next data batch
    """</span>
    <span class="c"># convert pandas data into a dict of numpy arrays</span>
    <span class="n">features</span> <span class="o">=</span> <span class="p">{</span><span class="n">key</span><span class="p">:</span><span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">value</span><span class="p">)</span> <span class="k">for</span> <span class="p">(</span><span class="n">key</span><span class="p">,</span> <span class="n">value</span><span class="p">)</span> <span class="ow">in</span> <span class="nb">dict</span><span class="p">(</span><span class="n">features</span><span class="p">)</span><span class="o">.</span><span class="n">items</span><span class="p">()}</span> <span class="c"># line 13</span>
    
    <span class="c"># Construct a dataset and configure batching and repeating</span>
    <span class="n">ds</span> <span class="o">=</span> <span class="n">Dataset</span><span class="o">.</span><span class="n">from_tensor_slices</span><span class="p">((</span><span class="n">features</span><span class="p">,</span><span class="n">targets</span><span class="p">))</span> <span class="c"># line 16</span>
    <span class="n">ds</span> <span class="o">=</span> <span class="n">ds</span><span class="o">.</span><span class="n">batch</span><span class="p">(</span><span class="n">batch_size</span><span class="p">)</span><span class="o">.</span><span class="n">repeat</span><span class="p">(</span><span class="n">num_epochs</span><span class="p">)</span> <span class="c"># line 17</span>
    
    <span class="c"># Shuffle the data if needed</span>
    <span class="k">if</span> <span class="n">shuffle</span><span class="p">:</span>
        <span class="n">ds</span> <span class="o">=</span> <span class="n">ds</span><span class="o">.</span><span class="n">shuffle</span><span class="p">(</span><span class="n">buffer_size</span><span class="o">=</span><span class="mi">10000</span><span class="p">)</span> <span class="c"># line 21</span>
        
    <span class="c"># return the next batch of data</span>
    <span class="n">features</span><span class="p">,</span> <span class="n">labels</span> <span class="o">=</span> <span class="n">ds</span><span class="o">.</span><span class="n">make_one_shot_iterator</span><span class="p">()</span><span class="o">.</span><span class="n">get_next</span><span class="p">()</span> <span class="c"># line 24</span>
    <span class="k">return</span> <span class="p">(</span><span class="n">features</span><span class="p">,</span> <span class="n">labels</span><span class="p">)</span>
</code></pre></div></div>

<p>Here is an explanation by line number of the code above.</p>

<ul>
  <li>
    <p><strong>line 13:</strong>
  <code class="highlighter-rouge">features</code> is a pandas dictionary, and <code class="highlighter-rouge">dict(features)</code> converts it into a dictionary with the column name as key and all the values in the column as pandas series of values. Therefore, the items generates an iterator of type <code class="highlighter-rouge">(feature_name, values)</code>, which we use to generate our feature dictionary. <code class="highlighter-rouge">values</code> is of type <code class="highlighter-rouge">pandas.Series</code>.</p>
  </li>
  <li>
    <p><strong>lines 16:</strong></p>

    <p>This line makes use of the <code class="highlighter-rouge">tf.data</code> API, which lets you take in distributed data from various sources and transform it in different ways to generate datasets for machine learning models. There are two cheif abstractions that <code class="highlighter-rouge">tf.data</code> introduces, one of which is the <code class="highlighter-rouge">tf.data.Dataset</code> module (the other is the <code class="highlighter-rouge">tf.data.Iterator</code> module which is used in line 24 and discussed next). Using <code class="highlighter-rouge">tf.data.Dataset</code>, there are two distinct ways to create a dataset:</p>

    <ol>
      <li>Creating a source (for example <code class="highlighter-rouge">Dataset.from_tensor_slices()</code>) which constructs a dataset from one or more <code class="highlighter-rouge">tf.Tensor</code> objects.</li>
      <li>Applying a transformations (for example <code class="highlighter-rouge">Dataset.batch()</code>) which constructs a dataset from one or more <code class="highlighter-rouge">tf.data.Datasets</code> objects.</li>
    </ol>

    <p>A tensor object is in principle a matrix of n-dimensions, except that its value is not evaluated until called for using a <code class="highlighter-rouge">tf.run()</code> command. The objects can be passed into operations to yield other tensor objects, but again, actual evaluation of the numerical value of the tensor is held off until later. This enables the <em>flow</em> of a tensor through various operations without numerical evaluation. With this context, what line 16 is doing is the following:</p>

    <p><code class="highlighter-rouge">Dataset.from_tensor_slices()</code> returns a dataset for each row of the input tensor. For example the code snippet below would return <code class="highlighter-rouge">[1, 2], [3, 4]</code> for the variable <code class="highlighter-rouge">ds</code>. In our case, we provide a tuple of the form <code class="highlighter-rouge">(features, targets)</code>, where <code class="highlighter-rouge">features</code> is a dictionary, and <code class="highlighter-rouge">targets</code> is a number, <code class="highlighter-rouge">from_tensor_slices</code> would essentially unpack this tuple, to provide multiple datasets, each of the form <code class="highlighter-rouge">({key: values}, target)</code>. In other words, for this dataset, if I were to print the Dataset object (with an appropriate <code class="highlighter-rouge">tf.Session.run()</code> call) I would see something like <code class="highlighter-rouge">({lattitide: -118, longitude: 30, ... , households: 600, ...}, &lt;median_house_value&gt;)</code>. It takes a row, converts it into a dictionary of the form <code class="highlighter-rouge">{column name: value}</code> an then constructs a iteratable and transformable <code class="highlighter-rouge">tf.Dataset</code> object which has a dictionary of features associated with the <code class="highlighter-rouge">target</code>. The code snippet in the next cell below should make things clear regarding the output format of the <code class="highlighter-rouge">tf.Dataset</code> call in this case.</p>
  </li>
</ul>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code>    <span class="n">t</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">constant</span><span class="p">([[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">],</span> <span class="p">[</span><span class="mi">3</span><span class="p">,</span> <span class="mi">4</span><span class="p">]])</span>
    <span class="n">ds</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">Dataset</span><span class="o">.</span><span class="n">from_tensor_slices</span><span class="p">(</span><span class="n">t</span><span class="p">)</span>   <span class="c"># [1, 2], [3, 4]</span>
</code></pre></div></div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">df_test</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">()</span>
<span class="n">df_test</span><span class="p">[</span><span class="s">'col1'</span><span class="p">]</span> <span class="o">=</span> <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">]</span>
<span class="n">df_test</span><span class="p">[</span><span class="s">'col2'</span><span class="p">]</span> <span class="o">=</span> <span class="p">[</span><span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">]</span>
<span class="n">t</span> <span class="o">=</span> <span class="p">[</span><span class="mi">100</span><span class="p">,</span> <span class="mi">200</span><span class="p">]</span>
<span class="n">x</span> <span class="o">=</span> <span class="p">{</span><span class="n">key</span><span class="p">:</span> <span class="n">value</span> <span class="k">for</span> <span class="p">(</span><span class="n">key</span><span class="p">,</span> <span class="n">value</span><span class="p">)</span> <span class="ow">in</span> <span class="nb">dict</span><span class="p">(</span><span class="n">df_test</span><span class="p">)</span><span class="o">.</span><span class="n">items</span><span class="p">()}</span>
<span class="k">print</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
<span class="n">ds_test</span> <span class="o">=</span> <span class="n">Dataset</span><span class="o">.</span><span class="n">from_tensor_slices</span><span class="p">((</span><span class="n">x</span><span class="p">,</span> <span class="n">t</span><span class="p">))</span>
<span class="n">tensor_test</span> <span class="o">=</span> <span class="n">ds_test</span><span class="o">.</span><span class="n">make_one_shot_iterator</span><span class="p">()</span><span class="o">.</span><span class="n">get_next</span><span class="p">()</span>
<span class="k">with</span> <span class="n">tf</span><span class="o">.</span><span class="n">Session</span><span class="p">()</span> <span class="k">as</span> <span class="n">session</span><span class="p">:</span>
    <span class="k">print</span><span class="p">(</span><span class="n">session</span><span class="o">.</span><span class="n">run</span><span class="p">(</span><span class="n">tensor_test</span><span class="p">))</span>
    <span class="k">print</span><span class="p">(</span><span class="n">session</span><span class="o">.</span><span class="n">run</span><span class="p">(</span><span class="n">tensor_test</span><span class="p">))</span>
</code></pre></div></div>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>{'col1': 0    0
1    1
Name: col1, dtype: int64, 'col2': 0    2
1    3
Name: col2, dtype: int64}
({'col1': 0, 'col2': 2}, 100)
({'col1': 1, 'col2': 3}, 200)
</code></pre></div></div>

<h3 id="step-5-train-the-model">Step 5: Train the model</h3>

<p>We now call <code class="highlighter-rouge">train()</code> on <code class="highlighter-rouge">linear_regressor</code> to train the model.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">_</span> <span class="o">=</span> <span class="n">linear_regressor</span><span class="o">.</span><span class="n">train</span><span class="p">(</span>
    <span class="n">input_fn</span><span class="o">=</span><span class="k">lambda</span><span class="p">:</span> <span class="n">my_input_fn</span><span class="p">(</span><span class="n">my_feature</span><span class="p">,</span> <span class="n">targets</span><span class="p">),</span>
    <span class="n">steps</span><span class="o">=</span><span class="mi">100</span><span class="p">)</span>
</code></pre></div></div>

<p>The use of the <code class="highlighter-rouge">lambda</code> function is somewhat unclear to me. It seems that it ensures that the input function is truly randomized? Or that datasets are correctly paired to models?</p>

<h3 id="evaluate-the-model">Evaluate the model</h3>

<p>We now just make predictions on the training data to see how well the model fit it during training.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c"># Create an input function for predictions</span>
<span class="n">prediction_input_fn</span> <span class="o">=</span> <span class="k">lambda</span><span class="p">:</span> <span class="n">my_input_fn</span><span class="p">(</span><span class="n">my_feature</span><span class="p">,</span> <span class="n">targets</span><span class="p">,</span> <span class="n">num_epochs</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">shuffle</span><span class="o">=</span><span class="bp">False</span><span class="p">)</span>

<span class="c"># Call predict on the linear_regressor to make predictions</span>
<span class="n">predictions</span> <span class="o">=</span> <span class="n">linear_regressor</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">input_fn</span><span class="o">=</span><span class="n">prediction_input_fn</span><span class="p">)</span>

<span class="c">#Format predictions as a numpy array so we can calculate error metrics</span>
<span class="n">predictions</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="n">item</span><span class="p">[</span><span class="s">'predictions'</span><span class="p">][</span><span class="mi">0</span><span class="p">]</span> <span class="k">for</span> <span class="n">item</span> <span class="ow">in</span> <span class="n">predictions</span><span class="p">])</span>

<span class="c">#Calculate the mean squared error and the root mean squared error</span>
<span class="n">mean_squared_error</span> <span class="o">=</span> <span class="n">metrics</span><span class="o">.</span><span class="n">mean_squared_error</span><span class="p">(</span><span class="n">predictions</span><span class="p">,</span> <span class="n">targets</span><span class="p">)</span>
<span class="n">root_mean_squared_error</span> <span class="o">=</span> <span class="n">math</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">mean_squared_error</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="s">"Mean Squared Error (on training data): </span><span class="si">%0.3</span><span class="s">f"</span> <span class="o">%</span> <span class="n">mean_squared_error</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="s">"Root Mean Squared Error (on training data): </span><span class="si">%0.3</span><span class="s">f"</span> <span class="o">%</span> <span class="n">root_mean_squared_error</span><span class="p">)</span>
</code></pre></div></div>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Mean Squared Error (on training data): 56367.025
Root Mean Squared Error (on training data): 237.417
</code></pre></div></div>

<p>To know if this mse and rmse is any good, we compare the rmse to the difference between the min and max of our targets</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">min_house_value</span> <span class="o">=</span> <span class="n">california_housing_dataframe</span><span class="p">[</span><span class="s">"median_house_value"</span><span class="p">]</span><span class="o">.</span><span class="nb">min</span><span class="p">()</span>
<span class="n">max_house_value</span> <span class="o">=</span> <span class="n">california_housing_dataframe</span><span class="p">[</span><span class="s">"median_house_value"</span><span class="p">]</span><span class="o">.</span><span class="nb">max</span><span class="p">()</span>
<span class="n">min_max_difference</span> <span class="o">=</span> <span class="n">max_house_value</span> <span class="o">-</span> <span class="n">min_house_value</span>

<span class="k">print</span><span class="p">(</span><span class="s">"Min. Median House Value: </span><span class="si">%0.3</span><span class="s">f"</span> <span class="o">%</span> <span class="n">min_house_value</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="s">"Max. Median House Value: </span><span class="si">%0.3</span><span class="s">f"</span> <span class="o">%</span> <span class="n">max_house_value</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="s">"Difference between Min. and Max.: </span><span class="si">%0.3</span><span class="s">f"</span> <span class="o">%</span> <span class="n">min_max_difference</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="s">"Root Mean Squared Error: </span><span class="si">%0.3</span><span class="s">f"</span> <span class="o">%</span> <span class="n">root_mean_squared_error</span><span class="p">)</span>
</code></pre></div></div>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Min. Median House Value: 14.999
Max. Median House Value: 500.001
Difference between Min. and Max.: 485.002
Root Mean Squared Error: 237.417
</code></pre></div></div>

<p>This seems really large - the rmse is nearly 50% of the max difference. Let us first look at some summary statistics on how well the model is doing.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">calibration_data</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">()</span>
<span class="n">calibration_data</span><span class="p">[</span><span class="s">'predictions'</span><span class="p">]</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">Series</span><span class="p">(</span><span class="n">predictions</span><span class="p">)</span>
<span class="n">calibration_data</span><span class="p">[</span><span class="s">'targets'</span><span class="p">]</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">Series</span><span class="p">(</span><span class="n">targets</span><span class="p">)</span>
<span class="n">calibration_data</span><span class="o">.</span><span class="n">describe</span><span class="p">()</span>
</code></pre></div></div>

<div>
<style scoped="">
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>predictions</th>
      <th>targets</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>count</th>
      <td>17000.0</td>
      <td>17000.0</td>
    </tr>
    <tr>
      <th>mean</th>
      <td>0.1</td>
      <td>207.3</td>
    </tr>
    <tr>
      <th>std</th>
      <td>0.1</td>
      <td>116.0</td>
    </tr>
    <tr>
      <th>min</th>
      <td>0.0</td>
      <td>15.0</td>
    </tr>
    <tr>
      <th>25%</th>
      <td>0.1</td>
      <td>119.4</td>
    </tr>
    <tr>
      <th>50%</th>
      <td>0.1</td>
      <td>180.4</td>
    </tr>
    <tr>
      <th>75%</th>
      <td>0.2</td>
      <td>265.0</td>
    </tr>
    <tr>
      <th>max</th>
      <td>1.9</td>
      <td>500.0</td>
    </tr>
  </tbody>
</table>
</div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c"># Plotting the data</span>

<span class="n">sample</span> <span class="o">=</span> <span class="n">california_housing_dataframe</span><span class="o">.</span><span class="n">sample</span><span class="p">(</span><span class="n">n</span><span class="o">=</span><span class="mi">300</span><span class="p">)</span>

<span class="c"># Get the min and max total_rooms values.</span>
<span class="n">x_0</span> <span class="o">=</span> <span class="n">sample</span><span class="p">[</span><span class="s">"total_rooms"</span><span class="p">]</span><span class="o">.</span><span class="nb">min</span><span class="p">()</span>
<span class="n">x_1</span> <span class="o">=</span> <span class="n">sample</span><span class="p">[</span><span class="s">"total_rooms"</span><span class="p">]</span><span class="o">.</span><span class="nb">max</span><span class="p">()</span>

<span class="c"># Retrieve the final weight and bias generated during training.</span>
<span class="n">weight</span> <span class="o">=</span> <span class="n">linear_regressor</span><span class="o">.</span><span class="n">get_variable_value</span><span class="p">(</span><span class="s">'linear/linear_model/total_rooms/weights'</span><span class="p">)[</span><span class="mi">0</span><span class="p">]</span>
<span class="n">bias</span> <span class="o">=</span> <span class="n">linear_regressor</span><span class="o">.</span><span class="n">get_variable_value</span><span class="p">(</span><span class="s">'linear/linear_model/bias_weights'</span><span class="p">)</span>

<span class="c"># Get the predicted median_house_values for the min and max total_rooms values.</span>
<span class="n">y_0</span> <span class="o">=</span> <span class="n">weight</span> <span class="o">*</span> <span class="n">x_0</span> <span class="o">+</span> <span class="n">bias</span> 
<span class="n">y_1</span> <span class="o">=</span> <span class="n">weight</span> <span class="o">*</span> <span class="n">x_1</span> <span class="o">+</span> <span class="n">bias</span>

<span class="c"># Plot our regression line from (x_0, y_0) to (x_1, y_1).</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">([</span><span class="n">x_0</span><span class="p">,</span> <span class="n">x_1</span><span class="p">],</span> <span class="p">[</span><span class="n">y_0</span><span class="p">,</span> <span class="n">y_1</span><span class="p">],</span> <span class="n">c</span><span class="o">=</span><span class="s">'r'</span><span class="p">)</span> <span class="c"># Plotting the line using just 2 points</span>

<span class="c"># Label the graph axes.</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s">"median_house_value"</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s">"total_rooms"</span><span class="p">)</span>

<span class="c"># Plot a scatter plot from our data sample.</span>
<span class="n">plt</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">sample</span><span class="p">[</span><span class="s">"total_rooms"</span><span class="p">],</span> <span class="n">sample</span><span class="p">[</span><span class="s">"median_house_value"</span><span class="p">])</span>

<span class="c"># Display graph.</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</code></pre></div></div>

<p><img src="/assets/images/coursenotes/GoogleML/output_27_0.png" alt="png" width="75%" class="align-center" /></p>

<p>The rest of the exercise goes into a discussion on tweaking the hyperparameters to overcome this por fit. With a suitable choice of <code class="highlighter-rouge">learning_rate</code>, <code class="highlighter-rouge">steps</code> and <code class="highlighter-rouge">batch_size</code> we can get a reasonably good fit. The parameter <code class="highlighter-rouge">steps</code> is just the number of times the mini-batch gradient descent attempts to go towards the minimum, and <code class="highlighter-rouge">batch_size</code> is the number of data points the gradient descent optimizer is optimizing over per step, so <code class="highlighter-rouge">steps * batch_size</code> is just the total number of data points involved in the optimization (not necessary all unique because the data points are randomly sampled)</p>

<h3 id="aside-synthetics-features-and-outliers">Aside: Synthetics features and outliers</h3>

<p>In this set of tasks, we are going to create a synthetic feature that is the ratio of two other features and we are going to use this new feature to train the linear regression model. We are also going to look at how we can remove outliers to improve the effectiveness of the model. We are going to use the same <code class="highlighter-rouge">my_input_fn</code> defined above to generate datasets. However let us define a new function <code class="highlighter-rouge">train_model</code> to train the model</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">train_model</span><span class="p">(</span><span class="n">learning_rate</span><span class="p">,</span> <span class="n">steps</span><span class="p">,</span> <span class="n">batch_size</span><span class="p">,</span> <span class="n">input_feature</span><span class="p">):</span>
    <span class="s">"""
    Args:
        learning_rate: A `float`, the learning rate.
        steps: A non-zero `int`, the total number of training steps. A training step
               consists of a forward and backward pass using a single batch.
        batch_size: A non-zero `int`, the batch size.
        input_feature: A `string` specifying a column from `california_housing_dataframe`
                       to use as input feature.
      
    Returns:
        A Pandas `DataFrame` containing targets and the corresponding predictions done
        after training the model.
  """</span>
    <span class="n">periods</span> <span class="o">=</span> <span class="mi">10</span> <span class="c"># line 15</span>
    <span class="n">steps_per_period</span> <span class="o">=</span> <span class="n">steps</span> <span class="o">/</span> <span class="n">periods</span>
    
    <span class="n">my_feature</span> <span class="o">=</span> <span class="n">input_feature</span>
    <span class="n">my_feature_data</span> <span class="o">=</span> <span class="n">california_housing_dataframe</span><span class="p">[[</span><span class="n">my_feature</span><span class="p">]]</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="s">'float32'</span><span class="p">)</span>
    <span class="n">my_label</span> <span class="o">=</span> <span class="s">'median_house_value'</span>
    <span class="n">targets</span> <span class="o">=</span> <span class="n">california_housing_dataframe</span><span class="p">[</span><span class="n">my_label</span><span class="p">]</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="s">'float32'</span><span class="p">)</span>
    
    <span class="c"># Create input functions</span>
    <span class="n">training_input_fn</span> <span class="o">=</span> <span class="k">lambda</span><span class="p">:</span> <span class="n">my_input_fn</span><span class="p">(</span><span class="n">my_feature_data</span><span class="p">,</span>
                                            <span class="n">targets</span><span class="p">,</span>
                                            <span class="n">batch_size</span><span class="o">=</span><span class="n">batch_size</span><span class="p">,</span>
                                            <span class="n">shuffle</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span>
                                            <span class="n">num_epochs</span><span class="o">=</span><span class="bp">None</span><span class="p">)</span>
    <span class="n">predict_training_input_fn</span> <span class="o">=</span> <span class="k">lambda</span><span class="p">:</span> <span class="n">my_input_fn</span><span class="p">(</span><span class="n">my_feature_data</span><span class="p">,</span> <span class="c"># line 29</span>
                                                   <span class="n">targets</span><span class="p">,</span>
                                                   <span class="n">batch_size</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
                                                   <span class="n">shuffle</span><span class="o">=</span><span class="bp">False</span><span class="p">,</span>
                                                   <span class="n">num_epochs</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
    
    <span class="c"># Create feature columns</span>
    <span class="n">feature_columns</span> <span class="o">=</span> <span class="p">[</span><span class="n">tf</span><span class="o">.</span><span class="n">feature_column</span><span class="o">.</span><span class="n">numeric_column</span><span class="p">(</span><span class="n">my_feature</span><span class="p">)]</span> <span class="c"># line36</span>
    
    <span class="c"># Create a LinearRegressor object</span>
    <span class="n">my_optimizer</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">train</span><span class="o">.</span><span class="n">GradientDescentOptimizer</span><span class="p">(</span><span class="n">learning_rate</span><span class="o">=</span><span class="n">learning_rate</span><span class="p">)</span>
    <span class="n">my_optimizer</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">contrib</span><span class="o">.</span><span class="n">estimator</span><span class="o">.</span><span class="n">clip_gradients_by_norm</span><span class="p">(</span><span class="n">my_optimizer</span><span class="p">,</span> <span class="mi">5</span><span class="p">)</span>
    <span class="n">linear_regressor</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">estimator</span><span class="o">.</span><span class="n">LinearRegressor</span><span class="p">(</span><span class="n">feature_columns</span><span class="o">=</span><span class="n">feature_columns</span><span class="p">,</span>
                                                   <span class="n">optimizer</span><span class="o">=</span><span class="n">my_optimizer</span><span class="p">)</span>
    
    <span class="c"># Plotting commands to plot our model's fit after each period</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">15</span><span class="p">,</span> <span class="mi">6</span><span class="p">))</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">subplot</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s">"Learned Line by Period"</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="n">my_label</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="n">my_feature</span><span class="p">)</span>
    <span class="n">sample</span> <span class="o">=</span> <span class="n">california_housing_dataframe</span><span class="o">.</span><span class="n">sample</span><span class="p">(</span><span class="n">n</span><span class="o">=</span><span class="mi">300</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">sample</span><span class="p">[</span><span class="n">my_feature</span><span class="p">],</span> <span class="n">sample</span><span class="p">[</span><span class="n">my_label</span><span class="p">])</span>
    <span class="n">colors</span> <span class="o">=</span> <span class="p">[</span><span class="n">cm</span><span class="o">.</span><span class="n">coolwarm</span><span class="p">(</span><span class="n">x</span><span class="p">)</span> <span class="k">for</span> <span class="n">x</span> <span class="ow">in</span> <span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="n">periods</span><span class="p">)]</span>
    
    <span class="c"># We now train the model inside a loop so that we can periodically</span>
    <span class="c"># assess loss metrics</span>
    <span class="k">print</span><span class="p">(</span><span class="s">"Training model..."</span><span class="p">)</span>
    <span class="k">print</span><span class="p">(</span><span class="s">"RMSE (on training data):"</span><span class="p">)</span>
    <span class="n">root_mean_squared_errors</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="k">for</span> <span class="n">period</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">periods</span><span class="p">):</span>
        <span class="n">linear_regressor</span><span class="o">.</span><span class="n">train</span><span class="p">(</span><span class="n">input_fn</span><span class="o">=</span><span class="n">training_input_fn</span><span class="p">,</span>
                              <span class="n">steps</span><span class="o">=</span><span class="n">steps_per_period</span><span class="p">)</span>
        <span class="c"># Compute predictions</span>
        <span class="n">predictions</span> <span class="o">=</span> <span class="n">linear_regressor</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">input_fn</span><span class="o">=</span><span class="n">predict_training_input_fn</span><span class="p">)</span>
        <span class="n">predictions</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="n">item</span><span class="p">[</span><span class="s">'predictions'</span><span class="p">][</span><span class="mi">0</span><span class="p">]</span> <span class="k">for</span> <span class="n">item</span> <span class="ow">in</span> <span class="n">predictions</span><span class="p">])</span> <span class="c"># line 64</span>
        
        <span class="c"># Compute mean squared error</span>
        <span class="n">root_mean_squared_error</span> <span class="o">=</span> <span class="n">math</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">metrics</span><span class="o">.</span><span class="n">mean_squared_error</span><span class="p">(</span><span class="n">predictions</span><span class="p">,</span> <span class="n">targets</span><span class="p">))</span>
        
        <span class="c"># Add the loss metrics from this period to our list.</span>
        <span class="n">root_mean_squared_errors</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">root_mean_squared_error</span><span class="p">)</span>
        
        <span class="c"># Occasionally print the current loss.</span>
        <span class="k">print</span><span class="p">(</span><span class="s">"  period </span><span class="si">%02</span><span class="s">d : </span><span class="si">%0.2</span><span class="s">f"</span> <span class="o">%</span> <span class="p">(</span><span class="n">period</span><span class="p">,</span> <span class="n">root_mean_squared_error</span><span class="p">))</span>
        
        <span class="c"># Finally, track the weights and biases over time.</span>
        
        <span class="c"># Apply some math to ensure that the data and line are plotted neatly.</span>
        <span class="n">y_extents</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mi">0</span><span class="p">,</span> <span class="n">sample</span><span class="p">[</span><span class="n">my_label</span><span class="p">]</span><span class="o">.</span><span class="nb">max</span><span class="p">()])</span>
        
        <span class="n">weight</span> <span class="o">=</span> <span class="n">linear_regressor</span><span class="o">.</span><span class="n">get_variable_value</span><span class="p">(</span><span class="s">'linear/linear_model/</span><span class="si">%</span><span class="s">s/weights'</span> <span class="o">%</span> <span class="n">input_feature</span><span class="p">)[</span><span class="mi">0</span><span class="p">]</span> <span class="c"># line 76</span>
        <span class="n">bias</span> <span class="o">=</span> <span class="n">linear_regressor</span><span class="o">.</span><span class="n">get_variable_value</span><span class="p">(</span><span class="s">'linear/linear_model/bias_weights'</span><span class="p">)</span> <span class="c"># line 77</span>
    
        <span class="n">x_extents</span> <span class="o">=</span> <span class="p">(</span><span class="n">y_extents</span> <span class="o">-</span> <span class="n">bias</span><span class="p">)</span> <span class="o">/</span> <span class="n">weight</span>
        <span class="n">x_extents</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">maximum</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">minimum</span><span class="p">(</span><span class="n">x_extents</span><span class="p">,</span>
                                          <span class="n">sample</span><span class="p">[</span><span class="n">my_feature</span><span class="p">]</span><span class="o">.</span><span class="nb">max</span><span class="p">()),</span>
                               <span class="n">sample</span><span class="p">[</span><span class="n">my_feature</span><span class="p">]</span><span class="o">.</span><span class="nb">min</span><span class="p">())</span>
        <span class="n">y_extents</span> <span class="o">=</span> <span class="n">weight</span> <span class="o">*</span> <span class="n">x_extents</span> <span class="o">+</span> <span class="n">bias</span>
        <span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">x_extents</span><span class="p">,</span> <span class="n">y_extents</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="n">colors</span><span class="p">[</span><span class="n">period</span><span class="p">])</span> 
    <span class="k">print</span><span class="p">(</span><span class="s">"Model training finished."</span><span class="p">)</span>
    
    <span class="c"># Output a graph of loss metrics over periods.</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">subplot</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s">'RMSE'</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s">'Periods'</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s">"Root Mean Squared Error vs. Periods"</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">tight_layout</span><span class="p">()</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">root_mean_squared_errors</span><span class="p">)</span>

    <span class="c"># Create a table with calibration data.</span>
    <span class="n">calibration_data</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">()</span>
    <span class="n">calibration_data</span><span class="p">[</span><span class="s">"predictions"</span><span class="p">]</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">Series</span><span class="p">(</span><span class="n">predictions</span><span class="p">)</span>
    <span class="n">calibration_data</span><span class="p">[</span><span class="s">"targets"</span><span class="p">]</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">Series</span><span class="p">(</span><span class="n">targets</span><span class="p">)</span>
    <span class="n">calibration_data</span><span class="o">.</span><span class="n">describe</span><span class="p">()</span>

    <span class="k">print</span><span class="p">(</span><span class="s">"Final RMSE (on training data): </span><span class="si">%0.2</span><span class="s">f"</span> <span class="o">%</span> <span class="n">root_mean_squared_error</span><span class="p">)</span>
  
    <span class="k">return</span> <span class="n">calibration_data</span>
</code></pre></div></div>

<p>Again, we study all the lines that introduce something new.</p>

<ul>
  <li>
    <p><strong>line 29:</strong></p>

    <p>Here, we define a <code class="highlighter-rouge">predict_training_input_fn</code>. This is used when we call <code class="highlighter-rouge">linear_regressor.predict()</code> later, which needs an <code class="highlighter-rouge">input_fn</code> of its own. This is because we need a function that constructs the features that is fed into the <code class="highlighter-rouge">linear_regressor.predict()</code> in order to generate a prediction for the label. The <code class="highlighter-rouge">LinearRegressor</code> object is of course blind to the targets, and makes a prediction based on the outcome of the training phase. Therefore, the output of the <code class="highlighter-rouge">predict_training_input_fn</code> has to be a <code class="highlighter-rouge">tf.data.Dataset</code> object. However, this function is different from <code class="highlighter-rouge">training_input_function</code> in a few ways.</p>

    <ol>
      <li><code class="highlighter-rouge">batch_size=1</code> because I create a prediction of a set of features one at a time</li>
      <li><code class="highlighter-rouge">shuffle=False</code>because there is no need toa shuffle a batch of size 1.</li>
      <li><code class="highlighter-rouge">num_epochs=1</code> is important. <code class="highlighter-rouge">tf.data.Dataset.repeat(count)</code> will throw an out of range error (<code class="highlighter-rouge">tf.errors.OutOfRangeError</code>) when the number of times the dataset is repeated exceeds <code class="highlighter-rouge">count</code>. In our case, <code class="highlighter-rouge">num_epochs</code> is used in place of count. <code class="highlighter-rouge">tf.LinearRegressor.predict()</code> continues to predict until an out of range error is thrown. So in the prediction case, we use this to ensure that the prediction is made 1 set of features at a time.</li>
    </ol>
  </li>
  <li>
    <p><strong>line 36:</strong></p>

    <p>Defining a feature column which is like a bridge, or intermediary between raw data and estimators. It transforms input data into formats that estimators can understand, giving a lot of richness to have input data speak with estimators. See the <a href="https://www.tensorflow.org/guide/feature_columns">docs</a> for the different kind of feature columns, of which <code class="highlighter-rouge">numeric_column</code> is one.</p>
  </li>
  <li>
    <p><strong>line 64:</strong></p>

    <p>Some thoughts on <code class="highlighter-rouge">linear_regressor.predict()</code>. The output of this function returns a generator which iterators over dictionaries. Each dictionary is of the form, for example, <code class="highlighter-rouge">{'predictions': array([15.93631], dtype=float32)}</code>. For each row of the form <code class="highlighter-rouge">(features, label)</code>, there is an entry in the generator object.</p>
  </li>
  <li>
    <p><strong>line 66, 67:</strong></p>

    <p><code class="highlighter-rouge">tf.estimator.get_variable_names()</code> gives me the names of the variables in the <code class="highlighter-rouge">tf.estimator</code> object. In this case, our estimator happens to be a <code class="highlighter-rouge">LinearRegressor</code> object. For example, calling <code class="highlighter-rouge">linear_regressor.get_variable_names()</code> outputs:</p>

    <div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>  ['global_step',
   'linear/linear_model/bias_weights',
   'linear/linear_model/total_rooms/weights']
</code></pre></div>    </div>

    <p>We can now get the values of these variables by calling the <code class="highlighter-rouge">get_variable_value(name)</code> method where <code class="highlighter-rouge">name</code> is one of the names returned by <code class="highlighter-rouge">get_variabale_names()</code>. For example,</p>
    <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code>  <span class="n">linear_regressor</span><span class="o">.</span><span class="n">get_variable_value</span><span class="p">(</span><span class="s">'linear/linear_model/{}/</span><span class="err">
</span><span class="s">                                      weights'</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="s">'total_rooms'</span><span class="p">))</span>
</code></pre></div>    </div>
    <p>returns <code class="highlighter-rouge">array([[4.999998e-05]], dtype=float32)</code> as the output.</p>
  </li>
</ul>

<h4 id="task-1-creating-my-own-synthetic-feature">Task 1: Creating my own synthetic feature</h4>

<p>The idea here is that <code class="highlighter-rouge">total_rooms</code> and <code class="highlighter-rouge">population</code> are both reported on a per city block basis. But some city blocks might have a lot of people for the same number of rooms, so the number of rooms per person is in a sense a better indicator of house value. So perhaps that would make a better feature.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">california_housing_dataframe</span><span class="p">[</span><span class="s">"rooms_per_person"</span><span class="p">]</span> <span class="o">=</span> <span class="p">(</span><span class="n">california_housing_dataframe</span><span class="p">[</span><span class="s">'total_rooms'</span><span class="p">]</span> <span class="o">/</span> 
                                                   <span class="n">california_housing_dataframe</span><span class="p">[</span><span class="s">'population'</span><span class="p">])</span>

<span class="n">calibration_data</span> <span class="o">=</span> <span class="n">train_model</span><span class="p">(</span>
    <span class="n">learning_rate</span><span class="o">=</span><span class="mf">0.05</span><span class="p">,</span>
    <span class="n">steps</span><span class="o">=</span><span class="mi">500</span><span class="p">,</span>
    <span class="n">batch_size</span><span class="o">=</span><span class="mi">5</span><span class="p">,</span>
    <span class="n">input_feature</span><span class="o">=</span><span class="s">"rooms_per_person"</span>
<span class="p">)</span>
</code></pre></div></div>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Training model...
RMSE (on training data):
  period 00 : 212.73
  period 01 : 189.62
  period 02 : 171.05
  period 03 : 153.35
  period 04 : 141.30
  period 05 : 135.36
  period 06 : 131.91
  period 07 : 130.58
  period 08 : 130.18
  period 09 : 130.61
Model training finished.
Final RMSE (on training data): 130.61
</code></pre></div></div>

<p><img src="/assets/images/coursenotes/GoogleML/output_33_1.png" alt="png" width="100%" class="align-center" /></p>

<h4 id="task-2-identify-outliers">Task 2: Identify outliers</h4>

<p>When we plot the predictions against the actual <code class="highlighter-rouge">median_house_value</code>, ideally we should see a straight line with slope 1. Do we really see that, and does that help us identify outliers?</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">plt</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">calibration_data</span><span class="p">[</span><span class="s">'targets'</span><span class="p">],</span> <span class="n">calibration_data</span><span class="p">[</span><span class="s">'predictions'</span><span class="p">])</span>
</code></pre></div></div>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>&lt;matplotlib.collections.PathCollection at 0x1a3a855cc0&gt;
</code></pre></div></div>

<p><img src="/assets/images/coursenotes/GoogleML/output_35_1.png" alt="png" width="75%" class="align-center" /></p>

<p>A histogram might help to see how the data is distributed</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">california_housing_dataframe</span><span class="p">[</span><span class="s">'rooms_per_person'</span><span class="p">]</span><span class="o">.</span><span class="n">hist</span><span class="p">(</span><span class="n">bins</span><span class="o">=</span><span class="mi">20</span><span class="p">)</span>
</code></pre></div></div>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>&lt;matplotlib.axes._subplots.AxesSubplot at 0x1a3a8324a8&gt;
</code></pre></div></div>

<p><img src="/assets/images/coursenotes/GoogleML/output_37_1.png" alt="png" width="75%" class="align-center" /></p>

<p>It looks like there are some small set of data points for <code class="highlighter-rouge">rooms_per_person</code> that is very large and might be throwing off the fit, so we will clip those</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">criterion</span> <span class="o">=</span> <span class="n">california_housing_dataframe</span><span class="p">[</span><span class="s">'rooms_per_person'</span><span class="p">]</span> <span class="o">&lt;</span> <span class="mi">4</span>
<span class="n">california_housing_dataframe</span> <span class="o">=</span> <span class="n">california_housing_dataframe</span><span class="p">[</span><span class="n">criterion</span><span class="p">]</span>
<span class="n">california_housing_dataframe</span><span class="o">.</span><span class="n">hist</span><span class="p">(</span><span class="s">'rooms_per_person'</span><span class="p">)</span>
</code></pre></div></div>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>array([[&lt;matplotlib.axes._subplots.AxesSubplot object at 0x1a3ce3d320&gt;]],
      dtype=object)
</code></pre></div></div>

<p><img src="/assets/images/coursenotes/GoogleML/output_39_1.png" alt="png" width="75%" class="align-center" /></p>

<p>This looks much more reasonable. We now follow the fitting procedure defined above by calling <code class="highlighter-rouge">train_model</code></p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">calibration_data</span> <span class="o">=</span> <span class="n">train_model</span><span class="p">(</span><span class="n">learning_rate</span><span class="o">=</span><span class="mf">0.05</span><span class="p">,</span>
    <span class="n">steps</span><span class="o">=</span><span class="mi">500</span><span class="p">,</span>
    <span class="n">batch_size</span><span class="o">=</span><span class="mi">5</span><span class="p">,</span>
    <span class="n">input_feature</span><span class="o">=</span><span class="s">"rooms_per_person"</span>
<span class="p">)</span>
</code></pre></div></div>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Training model...
RMSE (on training data):
  period 00 : 213.27
  period 01 : 189.60
  period 02 : 167.16
  period 03 : 147.02
  period 04 : 130.93
  period 05 : 118.40
  period 06 : 111.94
  period 07 : 109.36
  period 08 : 107.38
  period 09 : 105.99
Model training finished.
Final RMSE (on training data): 105.99
</code></pre></div></div>

<p><img src="/assets/images/coursenotes/GoogleML/output_41_1.png" alt="png" width="100%" class="align-center" /></p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c"># Replot the historgram and the parity plot to see if the plot looks any better now</span>

<span class="n">fig</span><span class="p">,</span> <span class="n">axs</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span><span class="mi">4</span><span class="p">))</span>
<span class="n">axs</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">hist</span><span class="p">(</span><span class="n">california_housing_dataframe</span><span class="p">[</span><span class="s">'rooms_per_person'</span><span class="p">])</span>
<span class="n">axs</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">calibration_data</span><span class="p">[</span><span class="s">'targets'</span><span class="p">],</span> <span class="n">calibration_data</span><span class="p">[</span><span class="s">'predictions'</span><span class="p">])</span>
</code></pre></div></div>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>&lt;matplotlib.collections.PathCollection at 0x1a3fcfad68&gt;
</code></pre></div></div>

<p><img src="/assets/images/coursenotes/GoogleML/output_42_1.png" alt="png" width="100%" class="align-center" /></p>


        
      </section>

      <footer class="page__meta">
        
        


        
      </footer>

      

      
  <nav class="pagination">
    
      <a href="/coursenotes/CFDpython/" class="pagination--pager" title="Computational Fluid Dynamics Python (CFD) - Python
">Previous</a>
    
    
      <a href="/coursenotes/machinelearning/" class="pagination--pager" title="Machine Learning with Python - From Linear Models to Deep Learning
">Next</a>
    
  </nav>

    </div>

    
  </article>

  
  
</div>

    </div>

    
      <div class="search-content">
        <div class="search-content__inner-wrap"><input type="text" id="search" class="search-input" tabindex="-1" placeholder="Enter your search term..." />
    <div id="results" class="results"></div></div>

      </div>
    

    <div class="page__footer">
      <footer>
        <!-- start custom footer snippets -->

<!-- end custom footer snippets -->
        <div class="page__footer-follow">
  <ul class="social-icons">
    
      <li><strong>Follow:</strong></li>
    

    
      
        
      
    

    <li><a href="/feed.xml"><i class="fas fa-fw fa-rss-square" aria-hidden="true"></i> Feed</a></li>
  </ul>
</div>

<div class="page__footer-copyright">&copy; 2019 Aditya Jaishankar. Powered by <a href="https://jekyllrb.com" rel="nofollow">Jekyll</a> &amp; <a href="https://mademistakes.com/work/minimal-mistakes-jekyll-theme/" rel="nofollow">Minimal Mistakes</a>.</div>

      </footer>
    </div>

    
  <script src="/assets/js/main.min.js"></script>
  <script defer src="https://use.fontawesome.com/releases/v5.6.0/js/all.js" integrity="sha384-z9ZOvGHHo21RqN5De4rfJMoAxYpaVoiYhuJXPyVmSs8yn20IE3PmBM534CffwSJI" crossorigin="anonymous"></script>




<script src="/assets/js/lunr/lunr.min.js"></script>
<script src="/assets/js/lunr/lunr-store.js"></script>
<script src="/assets/js/lunr/lunr-en.js"></script>








<script type="text/javascript"
        src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.0/MathJax.js?config=TeX-AMS_CHTML"></script>

<script type="text/x-mathjax-config">
MathJax.Hub.Config({
tex2jax: {
inlineMath: [['$','$'], ['\\(','\\)']],
processEscapes: true},
jax: ["input/TeX","input/MathML","input/AsciiMath","output/CommonHTML"],
extensions: ["tex2jax.js","mml2jax.js","asciimath2jax.js","MathMenu.js","MathZoom.js","AssistiveMML.js", "[Contrib]/a11y/accessibility-menu.js"],
TeX: {
extensions: ["AMSmath.js","AMSsymbols.js","noErrors.js","noUndefined.js"],
equationNumbers: {
autoNumber: "AMS"
}
}
});
</script>


  </body>
</html>
