<!doctype html>
<!--
  Minimal Mistakes Jekyll Theme 4.15.1 by Michael Rose
  Copyright 2013-2019 Michael Rose - mademistakes.com | @mmistakes
  Free for personal and commercial use under the MIT license
  https://github.com/mmistakes/minimal-mistakes/blob/master/LICENSE
-->
<html lang="en" class="no-js">
  <head>
    <meta charset="utf-8">

<!-- begin _includes/seo.html --><title>Fundamentals of Statistics(18.6501x) | Aditya Jaishankar</title>
<meta name="description" content="Unit 1: Introduction to Statistics">



<meta property="og:type" content="article">
<meta property="og:locale" content="en_US">
<meta property="og:site_name" content="Aditya Jaishankar">
<meta property="og:title" content="Fundamentals of Statistics(18.6501x)">
<meta property="og:url" content="http://localhost:4000/coursenotes/statistics/">


  <meta property="og:description" content="Unit 1: Introduction to Statistics">







  <meta property="article:published_time" content="2019-02-01T00:00:00-04:00">





  

  


<link rel="canonical" href="http://localhost:4000/coursenotes/statistics/">







  <script type="application/ld+json">
    {
      "@context": "https://schema.org",
      "@type": "Person",
      "name": "Aditya Jaishankar",
      "url": "http://localhost:4000",
      "sameAs": null
    }
  </script>







<!-- end _includes/seo.html -->


<link href="/feed.xml" type="application/atom+xml" rel="alternate" title="Aditya Jaishankar Feed">

<!-- https://t.co/dKP3o1e -->
<meta name="viewport" content="width=device-width, initial-scale=1.0">

<script>
  document.documentElement.className = document.documentElement.className.replace(/\bno-js\b/g, '') + ' js ';
</script>

<!-- For all browsers -->
<link rel="stylesheet" href="/assets/css/main.css">

<!--[if IE ]>
  <style>
    /* old IE unsupported flexbox fixes */
    .greedy-nav .site-title {
      padding-right: 3em;
    }
    .greedy-nav button {
      position: absolute;
      top: 0;
      right: 0;
      height: 100%;
    }
  </style>
<![endif]-->



    <!-- start custom head snippets -->

<!-- insert favicons. use https://realfavicongenerator.net/ -->







<link href="https://fonts.googleapis.com/css?family=Crimson+Text" rel="stylesheet">





<!-- end custom head snippets -->

  </head>

  <body class="layout--single">

    <!--[if lt IE 9]>
<div class="notice--danger align-center" style="margin: 0;">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience.</div>
<![endif]-->

    

<div class="masthead">
  <div class="masthead__inner-wrap">
    <div class="masthead__menu">
      <nav id="site-nav" class="greedy-nav">
        
        <a class="site-title" href="/"> </a>
        <ul class="visible-links"><li class="masthead__menu-item">
              <a href="/" >Home</a>
            </li><li class="masthead__menu-item">
              <a href="/coursework/" >Coursework</a>
            </li><li class="masthead__menu-item">
              <a href="/projects/" >Projects</a>
            </li><li class="masthead__menu-item">
              <a href="/research/" >Ph.D. Research</a>
            </li><li class="masthead__menu-item">
              <a href="/publications/" >Publications</a>
            </li></ul>
        
        <button class="search__toggle" type="button">
          <span class="visually-hidden">Toggle search</span>
          <svg class="icon" width="16" height="16" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 15.99 16">
            <path d="M15.5,13.12L13.19,10.8a1.69,1.69,0,0,0-1.28-.55l-0.06-.06A6.5,6.5,0,0,0,5.77,0,6.5,6.5,0,0,0,2.46,11.59a6.47,6.47,0,0,0,7.74.26l0.05,0.05a1.65,1.65,0,0,0,.5,1.24l2.38,2.38A1.68,1.68,0,0,0,15.5,13.12ZM6.4,2A4.41,4.41,0,1,1,2,6.4,4.43,4.43,0,0,1,6.4,2Z" transform="translate(-.01)"></path>
          </svg>
        </button>
        
        <button class="greedy-nav__toggle hidden" type="button">
          <span class="visually-hidden">Toggle menu</span>
          <div class="navicon"></div>
        </button>
        <ul class="hidden-links hidden"></ul>
      </nav>
    </div>
  </div>
</div>


    <div class="initial-content">
      


  
    



<nav class="breadcrumbs">
  <ol itemscope itemtype="https://schema.org/BreadcrumbList">
    
    
    
      
        <li itemprop="itemListElement" itemscope itemtype="https://schema.org/ListItem">
          <a href="http://localhost:4000/" itemprop="item"><span itemprop="name">Home</span></a>
          <meta itemprop="position" content="1" />
        </li>
        <span class="sep">/</span>
      
      
        
        <li itemprop="itemListElement" itemscope itemtype="https://schema.org/ListItem">
          <a href="/coursenotes" itemprop="item"><span itemprop="name">Coursenotes</span></a>
          <meta itemprop="position" content="2" />
        </li>
        <span class="sep">/</span>
      
    
      
      
        <li class="current">Fundamentals of Statistics(18.6501x)</li>
      
    
  </ol>
</nav>

  


<div id="main" role="main">
  
  <div class="sidebar sticky">
  


<div itemscope itemtype="https://schema.org/Person">

  
    <div class="author__avatar">
      

      
        <img src="/assets/images/bio-picture.jpg" alt="Aditya Jaishankar" itemprop="image">
      
    </div>
  

  <div class="author__content">
    
      <h3 class="author__name" itemprop="name">Aditya Jaishankar</h3>
    
    
      <p class="author__bio" itemprop="description">
        <b>Ph.D., Massachusetts Institute of Technology.</b><br>Soft Condensed Matter Physicist and Surface Scientist.
      </p>
    
  </div>

  <div class="author__urls-wrapper">
    <button class="btn btn--inverse">Follow</button>
    <ul class="author__urls social-icons">
      
        <li itemprop="homeLocation" itemscope itemtype="https://schema.org/Place">
          <i class="fas fa-fw fa-map-marker-alt" aria-hidden="true"></i> <span itemprop="name">New Jersey, USA</span>
        </li>
      

      
        
          
        
          
            <li><a href="https://www.github.com/aditya-jaishankar/" rel="nofollow noopener noreferrer"><i class="fab fa-fw fa-github" aria-hidden="true"></i> GitHub</a></li>
          
        
          
            <li><a href="https://www.linkedin.com/in/adityajaishankar" rel="nofollow noopener noreferrer"><i class="fab fa-linkedin" aria-hidden="true"></i> LinkedIn</a></li>
          
        
          
            <li><a href="https://dspace.mit.edu/handle/1721.1/92159" rel="nofollow noopener noreferrer"><i class="fas fa-file-alt" aria-hidden="true"></i> Resume</a></li>
          
        
      

      

      
        <li>
          <a href="mailto:aditya1642@gmail.com">
            <meta itemprop="email" content="aditya1642@gmail.com" />
            <i class="fas fa-fw fa-envelope-square" aria-hidden="true"></i> Email
          </a>
        </li>
      

      

      

      

      

      

      

      

      

      

      

      

      

      

      

      

      

      

      

      

      

      

      

      

      <!--
  <li>
    <a href="http://link-to-whatever-social-network.com/user/" itemprop="sameAs" rel="nofollow noopener noreferrer">
      <i class="fas fa-fw" aria-hidden="true"></i> Custom Social Profile Link
    </a>
  </li>
-->
    </ul>
  </div>
</div>

  
  </div>


  <article class="page" itemscope itemtype="https://schema.org/CreativeWork">
    <meta itemprop="headline" content="Fundamentals of Statistics(18.6501x)">
    <meta itemprop="description" content="Unit 1: Introduction to Statistics">
    <meta itemprop="datePublished" content="February 01, 2019">
    

    <div class="page__inner-wrap">
      
        <header>
          <h1 id="page-title" class="page__title" itemprop="headline">Fundamentals of Statistics(18.6501x)
</h1>
          
            <p class="page__meta"><i class="far fa-clock" aria-hidden="true"></i> 




  62 minute read
</p>
          
        </header>
      

      <section class="page__content" itemprop="text">
        
          <aside class="sidebar__right ">
            <nav class="toc">
              <header><h4 class="nav__title"><i class="fas fa-file-alt"></i> On this page</h4></header>
              <ul class="toc__menu">
  <li><a href="#unit-1-introduction-to-statistics">Unit 1: Introduction to Statistics</a>
    <ul>
      <li><a href="#lecture-1-what-is-statistics">Lecture 1: What is Statistics</a></li>
      <li><a href="#lecture-2-probability-redux">Lecture 2: Probability redux</a></li>
    </ul>
  </li>
  <li><a href="#unit-2-parametric-inference">Unit 2: Parametric Inference</a>
    <ul>
      <li><a href="#parametric-statistical-models">Parametric Statistical Models</a></li>
      <li><a href="#parametric-estimation-and-confidence-intervals">Parametric estimation and confidence intervals</a></li>
      <li><a href="#lecture-5-delta-method-and-confidence-intervals">Lecture 5: Delta method and confidence intervals</a></li>
      <li><a href="#lecture-6-introduction-to-hypothesis-testing-and-type-1-and-type-2-errors">Lecture 6: Introduction to hypothesis testing, and type 1 and type 2 errors</a></li>
    </ul>
  </li>
  <li><a href="#unit-3-methods-of-estimation">Unit 3: Methods of estimation</a>
    <ul>
      <li><a href="#lecture-8-distance-measures-between-distributions">Lecture 8: Distance measures between distributions</a></li>
      <li><a href="#lecture-9-introduction-to-maximum-likelihood-estimation">Lecture 9: Introduction to Maximum Likelihood Estimation</a></li>
      <li><a href="#lecture-10-consistency-of-mle-covariance-matrices-multivariate-statistics">Lecture 10: Consistency of MLE, covariance matrices, multivariate statistics</a></li>
      <li><a href="#lecture-11-fisher-information-asymptotic-normality-of-mle-method-of-moments">Lecture 11: Fisher Information, Asymptotic Normality of MLE, Method of Moments</a></li>
      <li><a href="#lecture-12-m-estimation">Lecture 12: M-Estimation</a></li>
    </ul>
  </li>
  <li><a href="#unit-4-hypothesis-testing">Unit 4: Hypothesis Testing</a>
    <ul>
      <li><a href="#lecture-13-chi-squared-distribution-t-test">Lecture 13: Chi squared distribution, T-test</a></li>
      <li><a href="#lecture-12-goodness-of-fit-tests-for-discrete-distributions">Lecture 12: Goodness of fit tests for discrete distributions</a></li>
      <li><a href="#lecture-12-goodness-of-fit-tests-for-discrete-distributions-1">Lecture 12: Goodness of fit tests for discrete distributions</a></li>
      <li><a href="#goodness-of-fit-tests-for-continuous-distributions">Goodness of fit tests for continuous distributions</a></li>
    </ul>
  </li>
  <li><a href="#unit-5-bayesian-statistics">Unit 5: Bayesian Statistics</a>
    <ul>
      <li><a href="#lecture-17-introduction-to-bayesian-statistics">Lecture 17: Introduction to Bayesian Statistics</a></li>
    </ul>
  </li>
  <li><a href="#unit-6-linear-regression">Unit 6: Linear Regression</a></li>
  <li><a href="#unit-7-generalized-linear-models">Unit 7: Generalized Linear Models</a>
    <ul>
      <li><a href="#lecture-21-introduction-exponential-families">Lecture 21: Introduction, Exponential Families</a></li>
    </ul>
  </li>
</ul>
            </nav>
          </aside>
        
        <h2 id="unit-1-introduction-to-statistics">Unit 1: Introduction to Statistics</h2>

<h3 id="lecture-1-what-is-statistics">Lecture 1: What is Statistics</h3>

<ul>
  <li>
    <p>Statistics is at the core, is fundamental to machine learning and data science. Without a thorough grounding in statistics, machine learning becomes a black box.</p>
  </li>
  <li>
    <p>There is the <strong>computational view</strong> of data: where problems are solved with very large amounts of data, and techniques such as spectral methods, low dimensional embedding (visualizing high dimensional data), distributed optimization, etc. Then there is the <strong>statistical view</strong> of data, where data is treated as a random variable, and each data point is the realization of a random variable, i.e., they are outcomes of a random process. Statistics essentially allows you to answer the question of, given this small amount of data, what accurate statements can I make about the case if I had an infinite amount of data.</p>
  </li>
  <li>
    <p>Probability and Statistics go hand in hand. Given the truth, probability lets us calculate what kind of data we should expect. Statistics on the other hand is the reverse - given a set of observations (data) what can we say about the truth, and with what level of certainty can we say that? Statistics is the process of trying to determine the truth from a set of observations.</p>
  </li>
</ul>

<h3 id="lecture-2-probability-redux">Lecture 2: Probability redux</h3>

<p><strong>Let us recall some basic inequalities, in order of conservatism</strong></p>

<p>In all of this, we assume that $\bar{X} _n$ is the sample mean, each of the $X_i$ are i.i.d r.v’s.</p>

<ul>
  <li>
    <p>Markov’s inequality:</p>

    <p>Given a non negative random varialbe $X$, the inequality states that</p>

    <script type="math/tex; mode=display">\begin{align}
      \mathbf{P}(X \geq a) \leq  \dfrac{\mathbf{E}[X]}{a}
  \end{align}</script>
  </li>
  <li>
    <p>Chebychev’s inequality:</p>

    <p>If $X$ is a random variable with <em>finite expected value</em> $\mu$ and non-zero variance $\sigma^2$ (can be unbounded), then for some $\varepsilon &gt; 0$,</p>

    <script type="math/tex; mode=display">\begin{align}
      \mathbf{P}(\lvert X - \mu \rvert > \varepsilon) \leq \dfrac{\sigma^2}{\varepsilon^2}
  \end{align}</script>

    <p>Note that the weak law of large numbers (that the sample mean converges in probability to the mean of the distribution) follows easily from the application of Chebychev’s inequality.</p>
  </li>
  <li>
    <p>Hoeffding’s inequality:</p>

    <p>Let $X_1, X_2, \ldots, X_n$ be independent identically distributed random variables that are almost surely bounded on the interval $[a, b]$. Further, let $\bar{X} _n$ be the sample mean. Then, the inequality states that:</p>

    <script type="math/tex; mode=display">\begin{align}
      \mathbf{P}(\bar{X} _n - \mathbf{E}[X] \geq \epsilon) \leq 2\exp\left(-\dfrac{2 n \epsilon^2}{(b - a)^2}\right)
  \end{align}</script>

    <p>for all $\epsilon &gt; 0$. The sample size $n$ does not need to be large.</p>
  </li>
  <li>
    <p>Central limit theorem:</p>

    <p>Let $X_1, X_2, \ldots, X_n$ be independent identically distributed random variables, with <em>finite</em> mean $\mu$ and <em>finite</em> variance $\sigma^2$. The CLT states that</p>

    <script type="math/tex; mode=display">\begin{align}
  \lim _{n \to \infty} \mathbf{P}\left(\dfrac{\sum X_i -n\mu}{\sqrt{n}\sigma} \leq z \right) = \Phi(z)
  \end{align}</script>

    <p>where $\Phi(\cdot)$ denotes the CDF of the standard normal.</p>
  </li>
  <li>
    <p>The conservatism decreases in the order shown because as we go down the list, we start to know more and more about the random variables (markov - only the mean, Chebychev - mean and variance, Hoeffding - mean and boundedness). In the case of the CLT, we have an infinite number of random variables which gives us the tightest bounds.</p>
  </li>
  <li>
    <p>Affine transformation of the Gaussian distribution: If $X \sim N(\mu, \sigma^2)$, then $aX + b \sim N(a\mu + b, a^2\sigma^2)$.</p>
  </li>
  <li>
    <p>Standardization of the Gaussian distribution: If $X \sim N(\mu, \sigma^2)$, then $Z = (X - \mu)/\sigma$ (sometimes called the $Z$ score) is a standard normal.</p>
  </li>
  <li>
    <p>Symmetry of the Gaussian distribution: If $X \sim N(0, \sigma^2)$ then $-X \sim N(0, \sigma^2)$. The probability distributions are the same.</p>
  </li>
  <li>
    <p>The quantile of a random variable $X$ of order $1-\alpha$ is the number $q_\alpha$ such that $\mathbf{P}(X \geq q_\alpha) = \alpha$ i.e., $\mathbf{P}(X \leq q_\alpha) = 1 - \alpha$.</p>
  </li>
  <li>
    <p>Can you recall convergence almost surely, convergence in probability, and convergence in distribution. Convergence a.s. is the strongest, then convergence in probability theorem, then convergence in distribution. Can you explain these intuitively? See <a href="https://stats.stackexchange.com/questions/2230/convergence-in-probability-vs-almost-sure-convergence">this</a> discussion for a really nice intutive discussion if you cannot recall.</p>
  </li>
  <li>
    <p>If $T_n \xrightarrow[n \to \infty]{\textrm{a.s}/ \mathbf{P}} T$ and $U_n \xrightarrow[n \to \infty]{\textrm{a.s}, \mathbf{P}} U$, then things are as you would expect and</p>

    <script type="math/tex; mode=display">% <![CDATA[
\begin{align}
      T_n \cdot U_n & \xrightarrow[n \to \infty]{\textrm{a.s}/ \mathbf{P}} T U\\
      T_n + U_n & \xrightarrow[n \to \infty]{\textrm{a.s}/ \mathbf{P}} T + U \\
      T_n / U_n & \xrightarrow[n \to \infty]{\textrm{a.s}/ \mathbf{P}} T/U, U \neq 0
  \end{align} %]]></script>
  </li>
  <li>
    <p>The above is in general not true for convergence in distribution! However, <strong>Slutsky’s theorem</strong> states that If $T_n \xrightarrow[n \to \infty]{(d)} T$ and $U_n \xrightarrow[n \to \infty]{\mathbf{P}} u$, where $u$ is a constant number, then the above results are true (with the replacement $U \to u$).</p>
  </li>
  <li>
    <p><strong>Continuous mapping theorem:</strong> If $T_n \xrightarrow[n \to \infty]{\textrm{a.s}/ \mathbf{P}/(d)} T$, then $f(T_n) \xrightarrow[n \to \infty]{\textrm{a.s}/ \mathbf{P}/(d)} f(T)$, if the function $f$ is continuous.</p>
  </li>
</ul>

<h2 id="unit-2-parametric-inference">Unit 2: Parametric Inference</h2>

<h3 id="parametric-statistical-models">Parametric Statistical Models</h3>

<ul>
  <li>
    <p>The <strong>holy trinity of statistical inference</strong> is 1. Estimation, 2. Confidence intervals, and 3. Hypothesis Testing.</p>
  </li>
  <li>
    <p><strong>Definition - Statistical Model</strong>: A statistical model is defined in terms of a pair</p>

    <script type="math/tex; mode=display">\begin{align}
  (E, \{\mathbb{P} _\theta\}_{\theta \in \Theta}) \label{eqn:modeldefn}
  \end{align}</script>

    <p>where $E$ is a measureable space known as the sample space (i.e. the set of values the random variable in question can take), $\{\mathbb{P}_\theta\}$ is a family of probability distributions with parameters $\theta$. The domain (space) of possible values that $\theta$ can take is denoted by $\Theta$. For example, for a Bernoulli distribution, $E = \{0, 1\}$, $\theta$ is the parameter of the distribution, and $\Theta$ is the interval $[0, 1]$.</p>
  </li>
  <li>
    <p>Here we will only study models that are well-specified, i.e., if $\mathbb{P}$ is the true distribution that generated the data, then if the model is well-specified, there exists $\theta \in \Theta$ such that $\mathbb{P}_\theta = \mathbb{P}$. The value of $\theta$ is unknown (also sometimes called the true parameter) and the aim of the statistical experiment is to estimate $\theta$.</p>
  </li>
  <li>
    <p>$\Theta \subseteq \mathbb{R}^d, d \geq 1$ in which case the model is called parametric. Sometimes, $\Theta$ can be infinite dimensional, in which case the model is called non-parametric.  In other words, a statistical model is called parametric if all parameters $\theta \in \Theta$ is specified by a finite number of unknowns. In particular, $\Theta \subset \mathbb{R}^m$, and the family of probability distributions $\{ \mathbb{P}_\theta \}$ is specified by the $m$ components of the vector $\theta$.</p>
  </li>
  <li>
    <p>To summarize: A statistical model consists of a sample space $E$ and a family of probability models $\{\mathbb{P}_\theta\}$ indexed by the parameter $\theta$, where $\theta \in \Theta$. The notation for denoting a statistical model is given in equation \ref{eqn:modeldefn}. <em>Note that the sample space cannot contain any unknown parameters; for example in a uniform distribution $U(0,a)$ with $a&gt;0$ unknown, I cannot say that the sample space is $(0, a)$ because $a$ is unknown.</em></p>
  </li>
  <li>
    <p><strong>Examples of statistical model notation:</strong></p>

    <p>Bernoulli Trials: $(\{0, 1\}, \{\textrm{Ber}(p)\} _{p \in (0, 1)})$.</p>

    <p>Poisson Model: $(\mathbb{N}^0, \{\textrm{Poiss}(\lambda)\} _{\lambda \in \mathbb{R}^+})$</p>

    <p>If $X_1, \ldots, X_n$ (i.i.d) $\tilde N(\mu, \sigma^2)$, for $\mu \in \mathbb{R}$ and $\sigma^2 &gt; 0$., $(\mathbb{R}, \{N(\mu, \sigma^2)\} _{(\mu, \sigma^2) \in \mathbb{R} \times (0, \infty)})$.</p>

    <p>There are also more complicated models for example Linear Regression. In this model, we have $(X_1, Y_1), ldots,(X_n, Y_n) \in \mathbb{R}^d \times \mathbb{R}$, with $Y_i = \beta^T X_i + \epsilon_i$, $\epsilon_i \sim N(0, 1)$ and $\beta \subset \mathbb{R}^d$, $X_i \sim N(0, I_d)$ where $I_d$ is the identity vector. You see that the sample space and $\Theta$ are easy to write out in this case, but $\mathbb{P} _\theta$ is significantly harder. We will need to first find $f_{Y_i \lvert X_i}(y_i \lvert x_i)$ and then determine $f_{X_i}(x_i)$ through standard techniques learned in the probability class (start with the CDF and then differentiate).</p>
  </li>
  <li>
    <p>Identifiability: A statistical model is said to be identifiable if $\theta \neq \theta’ \Rightarrow \mathbb{P}_\theta \neq \mathbb{P}_\theta’$. In other words, given the probability distribution,  I can uniquely identify the parameter set.</p>
  </li>
  <li>
    <p>In general, when trying to understand identifiability of a parameter in a parametric model, try and calculate the CDF or the PDF of and see if the pdf is an injective function i.e. if $f(x) = f(y) \Rightarrow x = y$.</p>
  </li>
</ul>

<h3 id="parametric-estimation-and-confidence-intervals">Parametric estimation and confidence intervals</h3>

<ul>
  <li>
    <p><strong>Definitions:</strong></p>

    <ul>
      <li>
        <p>Statistic: Any measureable function of the samples, for example $\bar{X}_n$ or $X_1 + \log (1+X_n^2)$ and so on. <em>Measureable</em> just says that given data, I should be able to compute the value</p>
      </li>
      <li>
        <p>Estimator: Any statistic that does not depend on the parameter $\theta$. The goal is designing an estimator is that I want to find a distribution that is as close as possible to the true distribution. I want to ensure that $\hat{\theta}_n$ converges to $\theta$ given enough samples.</p>
      </li>
      <li>
        <p>Weak consistency: An estimator $\hat{\theta}_n$ of $\theta$ is said to be weakly consistent if $\theta_n \xrightarrow[n \to \infty]{\mathbb{P}} \theta$.</p>
      </li>
      <li>
        <p>Strong consistency: An estimator $\hat{\theta}$ of $\theta$ is said to be strongly consistent if $\theta_n \xrightarrow[n \to \infty]{\textrm{a.s.}} \theta$.</p>
      </li>
      <li>
        <p>Asymptotic normality: An estimator $\hat{\theta}_n$ of $\theta$ is said to be asymptotically normal if</p>
      </li>
    </ul>

    <script type="math/tex; mode=display">\begin{align}
      \sqrt{n}(\hat{\theta}_n - \theta) \xrightarrow[n \to \infty]{(d)} N(0,\sigma^2)
  \end{align}</script>

    <p>Here, $\sigma^2$ is called its asymptotic variance.</p>
  </li>
  <li>
    <p>Bias of an estimator: The bias of an esitmator $\hat{\theta}_n$ of $\theta$, denoted by $\textrm{bias}(\hat{\theta}_n)$ is defined by $\mathbb{E}[\hat{\theta}_n] - \theta$. If the bias is 0, the estimator is called unbiased. Having an unbiased estimator is desirable, but should not be pursued at all consts. One could ontain an unbiased estimator but end up habving a very large variance for the estimator, so we it’s estimates might be fraught with uncertainty. Therefore, while judging how good estimators are, we also need to compute the variance of estimators.</p>
  </li>
  <li>
    <p>Jensen’s inequality: If $g(\cdot)$ is a convex function, then $g(\mathbf{E}[X]) \leq \mathbf{E}[g(X)]$. For a concave function, this inequality flips in direction.</p>
  </li>
  <li>
    <p>Quadratic risk: This is a way to try and optimize both the bias and the variance of the estimator. It is defined as $\mathbb{E}[\lvert \hat{\theta}_n - \theta \rvert^2]$. It is easy to derive that this quantity is equal to $\textrm{var}(\hat{\theta}_n - \theta) + (\mathbb{E}[\hat{\theta}_n] - \theta)^2$ which is the sum of the variance and the square of the bias. Here is an interesting insight: if the quadratic risk of $\hat{\theta}_n \xrightarrow[n \to \infty]{ } 0$, then the sum of the variance and the square of the bias goes to 0. If the sum of two non-negative quantites goes to zero, each must go to zero. This implies that th variance tends to 0 and the expected value of $\hat{\theta}_n$ tends to $\theta$. So the peak in the distribution becomes narrower and narrower, the location of the peak approaches $\theta$. This means that the estimator converges in probability to the parameter $\theta$.</p>
  </li>
  <li>Note: There are lots of potential advantages of using the sample mean as an estimator:
    <ul>
      <li>
        <p><em>It is consistent</em>, that is, $\hat{\theta}_n = \bar{X}_n \xrightarrow[n \to \infty] \theta$. Using Chebychev, it is easy show that $\bar{X}_n$ converges in probability to $\theta$. It is also trivial to show that the variance goes to 0 with increasing $n$ (assuming iid). So we have a distribution centered at $\theta$ with zero variance, so it is $\theta$ itself.</p>
      </li>
      <li>
        <p><em>It is unbiased</em>: Use linearity of expectations.</p>
      </li>
      <li>
        <p>It is <em>efficiently computable</em>: Given the samples $X_i$ it is very easy to calculate.</p>
      </li>
      <li>
        <p>The quadratic risk goes to 0 as $n \to \infty$. Both the variance and the bias go to 0 as $n \to \infty$ so this statement trivially follows.</p>
      </li>
    </ul>
  </li>
  <li>
    <p>Calculating confidence intervals can be tricky because the requirement is that the confidence interval should not contain any parameters. The confidence interval cannot be a function of the parameters. If we were to simply use the central limit theorem, we include the standard deviation of the probabilistic model, and for example if we were using a Bernoulli model, then $p$ appears in th calculation for the confidence interval. i.e. I would want to solve for $x$ such that</p>

    <script type="math/tex; mode=display">\begin{align}
      \mathbb{P}(\lvert \bar{R}_n - p \rvert \geq x) = 2\left(1 - \Phi\left(\dfrac{\sqrt{n}x}{\sqrt{p(1-p)}}\right)\right) = \alpha
  \end{align}</script>

    <p>and $p$ now appears. So how do we get around this?</p>
  </li>
  <li>
    <p>To get around this, we need to remind ourselves of quantiles, set up the problem and the notation and then proceed to see what we are going to do. Recall that the $q_{\alpha/2}$ is the $1-(\alpha/2)$ quantile of $X \sim N(0,1)$ if $\mathbb{P}(X &gt; q_{\alpha/2}) = \alpha/2$. Now in this case we are interested in the random variable $X = \bar{R_n} - p$, i.e.,  how far our estimator is from our the estimate we are trying to determine. Therefore,</p>

    <script type="math/tex; mode=display">% <![CDATA[
\begin{align}
      \mathbb{P}(\lvert \bar{R}_n - p \rvert \leq q_{\alpha/2}) & = 1- \alpha
  \end{align} %]]></script>

    <p>Note that the factor of 3 disappears on the right hand side when we remove the aboslute value symbol. The above equation after some algebra can be rearranged to</p>

    <script type="math/tex; mode=display">\begin{align}
      \mathbb{P} \left(\lvert \bar{R}_n - p \rvert \leq \dfrac{\sqrt{p(1-p)}q _{\alpha/2}}{\sqrt{n}}\right) = 1 - \alpha
  \end{align}</script>

    <p>In other words, with probability  $\approx 1 - \alpha$,</p>

    <script type="math/tex; mode=display">\begin{align}
      \bar{R}_n \in \left[p - \dfrac{\sqrt{p(1-p)}q _{\alpha/2}}{\sqrt{n}}, p + \dfrac{\sqrt{p(1-p)}q _{\alpha/2}}{\sqrt{n}} \right]
  \end{align}</script>
  </li>
  <li>
    <p>The first technique to eliminate $p$ is the <strong>conservative bound</strong>. We set $p$ to be its value at maximum variance. For the Bernoulli, $p=1/2$, and hence $\sqrt{p(1-p)} = 1/2$. Then we can say that with probability at least $1-\alpha$,</p>

    <script type="math/tex; mode=display">\begin{align}
      \bar{R}_n \in \left[p - \dfrac{q _{\alpha/2}}{2\sqrt{n}}, p + \dfrac{q _{\alpha/2}}{2\sqrt{n}} \right]
  \end{align}</script>

    <p>We therefore get the asymptotic confidence interval</p>

    <script type="math/tex; mode=display">\begin{align}
      \mathcal{I}_\textrm{conserv} = \left[\bar{R} _n - \dfrac{q _{\alpha/2}}{2\sqrt{n}}, \bar{R} _n + \dfrac{q_{\alpha/2}}{2\sqrt{n}} \right]
  \end{align}</script>

    <p>More precisely,</p>

    <script type="math/tex; mode=display">\begin{align}
      \lim_{n\to \infty} \mathbb{P}(\mathcal{I}_\textrm{conserv} \ni p) \geq 1 - \alpha
  \end{align}</script>
  </li>
  <li>
    <p>The second method is to actually solve for $p$ using a quadratic equation (for Bernoulli) or more complicated equations for other models. For Bernoulli, we get a new interval $\mathcal{I}_\textrm{solve} = [p_1, P_2]$, where $p_1, p_2$ are roots of the quadratic. More precisely, with this solution,</p>

    <script type="math/tex; mode=display">\begin{align}
      \lim_{n \to \infty} \mathbb{P}(\mathcal{I}_\textrm{solve} \ni p) = 1 - \alpha
  \end{align}</script>

    <p>Note that this is a strong equality. There are no approximations. We just require the central limit theorem to hold, which of course it does.</p>
  </li>
  <li>
    <p>The third method to get at a confidence interval that does not contain the parameter $p$ that we are trying to estimate is to to do a plug in. This proceeds as follows. Consider the quantity that I am interested in calculating</p>

    <script type="math/tex; mode=display">\begin{align}
      \dfrac{\sqrt{n}(\bar{R}_n - p)}{\sqrt{p(1-p)}}
  \end{align}</script>

    <p>Now form the law of large numbers, we know that $\bar{R}_n$ converges to $p$, i.e. $\hat{p} \xrightarrow[n \to \infty]{\mathbb{P}, \textrm{a.s.}} p$. Therefore, using the continuous mapping theorem,</p>

    <script type="math/tex; mode=display">\begin{align}
      \dfrac{\sqrt{\hat{p}(1-\hat{p})}}{\sqrt{p(1-p)}} \xrightarrow[n \to \infty] 1
  \end{align}</script>

    <p>Now, since this converges in proability (and almost surely), we can use Slutsky’s theorem:</p>

    <script type="math/tex; mode=display">% <![CDATA[
\begin{align}
      \dfrac{\sqrt{n}(\bar{R}_n - p)}{\sqrt{p(1-p)}} & = 
      \dfrac{\sqrt{n}(\bar{R}_n - p)}{\sqrt{\hat{p}(1-\hat{p})}}\cdot\dfrac{\sqrt{\hat{p}(1-\hat{p})}}{\sqrt{p(1-p)}} \\
      & \xrightarrow[n \to \infty]{(d)} N(0,1)
  \end{align} %]]></script>

    <p>So we get a new confidence interval $\mathcal{I}_\textrm{plug-in}$ given by</p>

    <script type="math/tex; mode=display">\begin{align}
      \mathcal{I}_\textrm{plug-in} = \left[\bar{R}_n - \dfrac{\sqrt{\hat{p}(1-\hat{p})}q _{\alpha/2}}{\sqrt{n}}, \bar{R}_n + \dfrac{\sqrt{\hat{p}(1-\hat{p})}q _{\alpha/2}}{\sqrt{n}} \right]
  \end{align}</script>

    <p>Such that</p>

    <script type="math/tex; mode=display">\begin{align}
      \lim_{n \to \infty} \mathbb{P}(\mathcal{I}_\textrm{plug-in} \ni p) = 1 - \alpha
  \end{align}</script>

    <p>A general note: <em>When replacing out things that you don’t know with things that you know by putting a hat on it, you need to think of slutsky and limits and the continuous mapping theorem. This is especially useful when calculating confidence intervals using</em> $\mathcal{I}_\textrm{plug-in}$.</p>
  </li>
  <li>
    <p>Summary: Given some unknown but fixed parameter $\theta \in \mathbb{R}$ for a parametric model and $X_1, \ldots, X_n$ distributed I.i.d. $\mathbb{P}_\theta$, the non-asymptotic 95% confidence interval of $p$ is an interval $\mathcal{I} = \mathcal{I}(X_1, \ldots, X_n)$ such that $\mathbf{P}(\mathcal{I} \ni \theta) \geq 95$%. It is important to note that $\mathcal{I}$ is random, because it depends on some function of random variables. One the other hand, the realization of a random variable is deterministic. Therefore, say I am given the interval $[0.37, 0.54]$ as a 95% confidence interval for $p$ and then asked for the probability that $p$ is in this interval, <em>it is not 95%</em>. In fact, this is a deterministic question. Either $p$ falls in this interval and the probability is 1, or $p$ falls outside this interval, and the probability is 0. So, when determining confidence intervals, we use a process to say that I constructed a random variable, and now I make a statement about the outcome of conducting an experiment on that random variable. Once the experiment has been carried out, there are no more probabilities involved. I just check to see if the high probability event happened, or the low probability event happened. Unlikely events happen all the time, so it is possible (but not probable) that the true value of $p$ does not lie in the 95% confidence interval. Once I get numbers for my estimator $\hat{\theta}$, the question becomes deterministic.</p>
  </li>
  <li>If asking whether $\theta \in [\hat{\theta}_1, \hat{\theta}_2]$ is a deterministic question, what does a 95% confidence interval really mean? The frequentist view of the confidence interval says that if I do an experiment over and over and over again i.e. I determine a whole bunch of confidence inervals $\mathcal{I}$, and further, if I define a Bernoulli random variable which takes value one if it turns out that $\mathcal{I} \ni \theta$, then the parameter of the Bernoulli random variable would be 0.95. In other words, I would find that 95% of the time, $\mathcal{I} \ni \theta$.</li>
</ul>

<h3 id="lecture-5-delta-method-and-confidence-intervals">Lecture 5: Delta method and confidence intervals</h3>

<ul>
  <li>
    <p><strong>The Delta method</strong></p>

    <p>The delta method allows us to apply functions to an estimator we construct so that we can estimate values of interest rather functions of the quantity being estimated. We will discuss an example next, but first let us look at the formal definition. If</p>

    <script type="math/tex; mode=display">\begin{align}
      \sqrt{n}(\bar{R}_n - \theta) \xrightarrow[n \to \infty]{(d)} N(0, \sigma^2)
  \end{align}</script>

    <p>then the delta method states that</p>

    <script type="math/tex; mode=display">\begin{align}
      \sqrt{n}(g(\bar{R}_n) - g(\theta)) \xrightarrow[n \to \infty]{(d)} N(0,(g'(\theta))^2 \sigma^2)
  \end{align}</script>

    <p>provided that $g’(\theta)$ is continuously differentiable, that is $g’(\theta)$ exists and is continuous. Let us not look at an example.</p>
  </li>
  <li>
    <p>Consider $X_1, \dots, X_n$ to be i.i.d. $\exp(\lambda)$. How do we construct an estimator for the parameter $\lambda$? We fist need to check for consistency and then to see if it unbiased. The first insight is to start with what we know quite well: the sample mean. We know from the weak law of large numbers that $\bar{X}_n \xrightarrow[n \to \infty]{\mathbf{P}} \mathbf{E}[X_i] = 1 / \lambda$. We can now use the continuous mapping theorem using the function $f(x) = x^{-1}$ to say that</p>

    <script type="math/tex; mode=display">\dfrac{1}{\bar{X}_n} \xrightarrow[n \to \infty]{\mathbf{P}} \lambda.</script>

    <p>Therefore, the estimator $\hat{\lambda} = 1/\bar{X}_n$ is consistent. Is it unbiased? Is $1/\bar{X}_n - \lambda = 0$? This can be calculated and it turns out that this is not an unbiased estimator. A simple way to see this would be to use Jensen’s inequality. $g(x) = 1/x$ is a convex function and hence $g(\mathbf{E}[x]) &lt; \mathbf{E}[g(x)]$. (The equality only holds if the function is affine (i.e. linear transformation + constant). An unbiased estimator is a desirable property but should not be pursued at all costs (for example at the expense of a very large variance.)</p>

    <p>Now, we know from the central limit theorem that</p>

    <script type="math/tex; mode=display">\sqrt{n}(\bar{X}_n - 1/\lambda) \xrightarrow[n \to \infty]{\mathbf{P}} N(0, 1/\lambda^2)</script>

    <p>We can now apply the delta method. Using $g(x) = 1/x$, $g’(1/\lambda) = - \lambda^2$. So applying the delta method yields</p>

    <script type="math/tex; mode=display">\sqrt{n}(1/\bar{X}_n - \lambda) \xrightarrow[n \to \infty]{\mathbf{P}} N(0, \lambda^4/\lambda^2)</script>

    <p>Now, let’s say that we need to find confidence intervals for this estimator? Calling $1/\bar{X}_n = \hat{\lambda}$, we have from the above equation that</p>

    <script type="math/tex; mode=display">\sqrt{n}(\hat{\lambda} - \lambda) \xrightarrow[n \to \infty]{\mathbf{P}} N(0, \lambda^4/\lambda^2)</script>

    <p>and from the central limit theorem, we have that the confidence interval at level alpha is defined by</p>

    <script type="math/tex; mode=display">\dfrac{\sqrt{n}\lvert \hat{\lambda} - \lambda \rvert }{\lambda} \leq q_{\alpha/2}</script>

    <p>and now we can either use $I_\textrm{plug-in}$ or $I_\textrm{solve}$ to find the confidence interval. Exercise, can you now work out the case where the estimator is $\hat{\lambda} = \min(X_1, \ldots, X_n)$. Again, the steps are to show that it is consistent, find it’s asymptotic variance and then calculate the confidence interval using with the conservative bound, plug-in, or solve technique.</p>
  </li>
</ul>

<h3 id="lecture-6-introduction-to-hypothesis-testing-and-type-1-and-type-2-errors">Lecture 6: Introduction to hypothesis testing, and type 1 and type 2 errors</h3>

<p>There is a lot of new terminology and new definitions in this unit here, so pay attention! The goal of this lecture is to introduce hypothesis testing. When doing hypothesis testing we are not interested in obtaining an estimator unknown parameters. We are only interested in asking binary questions about unknown parameters. Is the mean of average heights in the US larger or smaller than 5.5, is the waiting time in the ER less than 30 mins, etc.</p>

<ul>
  <li>
    <p>The example we are going to see here is to test between different boarding methods for flights. Let’s say we are interested in evaluating two different methods - R2F (rear to front) or WilMA (window, middle, aisle in that order, which is an inside to outside boarding method). Let’s, let’s say we are given the following data (usually called summary statistics)</p>

    <table>
      <thead>
        <tr>
          <th> </th>
          <th style="text-align: right">R2F</th>
          <th style="text-align: right">WilMA</th>
        </tr>
      </thead>
      <tbody>
        <tr>
          <td>Mean (mins)</td>
          <td style="text-align: right">24.2</td>
          <td style="text-align: right">15.9</td>
        </tr>
        <tr>
          <td>Std. Dev. (mins)</td>
          <td style="text-align: right">2.1</td>
          <td style="text-align: right">1.3</td>
        </tr>
        <tr>
          <td>Samples</td>
          <td style="text-align: right">72</td>
          <td style="text-align: right">56</td>
        </tr>
      </tbody>
    </table>

    <p>We now ask the question about whether the difference between means is significant? How do we quantify and evaluate this statistical significance, if any?</p>
  </li>
  <li>
    <p>What are some of the modeling assumptions we need to begin to answer this question?</p>

    <ol>
      <li>
        <p>Let $X$ and $Y$ denote the boarding time of a random R2F and WilMA flight respectively. Further, we assume that $X \sim N(\mu_1, \sigma_1^2)$ and $Y \sim N(\mu_2, \sigma_2^2)$. Let $n$ and $m$ denote the sample sizes of $X$ and $Y$ respectively. We assume that $X_1, \dots, X_n$ and $Y_1, \dots, Y_n$  are independent <strong>copies</strong> (meaning identically distributed) of $X$ and $Y$ respectively. Further the two samples themselves are independent. In effect, we assume that every random variable is independent from every other random variable in the problem.</p>
      </li>
      <li>
        <p>We now ask the question whether $\mu_1 = \mu_2$ or $\mu_1 &gt; \mu_2$. Note that with modeling assumptions, we decrease the number of ways the hypothesis $\mu_1 = \mu_2$ may be rejected. We do not allow for the case $\mu_1 &lt; \mu_2$.</p>
      </li>
      <li>
        <p>We have two samples, so this is a <strong>two sample test</strong>.</p>
      </li>
    </ol>
  </li>
  <li>
    <p>Note that although I have data, I cannot make an assumption about which hypothesis is better just by looking at the data. I have to still assume that $\mu_1 = \mu_2$ is a possibility. If I make assumptions after looking at the data, I will make biased assumptions and of course my assumptions will play out.</p>
  </li>
  <li>
    <p>So how do we actually go about this? As a starting heuristic, I could simply say that I can going to compare $\bar{X}_n$ and $\bar{Y}_m$, and if $\bar{X}_n &gt; \bar{Y}_m $, then I could say that this is enough to conclude that $\mu_1 &gt; \mu_2$. But this has some issues: what if $\mu_1 = 50.1$ and $\mu_1 = 50$. Or what if we had a sample size of 2? Then intuitively this heuristic doesn’t make much sense.  We somehow need to capture a sense for the variability of the data and the how big our sample size is. So we are going to come up with a heuristic that says if</p>

    <script type="math/tex; mode=display">\bar{X}_n - \textrm{buffer}_n > \bar{Y}_m + \textrm{buffer}_m</script>

    <p>then $\mu_1 &gt; \mu_2$. The idea here is to capture the fact that there is some fluctuation of $\bar{X}_n$ and $\bar{Y}_m$ about their respective sample means $\mu_1$ and $\mu_2$. Note that the sizes of these buffers must go to 0 as $n, m \to \infty$ because if we have an infinite amount of data, then we know everything there is to know about the random variables $X$ and $Y$, and there is no uncertainty or randomness.</p>
  </li>
  <li>
    <p>The idea here is best seen with an example. Let’s say we toss a coin 200 times and we find that we get 80 heads. We now ask the question “is the coin fair?” More formally we are asking the question “Is $p = 0.5$?”. To go about we look at what would happen if in fact $p = 0.5$ so we will go ahead and assume this. Let us also assume a sample mean $\bar{X}_n$ and be definition, this equals 80/200.  Then, because each coin toss is independent, assuming $p=0.5$ amounts to saying that some normal random variable took on the value defined by</p>

    <script type="math/tex; mode=display">% <![CDATA[
\begin{align}
      \dfrac{\sqrt{n}(\bar{X}_n - 0.5)}{\sqrt{0.5(1-0.5)}}  & = \dfrac{\sqrt{n}(80/200 - 0.5)}{\sqrt{0.5(1-0.5)}} = -2.8284
  \end{align} %]]></script>

    <p>In other words, we are syaing that if $p = 0.5$, then observing a sample mean of 80/200 is the same as oberving some standard normal random variable take on the value -2.8284. This turns out to be extremely unlikely, because this number is way out in the tail of the distribution. So the appropritate conclusion of this experiment is <em>it is unlikely that the coin is fair</em>. If you were to repeat the calculation and say that we say 106 heads in the 200 tosses, then this amounts to a standard normal taking on the value 0.8485, which is not that far in the tail of the distribution. In this case, we would conclude <em>it is likely that the coin is fair</em>.</p>
  </li>
  <li>
    <p>We now introduce the concepts of <strong>null hypothesis</strong> $H_0$ and <strong>alternate hypothesis</strong> $H_1$ formally defined by</p>

    <script type="math/tex; mode=display">\begin{cases}
      H_0: \theta \in \Theta_0 \\
      H_1: \theta \in \Theta_1
  \end{cases}</script>

    <p>The standard practice is to chose the null hypothesis as the status quo. For example, when checking to see if a new drug is any good, we choose as the null hypothesis that the drug is just as good as the placebo i.e. if I model the outcome of asking the question “Do you feel better” to the patients at the end of the trial as a $\textrm{Ber}(p)$, then the null hypothesis is $p_\textrm{drug} = p_\textrm{control}$. In this example, and always, we only look for evidence in the data that can falsify our null hypothesis. We look for falsifiability, we don’t look for proof of our hypothesis in our data. In other words, all we can say is “we did not find evidence that H_0” is false. This is not the same sa saying that $H_0$ is true. Keep the innocent until proven guilty idea in mind. A jury can concludes, “not guilty” meaning, there was not enough evidence to show that he was guilty. They do not conclude “innocent”.</p>
  </li>
  <li>
    <p>One more definition: A <em>test</em> is a statistic $\psi$ such that</p>

    <script type="math/tex; mode=display">\psi = 
  \begin{cases}
      0, \textrm{ if } H_0 \textrm{ is not rejected} \\ 
      1, \textrm{ if } H_0 \textrm{ is rejected}
  \end{cases}</script>

    <p>In oher words, if $\psi = 1$, we reject the null hypothesis and if $\psi = 0$, we will fail to reject the null hypothesis (doesn’t mean that the null hypothesis is correct). We can now formulate the test by comparing to a standard normal by saying that (using an example of a Bernoulli)</p>

    <script type="math/tex; mode=display">\psi = \mathcal{I} \left\{\dfrac{\sqrt{n} \lvert \bar{X}_n - p \rvert}{\sqrt{p(1-p)}} > C \right\}</script>

    <p>for some $C$. he question now is how do we choose $C$? Note that we can always write a test $\psi$ as an indicator function. We are asking a binary question. Was the null hypothesis rejected? Yes or no? At the end of the day, it is also a function that can be computed from data.</p>
  </li>
  <li>We now also introduce <strong>Type 1 errors</strong> and <strong>Type 2 errors</strong>.
    <ul>
      <li>
        <p>Type 1 errors are the case when my test rejects $H_0$ when it is actually true.</p>
      </li>
      <li>
        <p>Type 2 errors are the case when my test fails to reject $H_0$ when $H_1$ is actually true.</p>
      </li>
    </ul>
  </li>
  <li>
    <p>Some more formalism: The rejection region of a test $\psi$ is defined by</p>

    <script type="math/tex; mode=display">R_\psi = \{x \in E^n: \psi(x) = 1\}</script>

    <p>Here the $E^n$ just means that $x$ is a vector of dimension $n$. It is just where $x_1, \ldots, x_n$ lives.</p>
  </li>
  <li>
    <p>Here are a whole bunch of definitions, which are extremely important.</p>

    <ul>
      <li>
        <p>$\alpha_\psi$ is the <strong>Type 1 error</strong> which is a function of $\theta$:</p>

        <script type="math/tex; mode=display">% <![CDATA[
\begin{align}
          \alpha_\psi: \Theta_0 & \xrightarrow[ ]{ } \mathbb{R} \\
          \theta & \longmapsto \mathbf{P}_\theta(\psi = 1)
      \end{align} %]]></script>

        <p>The meaning of the notation is that this function of $\theta$ maps every $\theta \in \Theta_0$ to a probability that lies in the interval (0,1). It calculates the probability that $\psi = 1$ when $\theta \in \Theta_0$. In other words, it calculates as a function of $\theta$ the probability that the test $\psi$ rejects $H_0$ when $H_0$ is in fact true.</p>
      </li>
      <li>
        <p>$\beta_\psi$ is the <strong>Type 2 error</strong> which is also a function of $\theta$:</p>

        <script type="math/tex; mode=display">% <![CDATA[
\begin{align}
          \beta_\psi: \Theta_1 & \xrightarrow[ ]{ } \mathbb{R} \\
          \theta & \longmapsto \mathbf{P}_\theta(\psi = 0)
      \end{align} %]]></script>

        <p>The meaning here is: this function maps every $\theta \in \Theta_1$ to a probability that lies in the interval (0,1). It calculates the probaility that $\psi = 0$ when $\theta \in \Theta_1$. In other words, it calculates as a function o $\theta$ the probability that the test $\psi$ fails to reject $H_0$ when $H_1$ is infact true.</p>
      </li>
      <li>
        <p>Power of a test $\pi_\psi$:</p>
      </li>
    </ul>

    <script type="math/tex; mode=display">\begin{align}
      \pi_\psi = \inf\limits_{\theta \in \Theta_1} (1 - \beta_\psi(\theta))
  \end{align}</script>

    <ul>
      <li>A test $\psi$ has level $\alpha$ if</li>
    </ul>

    <script type="math/tex; mode=display">\alpha_\psi \leq \alpha \ \ \ \forall \theta \in \Theta_0</script>

    <ul>
      <li>A test $\psi$ has asymptotic level $\alpha$ if</li>
    </ul>

    <script type="math/tex; mode=display">\lim\limits_{n \to \infty} \alpha_{\psi_n} \leq \alpha \ \ \ \forall \theta \in \Theta_0</script>

    <ul>
      <li>In general a test has the form</li>
    </ul>

    <script type="math/tex; mode=display">\psi = \mathbb{I}(T_n > C)</script>

    <p>where $\mathbb{I}$ is the indicator function, which equals one if the condition $T_n &gt; C$ is satisfied. The quantity $T_n$ is called a test statistic, and the number $C$ is a threshold yet to be determined. The rejection region $R_\psi = {T_n &gt; C}$.</p>
  </li>
  <li>
    <p>In the case when $H_0$ is of the form $\theta \in \Theta_0$, $\Theta_0 = {\theta_0}$, we can refine the terminalogy further and define <em>one-sided</em> and <em>two-sided</em> tests. If $H_1: \theta \neq \theta_0$, then it is called a two-sided test. If $H_1: \theta &gt; \theta_0$ or $H_1: \theta &lt;&gt; \theta_0$, then it is called a two-sided test. This terminology is useful when determining type 1 and type 2 errors because it tells us if we need to use absolute values in our standardization, and hence whether there is a factor of 2 floating around.</p>
  </li>
  <li>Note that when we are deciding on the threshold $C$</li>
</ul>

<h2 id="unit-3-methods-of-estimation">Unit 3: Methods of estimation</h2>

<h3 id="lecture-8-distance-measures-between-distributions">Lecture 8: Distance measures between distributions</h3>

<ul>
  <li>
    <p>Maximum likelihood estimatiion should be the go to method when presented with some new problem at hand. Using the method of moments (say expectation or variance) often works, but what if I have multiple parameters, and if the expectation is a function of the multiple parameters. In this case, maximum likelihood estimation should be the standard method I should reach for. In the special cases where the method of moments works, I should still attempt that becuase of its simplicity.</p>
  </li>
  <li>
    <p>Because I am calculating maxima, maximum likelihood estimation can sometimes be computationally intractable.</p>
  </li>
  <li>
    <p>The goal of a statistician is to be able to estimate a probability distribution. Let’s say that there is some true probability distribution <script type="math/tex">\mathbb{P}_ {\theta^*}</script> that is generating the data, and that I have the model <script type="math/tex">\mathbb{P}_ \theta</script>. I want <script type="math/tex">\mathbb{P}_ \theta</script> to be as close as possible to <script type="math/tex">\mathbb{P}_ {\theta^*}</script> for all possible $x \subset E$. This naturally leads to the notion of a <em>total variation distance</em> which computes</p>

    <script type="math/tex; mode=display">\begin{align}
      TV(P_\theta, P_{\theta'}) = \max\limits_{A \subset E}\lvert P_\theta - P_{\theta'} \rvert
  \end{align}</script>

    <p>It turns out for a discrete probability distribution, there is a nice analytical form for this and the total variation distance becomes</p>

    <script type="math/tex; mode=display">\begin{align}
      TV(P_\theta, P_{\theta'}) = \dfrac{1}{2}\sum\limits_{x \in E} \lvert p_\theta (x) - p_{\theta'} (x) \rvert \label{eqn:TVsum}
  \end{align}</script>

    <p>where the lower case $p$ denotes the PMF.</p>
  </li>
  <li>
    <p>If we are computing the total variation distance between say a Gaussian and an exponenial which are defined over different sample spaces, then I just take the union of the sample space and set the appropriate ones to 0. What if I am interested in calculating the difference between a Bernoulli and a Gaussian? Then there is no way around calculating all possible subsets of the sample space and then actually finding the maximum. There is a trick that coming below to do this kind of thing.</p>
  </li>
  <li>
    <p>Note that for continuous probability distributions, we just replace the sum with integral, where the integral runs over the union of the sample spaces of the two distributions in question (just appropriately set the distribution to zero when $x$ takes values outside it’s domain).</p>
  </li>
  <li>
    <p>Can you think about the graphical interpretation of the total variation distance between two continuous PDFs as areas under the curve? Reason this out.</p>
  </li>
  <li>
    <p>Some properties of the total variation distance:</p>

    <script type="math/tex; mode=display">% <![CDATA[
\begin{align}
          \textrm{symmetry: } & TV(\mathbb{P}_ \theta, \mathbb{P}_ \theta') = TV(\mathbb{P}_ \theta, \mathbb{P}_ \theta') \\
          \textrm{non-negative: } & 0 \leq TV(\mathbb{P}_ \theta, \mathbb{P}_ \theta') \leq 1 \\  
          \textrm{definite: If } &  TV(\mathbb{P}_ \theta, \mathbb{P}_ \theta') = 0 \textrm{ then }\mathbb{P}_ \theta = \mathbb{P}_ \theta' \\
          \textrm{triangle inequality: } & TV(\mathbb{P}_ \theta, \mathbb{P}_ \theta') \leq TV(\mathbb{P}_ \theta, \mathbb{P}_ \theta'') + TV(\mathbb{P}_ \theta'', \mathbb{P}_ \theta')
      \end{align} %]]></script>

    <p>These four axioms are necessary for the notion of a distance, much like Cartesian distances. This is a distance between probability distributions.</p>
  </li>
  <li>
    <p>Most of the calculations of Total Variation Distance involved the formula given in equation \ref{eqn:TVsum} (or its integral equivalent for continuous distributions).</p>
  </li>
  <li>
    <p>Worked example: Find <script type="math/tex">TV(2\sqrt{n}(\bar{X}_n - 1/2), Z)</script> where <script type="math/tex">X_i \sim \textrm{Ber}(0.5)</script> and <script type="math/tex">Z \sim N(0,1)</script>. The trick here, and perhaps one in general is to realize that if we can find some <script type="math/tex">A \subset E</script> for which the total variation is 1, we know that this is the worst case and hence the maximum is 1. So this involves finding a subset of the sample space $A$ for which the total variation is one, and then using the primary definition:</p>

    <script type="math/tex; mode=display">\begin{align}
      TV(\mathbb{P}_ \theta, \mathbb{P}_ \theta') = \max\limits_{A \subset E} \lvert \mathbb{P}_ \theta (A) - \mathbb{P}_ \theta' (A) \rvert
  \end{align}</script>

    <p>To go about this, we first introduce some terminology: the support of a function $f$ is the set of all $x \in D$ for which $f(x) \neq 0$, where $D$ is the domain of the function. So, let us start the problem by finding a support for <script type="math/tex">2\sqrt{n}(\bar{X}_n - 1/2)</script>. We know that $\bar{X}_n \in \{1/n, 2/n,\ldots,n/n\}$ and hence $2\sqrt{n}(\bar{X}_n - 1/2) \in \{2\sqrt{n}(k/n -1/2): k = 0, 1,\ldots,n\}$. Now remember that we are looking for a subset of the sample space such that the <script type="math/tex">\mathbb{P} _\theta = 1</script> and <script type="math/tex">\mathbb{P} _\theta' = 0</script>. One obvious choice for <script type="math/tex">A \subset E</script> is to take the support itself, because necessarily for the first rv, the <script type="math/tex">\mathbb{P} _ \theta(A) = 1</script>. Furthermore, because the normal rv is continuous, the probability that it takes on values in a dicrete domain is 0, and hence <script type="math/tex">\mathbb{P} _ \theta'(A) = 0</script>. This is the worst case (i.e. the maximum) and it follows that the total variaion in this case is 1.</p>
  </li>
  <li>
    <p>The above example exemplifies the problem with the total variation distance: the fact that the TV distance quantifies that every discrete distribution deviates to the same extent from every continuous distribution is not very informative. For example, if I were to look at the distance between $X$ and $X + a$ where $X$ is Bernoulli and $a \subset (0, 1)$. Because this is a closed interval, the TV is always 1, regardless of the magnitude of $a$.</p>
  </li>
  <li>
    <p>Another good metric to quantify how far apart two distribution are is the <strong>Kullback-Leibler Divergence</strong>. The KL divergence between two pdfs (replace the integral with a sum for the discrete case) is given by</p>

    <script type="math/tex; mode=display">\begin{align}
      KL(\mathbb{P} _ \theta, \mathbb{P} _ \theta') = \int_x p_\theta(x) \ln\left(\dfrac{p_\theta (x)}{p_\theta'(x)}\right)
  \end{align}</script>

    <p>The definition above just says that the KL distance is the expectation of <script type="math/tex">\ln(p _ {\theta ^ *}/p _ \theta)</script>. That is,</p>

    <script type="math/tex; mode=display">\begin{align}
      KL(\mathbb{P} _ {\theta ^ *}, \mathbb{P} _ \theta) = \mathbb{E}\left[\ln\left(\dfrac{p _ {\theta ^ *} (x)}{p_\theta(x)}\right)\right] \label{eqn:KLdefn}
  \end{align}</script>

    <p>Note that some of the properties that we defined above for a distance metric metric do not apply for the KL divergence (hence the nomenclature of it being a divergence and not a distance). Specifically the properties are,</p>

    <ul>
      <li>It is not symmetric</li>
      <li>It is positive. There is a nice proof using Jensen’s inequality for the concave ln function. Can you work this out?</li>
      <li>It is definite. i.e. if the KL divergence is 0, then <script type="math/tex">\mathbb{P} _ \theta = \mathbb{P} _ \theta'</script>. This is really important because if we doing minimizations and maximizations, then we can identify a single optimum. This lets us find the value of $\theta ^ *$ s.t. $\theta ^ * = \theta$. More on minimizations coming later.</li>
      <li>The triangle inequality in general does not apply.</li>
    </ul>

    <p>A couple of points here. Note that because it is not symmetric, the order in which you calculate the divergence is important. In general, let’s say that we have some true distribution <script type="math/tex">\mathbb{P} _ {\theta ^ *}</script> that generated my data, and I am trying to estimate it with the distribution <script type="math/tex">\mathbb{P} _ {\theta}</script>. In this case, we will define the KL divergence to be <script type="math/tex">KL(\mathbb{P} _ {\theta ^ *}, \mathbb{P} _ {\theta})</script>. The second point is the definite property. If we can find some $\theta$ for which <script type="math/tex">KL(\mathbb{P} _ {\theta ^ *}, \mathbb{P} _ {\theta}) = 0</script>, it means that we have found $\theta ^ *$. In practice it is hard to make it exactly zero, and we will instead find the <script type="math/tex">\theta</script> at which the KL divergence is a minimum. We are trying to get as close to the true <script type="math/tex">\theta ^ *</script> as possible.</p>
  </li>
  <li>
    <p>The positiveness of the KL divergence turns out to be super useful while estimating the distribution given some data. This is important and really practical so pay attentioin! Consider again the definition given in Equation \ref{eqn:KLdefn}. We expand the logarithm to find (the expositon below is for a pmf but a very similar argument holds for a pdf)</p>

    <script type="math/tex; mode=display">% <![CDATA[
\begin{align}
      KL(\mathbb{P} _ {\theta ^ *}, \mathbb{P} _ \theta) & = \mathbf{E}[\ln p _ {\theta ^ *}(x) - p _ \theta(x)] \\
      & = \mathbf{E}[\ln p _ {\theta ^ *}(x)] - \mathbf{E}[\ln p _ \theta(x)]
  \end{align} %]]></script>

    <p>Now, because <script type="math/tex">\theta ^ *</script> is a fixed (unknown) number, the expectation is just a constant $C$ and hence the above can be rewritten as</p>

    <script type="math/tex; mode=display">% <![CDATA[
\begin{align}
      KL(\mathbb{P} _ {\theta ^ *}, \mathbb{P} _ \theta) & = C - \mathbf{E}[\ln p _ \theta(x)]
  \end{align} %]]></script>

    <p>Now we use the most useful hammer in our toolbox: from the law of large numbers, we can replace the expectation in the above equation with a sample mean, and hence we can rewrite the equation as</p>

    <script type="math/tex; mode=display">% <![CDATA[
\begin{align}
      \hat{KL}(\mathbb{P} _ {\theta ^ *}, \mathbb{P} _ \theta) & = C - \dfrac{1}{n}\sum\limits _ {i=1} ^ n \ln (p _ \theta (X_i))
  \end{align} %]]></script>

    <p>where the <script type="math/tex">X _ i</script> are the observations. Note that because we now put in the observations, the KL divergence now becomes an estimator and we now need the hat symbol above it. Now, from the definiteness property (i.e. if <script type="math/tex">KL(\mathbb{P} _ {\theta ^ *}, \mathbb{P} _ \theta) = 0</script>, then <script type="math/tex">\theta ^ * = \theta</script>), we ideally want to find a <script type="math/tex">\theta</script> as close to <script type="math/tex">\theta ^ *</script> as possible. Because the KL divergence is always positive, the ideal number would be 0, but in practice, we minimize the KL divergence. This means that we want to find</p>

    <script type="math/tex; mode=display">% <![CDATA[
\begin{align}
      & \min\limits _ {\theta \in \Theta} C - \dfrac{1}{n}\sum\limits _ {i=1} ^ n \ln (p _ \theta (X_i)) \\
      \Rightarrow & \max\limits _ {\theta \in \Theta} \dfrac{1}{n}\sum\limits _ {i=1} ^ n \ln (p _ \theta (X_i)) \\
      \Rightarrow & \max\limits _ {\theta \in \Theta} \ln (\prod\limits _ {i=1} ^ n p _ \theta (X_i)) \\
      \Rightarrow & \max\limits _ {\theta \in \Theta} \prod\limits _ {i=1} ^ n p _ \theta (X_i) \\
  \end{align} %]]></script>

    <p>Note that the quantity in the final equation is exactly the likelihood that I would see the observations $X_1 = x_1,\ldots,X_n = x_n$ under the model <script type="math/tex">\mathbb{P} _ \theta</script>. So in essence, we want to find that value of $\theta$ that maximizes the probability of seeing those observations occur. We can therefore define a likelihood function <script type="math/tex">L(x_1, x_2, \ldots, x_n; \theta)</script></p>

    <script type="math/tex; mode=display">% <![CDATA[
\begin{align}
      L: &  E^n \times \Theta \longrightarrow \mathbb{R} \\
         & {x_1,\ldots,x_n; \theta} \mapsto \mathbb{P} _ \theta (X_1=x_1,\ldots,X_n=x_n)
  \end{align} %]]></script>
  </li>
</ul>

<h3 id="lecture-9-introduction-to-maximum-likelihood-estimation">Lecture 9: Introduction to Maximum Likelihood Estimation</h3>

<ul>
  <li>
    <p>Let us look at an example using Bernoulli trials:</p>

    <script type="math/tex; mode=display">\begin{align}
  L(x_1,\ldots,x_n; p) = p ^ {\sum x_i} (1- p) ^ {n - \sum x_i}
  \end{align}</script>
  </li>
  <li>
    <p>An example using Poisson distributed data:</p>

    <script type="math/tex; mode=display">% <![CDATA[
\begin{align}
  L(x_1,\ldots,x_n; \lambda) & = \prod\limits _ {i = 1} ^ n \dfrac{e ^ {-\lambda} \lambda ^ {x_i}}{x_i!} \\
  & = \dfrac{e ^ {-n\lambda}\lambda ^ {\sum x_i}}{x_1 ! \ldots x_n!}
  \end{align} %]]></script>

    <p>You can similarly derive the likelihood functions for Gaussians, Exponentials, etc.</p>
  </li>
  <li>
    <p>One point to note is that sometimes we have to use indicator functions appropriately while defining the likelihood function (because if the obervation was outside the range of the underlying distribution, the likelihood that the obervations is described by that distribution is zero). Take for example the uniform distribution <script type="math/tex">\mathcal{U}([0, b])</script>. The likelihood function is defined as</p>

    <script type="math/tex; mode=display">% <![CDATA[
\begin{align}
      L(x_1, \ldots, x_n) & = \dfrac{1}{b^n} \prod\limits_{i=1}^{n}\mathbb{I} \{x_i > 0\}\cdot\mathbb{I} \{x_i < b\} \\
      & = \dfrac{1}{b^n} \prod\limits_{i=1}^{n}\mathbb{I} \{\min x_i > 0\} \cdot\mathbb{I} \{\max x_i < b\} \\
      & = \dfrac{1}{b^n} \prod\limits_{i=1}^{n}\mathbb{I} \mathbb{I} \{\max x_i < b\}
  \end{align} %]]></script>

    <p>Note that we omitted the requirement <script type="math/tex">\min x_i > 0</script> because we assumed that comes naturally from the fact that we assumed a uniform distribution. If this inequality was not respected, something is definitely wrong with our assumption.</p>
  </li>
  <li>
    <p>We often want to maximize the likelihood in the maximum likelihood estimator, but this is the same as trying to maximize the log likelihood because the log is an increasing function. This makes our life much easier in the face of a whole bunch of powers and exponentials, which often appears in the likelihood functions. <strong>Note that we are maximizing with respect to the parameters, not the</strong> <script type="math/tex">\mathbf{x_i}</script>.</p>
  </li>
  <li>
    <p>Notation: The maximum likelihood estimator is denoted by <script type="math/tex">\hat{\theta} ^ {MLE} _ n</script>, and the definition is (for a pmf, but an identical definition holds for a pdf):</p>

    <script type="math/tex; mode=display">\begin{align}
      \hat{\theta} ^ {MLE} _ n = \textrm{argmax} _ {\theta \in \Theta} \prod\limits_{i=1}^n p_\theta(x_i)
  \end{align}</script>
  </li>
  <li>
    <p>Now that we are interested in finding maxima of a function, we are enetering the realm of calculus, in fact, multivariable calculus. Very brief notes in thid section due to my familiarity with multivariable calculus:</p>

    <ul>
      <li>
        <p>If we have a function $f(\theta)$, where $\theta = (\theta_1, \theta_2,\ldots, \theta_n)^T$ is a column vector, then to calculate if the function in $d$-dimensional space is concave or convex I am interested in finding the Hessian Matrix $\mathbf{H}f(\theta)$, which is deifined as</p>

        <script type="math/tex; mode=display">% <![CDATA[
\begin{align}
      \mathbf{H}f(\theta) = \left(
      \begin{matrix}
          \dfrac{\partial^2 f}{\partial \theta_1 \theta_1} & \dfrac{\partial^2 f}{\partial \theta_1 \theta_2} & \dots & \dfrac{\partial^2 f}{\partial \theta_1 \theta_d} \\
          \dfrac{\partial^2 f}{\partial \theta_2 \theta_1} & \dfrac{\partial^2 f}{\partial \theta_2 \theta_2} & \dots & \dfrac{\partial^2 f}{\partial \theta_2 \theta_d} \\
          \vdots & \vdots & \ddots & \vdots \\
          \dfrac{\partial^2 f}{\partial \theta_d \theta_1} & \dfrac{\partial^2 f}{\partial \theta_d \theta_2} & \dots & \dfrac{\partial^2 f}{\partial \theta_d \theta_d}
      \end{matrix} \right) \label{eqn:Hessian}
  \end{align} %]]></script>

        <p>If $x^T \mathbf{H}f(\theta)x \leq 0$ (resp. $\geq 0$), for all $x \in \mathbb{R}^d$, then $f(\theta)$ is concave (resp. convex). This is a strong requirement. This requirement becomes clearer when thinking about saddle points, etc.</p>
      </li>
      <li>
        <p>A symmetric real-valued $d \times d$ matrix <script type="math/tex">\mathbf{A}</script> is positive semi-definite if <script type="math/tex">x^T\mathbf{A}x \geq 0</script> for all <script type="math/tex">x \in \mathbb{R}^d</script>. Similarly we define negative semi-definite. If the inequality holds strictly, we use positive deinite and negative definite respectively.</p>
      </li>
      <li>
        <p>Another way of checking for positive semi-definiteness: A symmetric real-valued $d \times d$ matrix <script type="math/tex">\mathbf{A}</script> is positive semi-definite is all of its eigen-values are non-negative. If they are strictly positive, then <script type="math/tex">\mathbb{A}</script> is positive definite. Similarly you can define negative (semi) definite matrices. So if all the eigenvalues are strictly negative, the function is strictly concave, and vice versa.</p>
      </li>
      <li>
        <p>There is a quick way to check if the eigen values are negative. If a <script type="math/tex">2 \times 2</script> matrix <script type="math/tex">\mathbf{A}</script> is negative semi-definite, then we need both the eigen values $\lambda_1, \lambda_2$ to be negative. This would mean that <script type="math/tex">\textrm{tr}(\mathbf{A}) \leq 0</script> and <script type="math/tex">\textrm{det}(\mathbf{A}) \geq 0</script>. In the <script type="math/tex">2 \times 2</script> case, this is easy to see because <script type="math/tex">\textrm{tr}(\mathbf{A}) = \lambda_1 + \lambda_2</script> and <script type="math/tex">\textrm{det}(\mathbf{A}) = \lambda_1\lambda_2</script>.</p>
      </li>
    </ul>
  </li>
  <li>
    <p>General procedure to check if a multidimensional function is concave or convex: First calculate the Hessian for the function, given by equation \ref{eqn:Hessian}. We then need to check if the Hessian is positive (semi)definite or negative (semi)definite (convex and concave respectively). One way to do this to check the definition directly and see if <script type="math/tex">x^T \mathbf{A}x</script> is greater than or less than zero, for all $x \in \mathbb{R}^d$. In some cases it might be easier to check the eigenvalues. If all eigenvalues are (non-negative)positive, then the matrix is positive (semi)definite, and vice versa.</p>
  </li>
</ul>

<h3 id="lecture-10-consistency-of-mle-covariance-matrices-multivariate-statistics">Lecture 10: Consistency of MLE, covariance matrices, multivariate statistics</h3>

<ul>
  <li>
    <p>It turns out that the maximum likelihood estimator is consistent under some mild regulaity conditions (that the pdf is continuous almost everywhere as a function of <script type="math/tex">\theta</script>), that is,</p>

    <script type="math/tex; mode=display">\begin{align}
      \hat{\theta}^{MLE} \xrightarrow[n \to \infty]{\mathbf{P}} \theta^*
  \end{align}</script>

    <p>This works even for multivariate distributions, where $\theta = (\mu\ \sigma^2)^T$ for example. The definition of consistency for a random vector is slightly more involved and we need some deinitions here. A random vector <script type="math/tex">\mathbf{X} = (X^{(1)},\dots, X^{(d)})^T</script> is a function that maps <script type="math/tex">\omega \in \Omega</script> to <script type="math/tex">\mathbf{R}^d</script>, where <script type="math/tex">\Omega</script> is the sample space of the random variables <script type="math/tex">X^{(i)}</script>, which are themselves scalar. That is,</p>

    <script type="math/tex; mode=display">% <![CDATA[
\begin{align}
      X:& \Omega \xrightarrow[]{} \mathbf{R}^d \\
        & \omega \mapsto \left(\begin{matrix}
                                  & X^{(1)}(\omega) \\
                                  & X^{(2)}(\omega) \\
                                  &\vdots\\
                                  & X^{(d)}(\omega)
                              \end{matrix}\right)
  \end{align} %]]></script>

    <p>Note:  in general a random vector collects the outcome of the oberving $d$ ransom variables once, but we could also intepret it (depending on the situation) as $d$ independent realizations of some underlying distributioin $X$. Thinking about the former, this means that <script type="math/tex">X^{(1)} \sim N(0,1), X^{(1)} \sim \mathrm{Poiss(\lambda)}</script>, etc. For example, I could look at one person and the different components could be height, weight, income, etc.</p>

    <p>Now assume that I have a sequence of random vectors <script type="math/tex">\mathbf{X}_1, \ldots, \mathbf{X}_n</script> and another random vector <script type="math/tex">\mathbf{X}</script> , then we say that</p>

    <script type="math/tex; mode=display">\begin{align}
      \mathbf{X}_n \xrightarrow[n \to \infty]{\mathbf{P}} \mathbf{X}
  \end{align}</script>

    <p>if and only if
  <script type="math/tex">X _ n ^ {(k)} \xrightarrow[n \to \infty]{\mathbf{P}} X ^ {(k)}</script> for all <script type="math/tex">1 \leq k \leq d</script>. In other words, every component of the sequence of vectors should converge to the limiting vector. The notion of a CDF for a random vector also applies and it is defined as a function <script type="math/tex">F</script> such that</p>

    <script type="math/tex; mode=display">% <![CDATA[
\begin{align}
      F: & \mathbf{R}^d \xrightarrow[]{} [0, 1] \\
         & \mathbf{x} \mapsto \mathbf{P}(X^{(1)} \leq x^{(1)},\ldots,X^{(1)} \leq x^{(1)})
  \end{align} %]]></script>
  </li>
  <li>
    <p>In general, <script type="math/tex">\textrm{Cov}(X,Y) = 0</script> does not imply that <script type="math/tex">X</script> and <script type="math/tex">Y</script> are independent. This is only true if <script type="math/tex">(X, Y)^T</script> form a Gaussian vector. What is a Gaussian vector? <strong>It doesn’t mean that</strong> <script type="math/tex">X</script> <strong>and</strong> <script type="math/tex">Y</script> <strong>are both Gaussian.</strong> It means something more, it means that any linear combination <script type="math/tex">\alpha X + \beta Y</script> is also Gaussian, where $(\alpha, \beta) \in \mathbb{R}^2 \ (0,0)$. Can you show this using the example <script type="math/tex">X \sim N(0,1)</script> and <script type="math/tex">Y \sim R \cdot X</script>, where <script type="math/tex">R = 2B -1</script>, with <script type="math/tex">B \sim \textrm{Ber}(1/2)</script>. In other words, <script type="math/tex">R</script> (called the Radhemacher random variable) flips the sign of <script type="math/tex">X</script> with probability 1/2. Take the linear combination <script type="math/tex">X+Y</script>. What happens here?</p>
  </li>
  <li>
    <p>When talking about a random vectors (remember that here we just say that <script type="math/tex">\mathbf{X} = (X^{(1)}\ X^{(2) \ldots X^{(d)}})^T)</script>, it becomes extremely useful to begin to think of covariance matrices which is defined by</p>

    <script type="math/tex; mode=display">\begin{align}
      \mathbb{C}\textrm{ov}(\mathbf{X}) _ {ij} = \sigma _ {ij} = \textrm{Cov}(X^{(i)}, X^{(j)})
  \end{align}</script>

    <p>It just calculates the pairwise covariances between every component of the random vector <script type="math/tex">\mathbf{X}</script>. This is just convenient notation that also allows us to make a lot of useful transformations and manipulations as we will see later. Just think of it as compactly finding pairwise covariances of the components of a random vector and arranging them into a matrix. Here are some properties of the covariance matrix:</p>

    <ul>
      <li>The diagonal entries <script type="math/tex">ii</script> are just the variance of <script type="math/tex">X^{(i)}</script>.</li>
      <li>Let <script type="math/tex">X</script> be a random vector of dimensions <script type="math/tex">d \times 1</script>, and let <script type="math/tex">\mathbf{A}</script> be a <script type="math/tex">n \times d</script> matrix, and let $b$ be a <script type="math/tex">n \times 1</script> column vector. Then under the affine transformation <script type="math/tex">\mathbf{AX} + b</script>, <script type="math/tex">\mathbb{C}\textrm{ov}(\mathbf{AX + b}) = \mathbf{A}\mathbb{C}\textrm{ov}(\mathbf{X})\mathbf{A}^T</script>.</li>
      <li><script type="math/tex">\Sigma</script> is positive definite, so it is diagonalizable.</li>
      <li><script type="math/tex">\Sigma</script> is positive definite, so there exists an orthogonal matrix <script type="math/tex">U</script> such that <script type="math/tex">D = U\Sigma U^T</script>, where <script type="math/tex">D</script> is a diagonal matrix. (For an orthogonal matrix <script type="math/tex">U</script>, <script type="math/tex">UU^T = U^TU = I</script>). <script type="math/tex">D</script> contains the eigenvalues of <script type="math/tex">\Sigma</script>.</li>
      <li><script type="math/tex">\Sigma</script> has a unique square too i.e. there exists a matrix <script type="math/tex">\Sigma^\frac{1}{2}</script> such that  <script type="math/tex">\Sigma^\frac{1}{2} \cdot \Sigma^\frac{1}{2} = \Sigma</script>.</li>
      <li><script type="math/tex">\Sigma</script> is positive definite, so there exists a diagonal matrix <script type="math/tex">D = U\Sigma U^T</script> that has entries that are all strictly positive, then it is invertible and the inverse <script type="math/tex">\Sigma^{-1}</script> satisfies the following: <script type="math/tex">\Sigma^{-\frac{1}{2}} \cdot \Sigma^{-\frac{1}{2}} = \Sigma^{-1}</script>, where <script type="math/tex">\Sigma^{-\frac{1}{2}}</script> is the inverse square root of <script type="math/tex">\Sigma</script>.</li>
    </ul>
  </li>
  <li>
    <p>Now, we are ready to define Gaussian random vectors and the multivariate Gaussian distribution. A vector <script type="math/tex">\mathbf{X} = (X^{(1)}\ X^{(2)})^T</script> is a Gaussian vector if any linear combinations of the <script type="math/tex">X^{(i)}</script> is also Gaussian i.e. <script type="math/tex">\alpha^T\mathbf{X}</script> is Gaussian for any <script type="math/tex">\alpha \in \mathbb{R}^d</script>. For a multivariate Gaussian distribution, I only need to define the expectation vector <script type="math/tex">\mu = \mathbf{E}[ \mathbf{X} ]</script> and the covarianc matrix <script type="math/tex">\Sigma = \mathbb{C}\textrm{ov}(\mathbf{X})</script>. Then the pdf of the multivariate Gaussian distribution is</p>

    <script type="math/tex; mode=display">% <![CDATA[
\begin{align}
      f(\mathbf{X}) = f(X^{(1)}, \ldots, X^{(d)}) = & \left(\dfrac{1}{(2\pi)\textrm{det}(\mathbf{\Sigma)}}\right)^{d/2} \cdot \\
      & \exp\left[-\dfrac{1}{2} (x-\mu)^T\Sigma^{-1}(x-\mu)\right]
  \end{align} %]]></script>
  </li>
  <li>
    <p>Armed with this knowledge, we can now extend the central limit theorem to $d$ dimensions. The formulation in this case look very similar to the case of a scalar random variable except that we replace all the scalars with Gaussian vectors (remember the conditions on what makes something a Gaussian vector?). The central limit theorem in $d$ dimensions says that if <script type="math/tex">\mathbf{X} \in \mathbb{R}^d</script> is a random vector with mean <script type="math/tex">\mathbf{E}[\mathbf{X}] = \mu</script> and covariance matrix $\Sigma$. Let <script type="math/tex">\mathbf{X}_1, \ldots, \mathbf{X}_1</script> be i.i.d copies of <script type="math/tex">\mathbf{X}</script>. Then,</p>

    <script type="math/tex; mode=display">\begin{align}
      \sqrt{n}\Sigma^{-1/2}(\bar{\mathbf{X}}_n - \mathbf{E}[X])\xrightarrow[n \to \infty]{(d)} N_d(0, I_d)
  \end{align}</script>

    <p>where the expectation is defined somewhat intuitively - take the expectation of every single component in the vector, $\Sigma^{-1/2}$ is the square root of the covariance matrix, and $I_d$ is the identity matrix.</p>
  </li>
  <li>
    <p>In addition to having the central limit theorem in the case of random vectors of $d$ dimensions, we also have the multivariate delta method, which generalizes the delta method. Consider a sequence of random vectors <script type="math/tex">\mathbf{T}_n in \mathbb{R}^d</script>. Assume that the central limit theorem applies, i.e.,</p>

    <script type="math/tex; mode=display">\begin{align}
      \sqrt(n)(\mathbf{T}_n -\theta) \xrightarrow[n \to \infty]{(d)} N_d(0, \Sigma)
  \end{align}</script>

    <p>for some <script type="math/tex">\theta \in \mathbb{R}^d</script>. Now consider I have a function <script type="math/tex">g: \mathbb{R}^d \mapsto \mathbb{R}^k, k \geq 1</script>,  and if <script type="math/tex">g(\theta)</script> is continuously differentiable, then the multivariate delta method states that</p>

    <script type="math/tex; mode=display">\begin{align}
      \sqrt(n)(g(\mathbf{T}_n) -g(\theta)) \xrightarrow[n \to \infty]{(d)} N_d(0, (\nabla g(\theta))^T \Sigma \nabla g(\theta))
  \end{align}</script>

    <p>Some notes on taking the gradient of a vector function $\nabla g(\theta)$. We know that <script type="math/tex">g = g(\theta_1, \ldots, \theta_d)</script> lives in <script type="math/tex">\mathbb{R}^k</script>, and hence has <script type="math/tex">k</script> components <script type="math/tex">(g_1(\theta_1, \ldots, \theta_d), \ldots, g_k(\theta_1, \ldots, \theta_d))</script>. Therefore, we can think of the gradient of this function taking the gradient of each component <script type="math/tex">g_j</script> and stacking them beside each other. The gradient of each component is itself a column vector in <script type="math/tex">\mathbb{R}^d</script>. In other words,</p>

    <script type="math/tex; mode=display">\begin{align}
      (\nabla g(\theta))_{ij}  = \left(\dfrac{\partial g(\theta)_j}{\partial \theta_i}\right)_{1 \leq i \leq d, 1 \leq j \leq k} \in \mathbb{R}^{d \times k}
  \end{align}</script>

    <p>For example, consider taking the gradient of</p>

    <script type="math/tex; mode=display">% <![CDATA[
\begin{align}
      f(x, y, z) = \left(
                          \begin{matrix}
                              &x^2 + y^2 \\
                              & 2xyz \\
                              & z^2 + 4\\
                              & xy + \sin(z)
                          \end{matrix}
                  \right)
  \end{align} %]]></script>

    <p>Here <script type="math/tex">\theta = (x, y, z)^T</script> and <script type="math/tex">f</script> has four components. Therefore <script type="math/tex">k \in {1,2,3,4}</script> and <script type="math/tex">i \in {1,2,3}</script>. The transpose of the gradient matrix is also called the <strong>Jacobian matrix</strong>.</p>
  </li>
  <li>
    <p>Recitation 4 in unit 3 taught me something quite nice about when and why we need to use the multivariate Gaussian distribution during parameter estimation. Let’s say that we have a Gaussian of unknown mean and variance $N(\mu, \sigma^2)$. It is easy enough to construct an estimator for the mean $\mu$ because the law of large numbers dictates that it is just the sample mean. However, finding an estimator for the variance is a little bit more involved. It is natural to assume that given a series of observations $X_1, \ldots, X_n$, we can assume that a reasonable estimator is (see how sample means are your best friend?!)</p>

    <script type="math/tex; mode=display">\begin{align}
     \hat{\sigma^2} = \dfrac{1}{n}\sum\limits_{i=1}^nX_i^2 - \left(\dfrac{1}{n}\sum\limits_{i=1}^n X_i \right)^2
 \end{align}</script>

    <p>This is reasonable because by the LLN, we would expect the first term to converge to <script type="math/tex">\mathbf{E}[X_1^2]</script> and the second term to converge to <script type="math/tex">(\mathbf{E}[X_1])^2</script> (after an application of the continuous mapping theorem as well actually). But because these two terms clearly have non-zero covariance, if I want to find confidence intervals for my estimator, to find the variance of the normal distribution that it converges to involved accounting for the fact that the two terms ae covariant. That is, we can say that the vector</p>

    <script type="math/tex; mode=display">% <![CDATA[
\begin{align}
     T_n = \left( \begin{matrix}
                 & \dfrac{1}{n}\sum\limits_{i=1}^nX_i^2 \\
                 & \dfrac{1}{n}\sum\limits_{i=1}^n X_i
             \end{matrix} 
     \right) 
     \xrightarrow[n \to \infty]{\mathbf{P}} 
     \left( \begin{matrix}
                 & \mathbf{E}[X_1^2] = \sigma^2 + \mu^2 \\
                 & \mathbf{E}[X_1] = \mu
             \end{matrix} 
     \right) = \theta,
 \end{align} %]]></script>

    <p>and we can apply the central limit theorem on that vector to obtain</p>

    <p>$$
 \begin{align}
     \sqrt{n}(T_n - \theta) \xrightarrow[n \to \infty]{(d)} N_2(0, \Sigma)</p>

    <p>where $\Sigma$ is the covariance matrix between $X_1$ and $X_1^2$. So to properly apply the CLT, we need to talk about the multivariate CLT, and properly account for the covariances between the components of the matrix that is converging. But when we want to find the estimator, what we are doing is taking that vector and feeding it into a function <script type="math/tex">g: \mathbb{R}^2 \mapsto \mathbb{R}</script> that takes the components of this vector and spits out a number. Specifically, in this case the function is <script type="math/tex">g(x, y) = x - y^2</script>. Applying this function to <script type="math/tex">T_n</script> correctly outputs my estimator of <script type="math/tex">\sigma^2</script>, and hence the delta method applies here:</p>

    <script type="math/tex; mode=display">\begin{align}
     \sqrt{n}(g(T_n) - g(\theta)) \xrightarrow[n \to \infty]{(d)} N_2(0, (\nabla g(\theta))^T \Sigma (\nabla g(\theta)))
 \end{align}</script>

    <p>where $x$ and $y$ in the function $g$ represent the $x$ and $y$ (i.e. the appropriate) components of $\theta$. Carrying out this calculation gives us the correct variance of the distribution that my scaled and centered estimator converges to,  and hence I can find the correct confidence intervals for my estimator (which is a function of dependent random variables, but remember that the observations are themselves independent from each other).</p>
  </li>
</ul>

<h3 id="lecture-11-fisher-information-asymptotic-normality-of-mle-method-of-moments">Lecture 11: Fisher Information, Asymptotic Normality of MLE, Method of Moments</h3>

<ul>
  <li>
    <p>Let us go begin by going back to likelihood functions. The likelihood function <script type="math/tex">L_1(x; \theta),\ \theta \in \mathbb{R}^d</script> is just a different notation for the pdf <script type="math/tex">f_\theta (x)</script>. Define the log-likelihood as <script type="math/tex">l_1(\theta) = \ln(L_1(x; \theta))</script>. Now, can calculate the gradient of <script type="math/tex">l_1(\theta)</script>, and because <script type="math/tex">\theta \in \mathbb{R}^d</script>,  <script type="math/tex">\nabla l_1(\theta) \in \mathbb{R}^d</script>. We have already defined the notion of a covariance matrix for a $d$ dimensional vector and hence we can calculate</p>

    <script type="math/tex; mode=display">\begin{align}
      \mathbb{C}\textrm{ov}(\nabla l(\theta)) = \mathbf{E}[\nabla l(\theta)(\nabla l(\theta))^T] - \mathbf{E}[\nabla l(\theta)]\mathbf{E}[\nabla l(\theta)]^T \equiv I(\theta)
  \end{align}</script>

    <p>where by definition we define this covariance matrix of the gradient of the log-likelihood function to be the <strong>Fisher Information</strong> <script type="math/tex">I(\theta)</script>. Now there is a theorem that states that</p>

    <script type="math/tex; mode=display">\begin{align}
      I(\theta) = -\mathbf{E}[\mathbf{H}l(\theta)]
  \end{align}</script>

    <p>where <script type="math/tex">\mathbf{H}</script> denotes the Hessian matrix as before. This is somewhat remarkable: the covariance matrix is related to an expected value. For the 1D case, the covariance matrix is just <script type="math/tex">1 \times 1</script>, and hence <script type="math/tex">\nabla l(\theta) = l'(\theta)</script>,and  <script type="math/tex">I(\theta) = \textrm{Var}(l'(\theta))</script>. Then, from the theorem above, we have that</p>

    <script type="math/tex; mode=display">\begin{align}
      \textrm{Var}(l'(\theta)) = -\mathbf{E}[l''(\theta)]
  \end{align}</script>
  </li>
  <li>
    <p>So why go through all this singing and dancing for the Fisher information? It turns out that it has implications for the aymptotic normality of the MLE estimator. Under some fairly mild conditions (see the lecture - it accounts for the standard conditions such as identifiability, the support of <script type="math/tex">\mathbb{P} _ \theta</script> does not depend on <script type="math/tex">\theta</script>, and importantly, that the Fisher information matrix is invertible in a region around <script type="math/tex">\theta ^ *</script>), we can show that the MLE estimator is both consistent and asumptotically normal. In other words,</p>

    <script type="math/tex; mode=display">% <![CDATA[
\begin{align}
      & \hat{\theta}_n^{MLE} \xrightarrow[n \to \infty]{\mathbf{P}} \theta^*  \mathrm{w.r.t.\ \mathbb{P} _ {\theta^*}}\\
      & \sqrt{n}(\hat{\theta}_n^{MLE} - \theta^ * ) \xrightarrow[n \to \infty]{(d)} N_d(0, I^{-1}(\theta ^ * )) \mathrm{w.r.t.\ \mathbb{P} _ {\theta^*}}
  \end{align} %]]></script>

    <p>where <script type="math/tex">\theta^ *</script> is the true parameter. There is an outline of a proof in the lecture videos, but the detailed proof is beyond the scope of this class. The key idea is that the likelihood function is maximized near <script type="math/tex">\theta ^ *</script> and hence the slope of <script type="math/tex">l(\theta$)</script> and the expected value of the slope is 0.</p>

    <p><strong>The whole point of the Fisher information is that helps us determine the variance of the normal distribution that that</strong> <script type="math/tex">\hat{\theta}_n^{MLE} - \theta ^ *</script> <strong>converges to. So to find confidence intervals, for example, we need to write out the log-likelihood function for one observation</strong> <script type="math/tex">l(\theta)</script>, <strong>find the Fisher information</strong> <script type="math/tex">I(\theta)</script> <strong>at</strong> <script type="math/tex">\theta ^ *</script>,  <strong>and find it’s inverse. Intuitively, the more “information” I have, the tighter the variance in my estimator.</strong></p>
  </li>
</ul>

<p><strong>Method of moments</strong></p>

<ul>
  <li>
    <p>Assume that I have some statiscal model <script type="math/tex">(E, \mathbb{P} _ {\theta, \theta \in \Theta})</script> and <script type="math/tex">\Theta \subset \mathbb{R}^d</script>, for some $d \geq 1$$. Then I define the moment of this distribution as being</p>

    <script type="math/tex; mode=display">\begin{align}
      m_k(\theta) = \mathbf{E}[X_1^k]
  \end{align}</script>

    <p>Now the law of large numbers tells us that (for the method of moments, we only consider <script type="math/tex">k = 1, \ldots, d</script>)</p>

    <script type="math/tex; mode=display">\begin{align}
      \hat{m} _ k = \dfrac{1}{n}\sum\limits _ 1 ^ n X_i ^ k \xrightarrow[n \to \infty]{\mathbf{P}, \mathrm{a.s.}} m_k(\theta)
  \end{align}</script>

    <p>where <script type="math/tex">\hat{m} _ k</script> are called the emperical moments. More compactaly, we can say that the LLN tells us</p>

    <script type="math/tex; mode=display">\begin{align}
      (\hat{m} _ 1, \ldots, \hat{m} _ d \xrightarrow[n \to \infty]{\mathbf{P}, \mathrm{a.s.}} (\hat{m} _ 1(\theta), \ldots, m _ d(\theta))
  \end{align}</script>

    <p>Now we define a function <script type="math/tex">M: \mathbb{R} ^ d \mapsto \mathbb{R} ^ d</script>, where it takes the <script type="math/tex">d</script> dimensional vector <script type="math/tex">\theta</script> and maps it to the vector of its first <script type="math/tex">d</script> moments, i.e.,</p>

    <script type="math/tex; mode=display">\begin{align}
      M(\theta) = (m_1(\theta), \ldots, m_d(\theta))
  \end{align}</script>

    <p>For example, for a Gaussian, <script type="math/tex">M(\mu, \sigma) = (m_1(\mu, \sigma), m_2(\mu, \sigma)) = (\mu, \mu^2 + \sigma^2)</script>. 
  Assuming that <script type="math/tex">M</script> is one to one, <script type="math/tex">\theta = M^{-1}(m_1(\theta), \ldots, m_d(\theta))</script>. Then, by definition, the moments estimator of $\theta$$ is given by</p>

    <script type="math/tex; mode=display">\begin{align}
      \hat{\theta}_n^{MM} = M^{-1}(\hat{m}_1, \ldots, \hat{m}_1)
  \end{align}</script>

    <p>provided it exits.</p>
  </li>
  <li>
    <p>Note that choosing the first $d$ moments is somewhat arbitrary. The goal here is to integrate out the values of the random variables (specifically through expectations because then we can replace the expectation with a sample mean by the LLN) so that I have $d$ different equations for the $d$ parameters. So in general if <script type="math/tex">\theta \in \mathbb{R}^d</script>, we only need to define the expectation vector to be <script type="math/tex">M(\theta) = (g_1(\theta), \ldots, g_d(\theta))</script> and then our estimator would consist of the vector that calculates the sample mean of the values of the function. It is just so happens that the $k$-th moment is often convenient.</p>
  </li>
  <li>
    <p>The choice of the functions being the <script type="math/tex">k</script>-th moments is somewhat arbitrary. We are really looking for a set of functions that gives us a set of <script type="math/tex">d</script> equations that we can use to solve for each of the components of <script type="math/tex">\theta \in \mathbb{R}^d</script>. More generally, we could have written our a function</p>

    <script type="math/tex; mode=display">\begin{align}
      M(\theta) = (g_1(X), \ldots, g_d(X))
  \end{align}</script>

    <p>and then use our hammer of replacing expected values with sample means to obtain an estimator of the form</p>

    <script type="math/tex; mode=display">\begin{align}
      \hat{M} = (\frac{1}{n}\sum_i g_1(X_i), \ldots, \frac{1}{n}\sum_i g_d(X_i))
  \end{align}</script>

    <p>Now, we know from an application of the central limit theorem that</p>

    <script type="math/tex; mode=display">\begin{align}
      \sqrt{n}(\hat{M} - M(\theta)) \xrightarrow[n \to \infty]{(d)} N_d(0, \Sigma)
  \end{align}</script>

    <p>where <script type="math/tex">\Sigma = \mathbb{C}\mathrm{ov}((g_1(X_1)), \ldots, g_d(X_1)^T)</script> is the covariance matrix. We can now apply the multivariate delta method to find the actual estimator. We are interested in findging <script type="math/tex">M^{-1}(M(\theta))</script>, which means we need to use the delta method. Therefore, we have after using the delta method that</p>

    <script type="math/tex; mode=display">\begin{align}
      \sqrt{n}(M^{-1}(\hat{M}) - M^{-1}(M(\theta))) \xrightarrow[n \to \infty]{(d)} N_d(0, (\nabla M^{-1}(M(\theta))^T\Sigma \nabla M^{-1}(M(\theta))
  \end{align}</script>
  </li>
</ul>

<h3 id="lecture-12-m-estimation">Lecture 12: M-Estimation</h3>

<ul>
  <li>
    <p>M-Estimation is very similar to log-likelihood in that it generalizes the idea behind the log-likelihood. The idea here is that let’s say we have a function <script type="math/tex">\rho(X_1, \mu)</script> and I want to find the value of $\mu$ that maximizes (or minimizes, depending on my goal and the specfic nature of my function <script type="math/tex">\rho</script>) the quantity <script type="math/tex">\mathbf{E}[ \rho(X_1,\mu)]</script>. That is, we want to find a function <script type="math/tex">\rho: E \times \mathcal{M} \mapsto \mathbb{R}</script>, where <script type="math/tex">\mathcal{M}</script> is the set of possible values of the unknown parameter <script type="math/tex">\mu^*</script> such that</p>

    <script type="math/tex; mode=display">\begin{align}
      \mu = \underset{\mu \in \mathbb{R}^d}{\mathrm{argmin}}\mathbf{E}[\rho(X_1,\mu)]
  \end{align}</script>

    <p>achieves its minimum at <script type="math/tex">\mu = \mu^*</script>.</p>

    <p>Note that we need to make no modeling assumption that the data is drawn from some family of distributions <script type="math/tex">\mathbb{P}</script>.</p>

    <p>In general, the goal is to find the <strong>loss function</strong> <script type="math/tex">\rho(X,\mu)</script> such that <script type="math/tex">\mathcal{Q}(\mu) = \mathbf{E}[\rho(X,\mu)]</script> attains a minimum at <script type="math/tex">\mu = \mu^ *</script>. Because <script type="math/tex">\mathcal{Q}</script> is an expectation, we can proceed by replacing this expectation with the sample mean of <script type="math/tex">\mathcal{Q}(\mu)</script>. The goal is to find a function <script type="math/tex">\rho(X, \mu)</script> such that the expectation of that function is minimized at the parameter of interest <script type="math/tex">\mu^ *</script>.</p>
  </li>
  <li>
    <p><strong>Theorem</strong>: Assume that <script type="math/tex">(E, {\mathbb{P} _ \theta)} _ {\theta \in \Theta})</script> is a statistical model associated with the data.  Let <script type="math/tex">\mathbb{M} = \Theta</script> and <script type="math/tex">\rho(x, \theta)  = -L_1(x, \theta</script>, provided the log likelihood is positive everywhere. Then <script type="math/tex">\mu^* = \theta^*</script>, that is <script type="math/tex">\theta^*</script> is the true value of the parameter. The goal is to find a function $$\rho(X, \musuch that the expectation of that function is minimized at the parameter of interest.</p>
  </li>
  <li>
    <p>As a special case of M-estimation, we can think about a median <script type="math/tex">\mathrm{med}(X)</script> which is any number such that if <script type="math/tex">X</script> is a discrete random variable,</p>

    <script type="math/tex; mode=display">\begin{align}
      \mathbf{P}(X > \mathrm{med}(X)) = \mathbf{P}(X > \mathrm{med}(X)) = 1/2
  \end{align}</script>

    <p>It turns out that if I pick the loss function <script type="math/tex">\rho(X,\mu) = \lvert X - \mu \rvert</script>, then</p>

    <script type="math/tex; mode=display">\begin{align}
      \textrm{argmin}_{\mu \in \mathcal{M}} \mathbf{E}[\lvert X - \mu \rvert]
  \end{align}</script>

    <p>is the median of <script type="math/tex">X</script>.</p>
  </li>
  <li>
    <p>Note: The expectation of a convex function is convex. This can be proved by an application of the mean value theorem (i.e. any chord lies above the curve for a convex function).</p>
  </li>
  <li>
    <p>The strategy in M-estimation follows similar lines as in maximum likelihood estimation. There are three steps:</p>

    <ol>
      <li>Define a function <script type="math/tex">\rho(X, \mu)</script> and hence the expectation of that function <script type="math/tex">Q(\mu) = \mathbf{E}[\rho(X,\mu)]</script>.</li>
      <li>Replace that expectation with a sample mean (allowed from the law of large numbers) and define your estimator</li>
      <li>The estimator is the argmin of this expectation.</li>
    </ol>
  </li>
  <li>
    <p>Define</p>

    <script type="math/tex; mode=display">\begin{align}
      \hat{\mu} _ n = \underset{\mu \in \mathcal{M}}{\mathrm{argmin}} \dfrac{1}{n}\sum\limits _ {i=1}^n \rho(X_i, \mu)
  \end{align}</script>

    <p>where <script type="math/tex">\mathcal{M}</script> is the set of all possible value the true parameter of interest <script type="math/tex">\mu^*</script> can take. <script type="math/tex">\rho</script> is chosen according to what we want. Some examples include the emperical mean, emperical median, MLE,  emperical quantiles, etc.</p>

    <p>Let</p>

    <script type="math/tex; mode=display">\begin{align}
      J(\mu) = \mathbb{E}[\dfrac{\partial^2\rho(X_1, \mu)}{\partial \mu \partial \mu^T}]
  \end{align}</script>

    <p>Let</p>

    <script type="math/tex; mode=display">\begin{align}
      K(\mu) = \textrm(Cov)\left( \dfrac{\partial \rho(X_1, \mu)}{\partial \mu} \right)
  \end{align}</script>

    <p>Let <script type="math/tex">\mu^ * \in \mathcal{M}</script> be the true parameter and assume the following:</p>

    <ol>
      <li><script type="math/tex">\mu^*</script> is the only minimizer of the function <script type="math/tex">\mathcal{Q}</script></li>
      <li><script type="math/tex">J(\mu)</script> is invertible for all <script type="math/tex">\mu \in \mathcal{M}</script>.</li>
      <li>A few more technical conditons</li>
    </ol>

    <p>Then , <script type="math/tex">\hat{\mu}_n</script> satisfies</p>

    <ol>
      <li>$\hat{\mu} _ n \xrightarrow[n \to \infty]{\mathbb{P}} \mu^* $</li>
      <li>$\sqrt{n}(\hat{\mu}_n - \mu^ * ) \xrightarrow[n \to \infty]{(d)} N(0, (J(\mu^ * ))^{-1} K(\mu^* )(J(\mu^ * ))^{-1}) $</li>
    </ol>

    <p>Note that M-estimation is a minimiization problem. Therefore, to reduce this to the log-likelihood case, <script type="math/tex">\rho = -l(\theta)</script>. This sign is very important and has confused you before because we have so far looked at maximizing the log-likelihood.</p>
  </li>
</ul>

<h2 id="unit-4-hypothesis-testing">Unit 4: Hypothesis Testing</h2>

<p>Here we are looking at some more specific cases of hypothesis testing where the sample size is small and the central limit theorem does not hold. We look at some specific tests designed especially for small sample sizes. What if I was given that my data is Gaussian, but I don’t know the variance, and that the sample size is small?</p>

<p>We also look at example of non-parametric tests, for example, goodness of fit tests. Is my assumption on the distribution a good assumption? Presumably this would be very widely useful.</p>

<h3 id="lecture-13-chi-squared-distribution-t-test">Lecture 13: Chi squared distribution, T-test</h3>

<ul>
  <li>
    <p>To understand the case where I have only a small number of samples but I am given that the data is Gaussian, let us consider the case of a clinical trial. We want to determine if a particular drug decreases the LDL cholestrol. In practice, we generally take two groups of people: a test group who gets the drug and a control group who get a placebo. Let <script type="math/tex">\Delta_d</script> be the decrease in the LDL level in the test group, and let <script type="math/tex">\Delta_c</script> be the decrease in the control group. Let <script type="math/tex">X_1, \ldots, X_n \sim \mathcal{N}(\Delta_d, \sigma_d^2)</script> and  <script type="math/tex">Y_1, \ldots, Y_m \sim \mathcal{N}(\Delta_c, \sigma_c^2)</script>, that is, we are given that the data is Gaussian (which is very often the case). Note that typically <script type="math/tex">n>m</script> because we want to give more people at getting a chance at receving a potential drug.</p>
  </li>
  <li>
    <p>Under these assumptions, we can now define the null hypothesis <script type="math/tex">H_0</script> and the alternative hypothesis <script type="math/tex">H_1</script>.</p>

    <script type="math/tex; mode=display">\begin{align}
      H_0: \Delta_d = \Delta_c, H_1: \Delta_d > \Delta_c
  \end{align}</script>

    <p>Now, because we are given that the two distributions are Gaussian, we know that</p>

    <script type="math/tex; mode=display">\begin{align}
      \bar{X}_n \sim \mathcal{N}\left(\Delta_d, \dfrac{\sigma_d^2}{n}\right) \\
      \bar{Y}_m \sim \mathcal{N}\left(\Delta_c, \dfrac{\sigma_c^2}{m}\right)
  \end{align}</script>

    <p>and hence</p>

    <script type="math/tex; mode=display">\begin{align}
      \dfrac{\bar{X}_n - \bar{Y}_m - (\Delta_d - \Delta_c)}{\sqrt{\sigma_d^2/n + \sigma_c^2/m}} \sim \mathcal{N}(0,1)
  \end{align}</script>

    <p>Note that because we were given that the data is Gaussian, there was no need to invoke the central limit theorem to show the normality of this random variable.</p>
  </li>
  <li>
    <p>Note that as before we might want to not have the unknown variance in the denominator and we want to use Slutsky to do plug-in. In this case, we have to think about the relative sizes of <script type="math/tex">m</script> and <script type="math/tex">n</script>. If <script type="math/tex">m</script> is 12 and <script type="math/tex">n</script> is 12000, then we really have to be careful and have a problem. In general, if we assume that <script type="math/tex">n = cm</script>, where <script type="math/tex">c</script> is a constant, we are typically OK.</p>
  </li>
  <li>
    <p>Consider now that I want to formulate a test statistic for this problem. The natural choice would be</p>

    <script type="math/tex; mode=display">\begin{align}
      R_\psi = \left\{\dfrac{\bar{X}_n - \bar{Y}_m - (\Delta_d - \Delta_c)}{\sqrt{\dfrac{\hat{\sigma^2_d}}{n} + \dfrac{\hat{\sigma^2_c}}{m}}} > q_\alpha \right\}
  \end{align}</script>

    <p>Note that this is one sided because of the choice of $H_0$: I have eliminated the case that <script type="math/tex">% <![CDATA[
\Delta_d < \Delta_c %]]></script>.</p>
  </li>
  <li>
    <p>Now to determine whether or not I accept or reject the null hypothesis based on the test statistic given above, I need to know what the variances are and therein lies the problem. The initial $X_i$ were iid <script type="math/tex">\mathcal{N}(\Delta_d, \sigma_d^2)</script> with unknown $\Delta_d$ and $\sigma_d^2$ (and likewise for the $Y_i$, so how do I go about making this decision?) This is where the student T test and the student T distribution enters into the picture. We first need to introduce various other definitions.</p>
  </li>
  <li>
    <p>The <script type="math/tex">\chi_d^2</script> distribution (with $d$ degrees of freedom). Let <script type="math/tex">Z_1,\ldots, Z_n</script> be iid <script type="math/tex">\mathcal{N}(0, 1)</script>. Further, let <script type="math/tex">\bar{X}_n</script> represent the sample mean and let</p>

    <script type="math/tex; mode=display">\begin{align}
      S_n = \dfrac{1}{n}\sum\limits_{i=1}^n (X_i - \bar{X}_ n )^2 = \dfrac{1}{n}\left(\sum\limits_{i=1}^n X_i^2\right) - (\bar{X}_n)^2 
  \end{align}</script>

    <p>denote a estimator (biased) for the variance. <strong>Conchran’s theorem</strong> then states that</p>

    <script type="math/tex; mode=display">\begin{align}
      \bar{X}_n \textrm{ is independent of } S_n \\
      \dfrac{nS_n}{\sigma^2} = \chi_{n-1}^2
  \end{align}</script>

    <p>The proof is a good exercise in understanding orthoginal matrices, so make sure you can recall and understand (see the recitation where this is derived)</p>
  </li>
  <li>
    <p>We also define the student t distribution, which is defined by the distribution of the random variable</p>

    <script type="math/tex; mode=display">\begin{align}
      \dfrac{Z}{\sqrt{V/n}}
  \end{align}</script>

    <p>where $Z \sim \mathcal{N}(0,1)$, $V \sim \chi^2_{n}$. Note that this distribution only depends on the degree of the <script type="math/tex">\chi^2</script> distribution. We can now exploit this fact to solve our initial problem: how do we design tests for normally distributed data when we have small sample sizes and we cannot apply Slutsky?</p>
  </li>
  <li>
    <p>The first step is to center and scale our estimators. consider the test statistic</p>

    <script type="math/tex; mode=display">\begin{align}
      T_n = \sqrt{n}\dfrac{\bar{X}_n - \mu}{\sigma}
  \end{align}</script>

    <p>We know that <script type="math/tex">T_n</script> is a standard Gaussian for all $n$. But we do not know <script type="math/tex">\sigma^2</script>, and we can’t use Slutsky to say that <script type="math/tex">\hat{\sigma^2} \to \sigma^2</script> because we don’t have a large enough sample size. So what we do is to consider the statistic above. Now we know that <script type="math/tex">S_n = (n-1)/n\tilde{S}_n</script>, where <script type="math/tex">\tilde{S}_n</script> is the unbiased estimator</p>

    <script type="math/tex; mode=display">\begin{align}
      \tilde{S} _ n = \dfrac{1}{n-1}\sum\limits_{i=1}^n (X_i - \bar{X}_ n )^2 
  \end{align}</script>

    <p>Therefore, we now do the following manipulations. Consider the estimator</p>

    <script type="math/tex; mode=display">% <![CDATA[
\begin{align}
      T_n & = \sqrt{n}\dfrac{\bar{X}_n - \mu}{\sqrt{\tilde{S}_n}} \\
          & = \sqrt{n}\dfrac{(\bar{X}_n - \mu)/\sigma}{\sqrt{\tilde{S}_n/\sigma^2}}\\
          & = \dfrac{Z}{\sqrt{\tilde{S}_n/\sigma^2}}
  \end{align} %]]></script>

    <p>where <script type="math/tex">Z</script> is a standard normal. Now, by Cochran’s theorem, we know that <script type="math/tex">nS_n/sigma^2 = (n-1)\tilde{S}_n\sigma^2 \sim \chi^2 _ {n-1}</script> and hence we ca substitute this in above to get</p>

    <script type="math/tex; mode=display">% <![CDATA[
\begin{align}
      T_n & = \dfrac{Z}{\sqrt{V/(n-1)}} \sim t_{n-1}
  \end{align} %]]></script>

    <p>where <script type="math/tex">V \sim \chi^2_{n-1}</script>. The distribution <script type="math/tex">t_{n-1}</script> is called the Student <script type="math/tex">t</script> distribution. This is a pivotal distribution (meaning that it doesn’t depend on any parameter of interest) and hence we can look up tables as a function of the degree and now derive a test statistic. Therefore, a good test at level alpha would be</p>

    <script type="math/tex; mode=display">\begin{align}
      \psi_\alpha = \mathbb{1}\left\{ T_n > q_\alpha \right\}
  \end{align}</script>

    <p>or</p>

    <p><script type="math/tex">\begin{align}
      \psi_\alpha = \mathbb{1}\left\{ \lvert T_n \rvert > q _ {\alpha/2} \right\}
  \end{align}</script> 
  depending on whether I have a one-sided test or two-sided test respectively. Here $q_\alpha$ is the <script type="math/tex">1 - \alpha</script> quantile of the student t distribution (not Gaussian of course). Rememeber that <script type="math/tex">q_\alpha</script>, the <script type="math/tex">1 - \alpha</script> quantile, defined as that number such that <script type="math/tex">\mathbf{P}(T > q_\alpha) = \alpha</script>, where <script type="math/tex">T</script> is a random variable described by the pdf of the Student t distribution.</p>

    <p>The student t distribution is a bit heavier tailed than the Gaussian. This makes sense because we know less about the distribution. We don’t know the variance. However for large enough $n$ (usually <script type="math/tex">n > 40</script>), we can use Slutsky to show that $t_n \sim Z$, a standard normal.</p>
  </li>
  <li>
    <p>Now what about the two sample test we started with earlier. We could deisgn the problem as having an estimator that is a vector:</p>

    <script type="math/tex; mode=display">\begin{align}
      \hat{\theta} = \left( 
                          \begin{matrix}
                              \bar{X}_n \\
                              \bar{Y}_n
                          \end{matrix}
                     \right)
  \end{align}</script>

    <p>and the applying the multivariate delta method (using the function <script type="math/tex">g(x, y) = x - y</script>, and the modifying the covariance matrix as <script type="math/tex">(\nabla g(\theta_0))^T \Sigma (\nabla g(\theta_0))</script> and proceeding. <script type="math/tex">\Sigma</script> would be diagonal because <script type="math/tex">X_i</script> and <script type="math/tex">Y_i</script> are independent. This would be the formal, foolproof way. But we can also proceed by just noting that because because <script type="math/tex">X_i</script> and <script type="math/tex">Y_i</script> are independent, we know by observation that <script type="math/tex">\textrm{var}(\bar{X}_n - \bar{Y}_n) = \sigma_d^2/m + \sigma_c^2/m</script> and hence by replacing the <script type="math/tex">\sigma^2</script> with the unbiased estimator <script type="math/tex">\hat{\sigma^2} = \tilde{S}_n</script>, we know that</p>

    <script type="math/tex; mode=display">\begin{align}
      T_n = \dfrac{\bar{X}_n - \bar{Y}_m - (\Delta_d - \Delta_c)}{\sqrt{ \hat{\sigma_d^2}/m + \hat{\sigma_c^2}/m }} \sim t_N
  \end{align}</script>

    <p>and to be conservative we would choose <script type="math/tex">N = \min(n,m)</script>. But there exists a more accurate formula called the Welch-Satterthwaite formula, which says that we choose</p>

    <script type="math/tex; mode=display">\begin{align}
      N = \dfrac{\sqrt{ \hat{\sigma_d^2}/m + \hat{\sigma_c^2}/m }}{ \hat{\sigma_d^4}/(n^2(n-1)) + \hat{\sigma_c^4}/(m^2(m-1)) }
  \end{align}</script>
  </li>
  <li>
    <p>Computing <script type="math/tex">p</script> values for a statistic that is distributed according to a <script type="math/tex">p</script> value is the same procedure. We calculate the value of the test statistic (think carefully whether we want the absolute value signs, which direction should inequalitites go - this is most intuitive. In general, we want a test to reject only if some it is large enough, after allowing for some error), and then the <script type="math/tex">p</script> value becomes</p>

    <script type="math/tex; mode=display">\begin{align}
      p = \mathbf{P}(t_N > T_N)
  \end{align}</script>

    <p>i.e. what is the probability that a Student t distributed random variable can take on values larger than the calculated test statistic.</p>
  </li>
  <li>
    <p>An advantage of using the Student t test is that it is a one size fits all in the sense that, for small sample sizes, (and for normally distributed data), is the correct test to use, but for large sample sizes, we should be looking up CLT tables, but, Student t converges to CLT anyway. So no need to look up two different tables.</p>
  </li>
  <li>
    <p><strong>Wald’s Test</strong>: This is a slight modification to the MLE estimator. Consider again the MLE estimator, under the hypotheses <script type="math/tex">H_0: \theta = \theta_0, H_1: \theta \neq \theta_0</script>. Then, we know that</p>

    <script type="math/tex; mode=display">\begin{align}
      \sqrt{n}(I(\theta_0))^{1/2}(\hat{\theta}^{MLE} - \theta_0) \xrightarrow[n \to \infty]{(d)} \mathcal{N}_d (0, I _ d)
  \end{align}</script>

    <p>where <script type="math/tex">I(\theta_0) = I(\theta^*) = I(\hat{\theta}^{MLE})</script> is the Fisher information. That last replacement comes from one of the assumptions of Wald’s test. According to the test, we first take the square norm (or the L2 norm) on both sides</p>

    <script type="math/tex; mode=display">\left\Vert \sqrt{n}(I(\theta_0))^{1/2}(\hat{\theta}^{MLE} - \theta_0) \right\Vert \xrightarrow[n \to \infty]{(d)} \left\Vert\mathcal{N}_d (0, I _ d) \right\Vert</script>

    <p>Now, we can expand that LHS using the fact that <script type="math/tex">\Vert (\cdot) \Vert = (\cdot) ^T (\cdot)</script> and that the fisher information is a symmetric matrix (and hence <script type="math/tex">(\cdot)^T = (\cdot)^{-1}</script>) to obtain</p>

    <script type="math/tex; mode=display">\begin{align}
      n\left(  (\hat{\theta} - \theta_0)^T I(\hat{\theta}) (\hat{\theta} - \theta_0) \right) \xrightarrow[n \to \infty]{(d)} \chi_d^2
  \end{align}</script>

    <p>where <script type="math/tex">d</script> is the length of <script type="math/tex">\theta</script> (or <script type="math/tex">\hat{\theta}</script>). We can now use the LHS as a test statistic <script type="math/tex">T_n</script> and now define quantiles for the chi-squared distribution to get tests at a desired level.</p>
  </li>
  <li>
    <p>The geometric interpretation of Wald’s test is that we have a vector for the estimator and vector for the true parameter (or the parameter in the hypothesis). The length of the vector joining these two vectors is a random variable, and the length is distributed according to the Chi sqaured distribution. The test then defines a level around the probability that the length of this difference vector exceeds a certain length (because it is chi squared distributed).</p>
  </li>
  <li>
    <p>We can show that for 1D, the quantiles for the Wald’s test is related to the quantiles of the standard Gaussian through <script type="math/tex">q_\alpha(\chi^2) = (q_{\alpha/2}(\mathcal{N}(0,1)))^2</script>.</p>
  </li>
  <li>
    <p><strong>Constrained Maximum Likelihood estimator</strong>: Suppose that my hypothesis has the form</p>

    <script type="math/tex; mode=display">% <![CDATA[
\begin{align}
      H_0: (\theta^* _ {r+1}, \ldots, \theta^* _ {d}) & = (\theta^{(o)} _ {r+1}, \ldots, \theta^{(o)} _ {d}) \\
      H_1:  (\theta^* _ {r+1}, \ldots, \theta^* _ {d}) & \neq (\theta^{(o)} _ {r+1}, \ldots, \theta^{(o)} _ {d})
  \end{align} %]]></script>

    <p>then <script type="math/tex">\Theta_0</script>, the region of the null hypothesis, is</p>

    <p><script type="math/tex">\begin{align}
      \Theta_0:= \left\{  \mathbf{v} \in \mathbb{R}^d: ( v_{r+1}, \ldots, v_{d} ) = (\theta_{r+1}^{(o)}, \ldots, \theta_{d}^{(o)})  \right\}
  \end{align}</script>
  where <script type="math/tex">\theta_{r+1}^{(o)}, \ldots, \theta_{d}^{(o)}</script> are all known. In other words, we are saying that we want to find the maximum under the constraint that those values of my estimator are fixed. Now, under the likelihood estimator test, the test statistic is</p>

    <p><script type="math/tex">\begin{align}
      T_n = 2\left(l_n\left(\hat{\theta}_n^{MLE}\right) - l_n\left(\hat{\theta}_n^c\right)\right)
  \end{align}</script>
  where <script type="math/tex">l_n</script> is the maximum likelihood estimator. The estimator <script type="math/tex">\hat{\theta}_n^c</script> is the constrained MLE estimator and is defined as</p>

    <script type="math/tex; mode=display">\begin{align}
      \hat{\theta}_n^c = \underset{\theta \in \Theta _ 0}{\textrm{argmax}} l_n(X_1, \ldots, X_n; \theta)
  \end{align}</script>

    <p>Note that this estimator is always positive, and this is easily seen. Now, <strong>Wilk’s theorem</strong> states that if <script type="math/tex">H_0</script> is actually true, then</p>

    <script type="math/tex; mode=display">\begin{align}
      T_n \xrightarrow[n \to \infty]{(d)} \chi_{d-r}^2
  \end{align}</script>

    <p>and hence a test at level <script type="math/tex">\alpha</script> is <script type="math/tex">\psi_\alpha = \mathbb{1}\left\{  T_n > q_\alpha \right\}</script>, and I look up the <script type="math/tex">\chi^2_{d-r}</script> quantile tables for hypothesis testing.</p>
  </li>
  <li>
    <p><strong>Implicit Hypothesis Testing:</strong> Consider that you are unable to test hypotheses of the form <script type="math/tex">\theta = \theta_0</script>, can you are only able to say if <script type="math/tex">g(\theta) = 0</script> or not. You don’t have access to <script type="math/tex">\theta_0</script> but only some function of <script type="math/tex">\theta_0</script>. In this case, we can leverage the multivariate delta method and then use Wald’s test to do the hypothesis testing. The formalism is as follows. Consider the estimator <script type="math/tex">\hat{\theta}</script>, we know that (for as yet unknwon <script type="math/tex">\theta_0</script>)</p>

    <script type="math/tex; mode=display">\begin{align}
      \sqrt{n}(\hat{\theta} - \theta_0) \xrightarrow[n \to \infty]{(d)} \mathcal{N}(0, \Sigma(\theta))
  \end{align}</script>

    <p>then by the delta method we know that</p>

    <script type="math/tex; mode=display">\begin{align}
      \sqrt{n}(\Gamma(\theta))^{-1/2}(g(\hat{\theta}) - g(\theta_0)) \xrightarrow[n \to \infty]{(d)} \mathcal{N}(0, I_d)
  \end{align}</script>

    <p>so long as the conditions for the delta method are met, and if <script type="math/tex">\Gamma(\theta) = (\nabla g(\theta))^T \Sigma(\theta) (\nabla g(\theta))</script> is invertible. Note that in general <script type="math/tex">g: \mathbb{R}^d \mapsto \mathbb{R}^k</script>. Now, we can apply Wald’s test, and if <script type="math/tex">g(\theta_0)=0</script>, we have</p>

    <script type="math/tex; mode=display">\begin{align}
      n(g(\hat{\theta})^T\Gamma(\hat{\theta})(g(\hat{\theta}) \xrightarrow[n \to \infty]{(d)} \chi^2_{k}
  \end{align}</script>

    <p>this is where the advantage of Wald’s test using <script type="math/tex">\hat{\theta}</script> in the covariance matrix comes through, because I don’t really known anythng about <script type="math/tex">\theta_0</script>; all is know is something about <script type="math/tex">g(\theta_0)</script>. In the asymptotic limit this is allowed by Slutsky. Now we can use the LHS as the estimator and design a statistical test by looking at quantile tables of the <script type="math/tex">\chi^2_k</script> distribution.</p>
  </li>
</ul>

<h3 id="lecture-12-goodness-of-fit-tests-for-discrete-distributions">Lecture 12: Goodness of fit tests for discrete distributions</h3>

<ul>
  <li>
    <p>Goodness of fit tests falls under the class of non-parametric hypothesis testing. We are interested in knowing if the data follows a certain distribution or not. For example, let’s say I have data on race from a county, and then I have data for jurors from the county. The census data gives me a PMF for the race. Now, if I collect data on the races of the jurors and find the proportion of each race, does this fit the PMF obtained from census data?</p>
  </li>
  <li>
    <p>We begin by introducing some formalism. Consider that I have a sample space <script type="math/tex">E = {a_1, \ldots, a_k}</script> and statistical model <script type="math/tex">E, \mathbf{P} _ {\mathbf{p},  p \in \Delta_k}</script>. We define <script type="math/tex">\Delta_k</script> by</p>

    <p><script type="math/tex">\begin{align}
      \Delta_k = \left\{  \mathbf{p} = (p_1, \ldots, p_k) \in (0,1)^K: \sum\limits_{i=1}^n p_i = 1 \right\}
  \end{align}</script>
  If <script type="math/tex">\mathbf{p} \in \Delta_k</script> and <script type="math/tex">X \sim \mathbf{P} _ {\mathbf{p}}</script>, then</p>

    <script type="math/tex; mode=display">\begin{align}
      \mathbf{P} _ \mathbf{p}(X = a_j) = p_j
  \end{align}</script>

    <p>$Delta_k$ is known as the probability simplex in <script type="math/tex">\mathbf{R}^k</script>. A more compact notation for this is to say that <script type="math/tex">Delta_k</script> is the set of all vectors <script type="math/tex">\mathbf{p}</script> such that <script type="math/tex">\mathbf{p} \cdot \mathbf{1} = \mathbf{p}^T \mathbf{1} = 1</script>, <script type="math/tex">p_i \geq 0</script>.</p>
  </li>
  <li>
    <p>Let <script type="math/tex">x_1, \ldots, X_n \sim \mathbb{P} _ \mathbf{p}</script> for some unknown <script type="math/tex">\mathbf{p} \in \Delta _ k</script>, and the <script type="math/tex">\mathbf{p} ^ 0 \in \Delta _ k</script> be fixed. The goodness of fit test wants to test</p>

    <p><script type="math/tex">\begin{align}
      H_0: \mathbf{p} = \mathbf{p} ^ 0, H_1: \mathbf{p} \neq p ^ 0
  \end{align}</script>
  with asymptotic level <script type="math/tex">\alpha \in (0,1)</script>. For example, if <script type="math/tex">\mathbf{p} ^ 0 = (1/K, 1/K, \ldots, 1/K)</script> we are testing to see if the data follows a uniform distribution.</p>
  </li>
  <li>
    <p>The setup above is identical to a multinomial distribution. The likelihood of the model can be written as</p>

    <script type="math/tex; mode=display">\begin{align}
      L_n(X_1, \ldots, X_n; \mathbf{p}) = p_1^{N_1}p_2^{N_2}\cdots p_K^{N_k}
  \end{align}</script>

    <p>where <script type="math/tex">N_j</script> is the number of times any of the $X$ takes the value <script type="math/tex">a_j</script>. This is called the multinomial distribution with <script type="math/tex">K</script> modalities. It model <script type="math/tex">n</script> trials with <script type="math/tex">K</script> possible outcomes on each trial. We can model the number of instances of each trial using a random vector <script type="math/tex">N</script> such that <script type="math/tex">\sum\limits_{i=1}^K N ^ {(i)} = n</script>, the total number of trials, with <script type="math/tex">n ^ {(i)} \geq 0</script>. Then the pmf of the multinomial distribution is given by</p>

    <script type="math/tex; mode=display">\begin{align}
      p _ N \left(N ^ {(1)} = n ^ {(i)}, N ^ {(K)} = n ^ {(K)}\right) = \dfrac{n!}{ n ^ {(1)}! n ^ {(2)}! \cdots n ^ {(K)}! }
  \end{align}</script>

    <p>The likelihood function is fairly straightforward in this case, and is given above.</p>
  </li>
  <li>
    <p>With an application of constrained optimization, we can show that <script type="math/tex">\hat{p_j} = N_j/N</script>.</p>
  </li>
</ul>

<h3 id="lecture-12-goodness-of-fit-tests-for-discrete-distributions-1">Lecture 12: Goodness of fit tests for discrete distributions</h3>

<ul>
  <li>
    <p>The <script type="math/tex">\chi^2</script> test: this test is used to determine goodness of fit tests for the case of a discrete distribution with a pmf, such as the Bernoulli distribution, multinomial distribution, binomial distribution, etc. For the multinomial distribution, the test setup is as follows: Assume that I am testing the hypothesis</p>

    <script type="math/tex; mode=display">\begin{align}
      H_0: \mathbf{p} = \mathbf{p^0} \\
      H_0: \mathbf{p} \neq \mathbf{p^0}
  \end{align}</script>

    <p>Then assuming that <script type="math/tex">H_0</script> is true, <script type="math/tex">\sqrt{n}(\mathbf{\hat{p}} - \mathbf{p^0})</script> is asymptotically normal, and the following theorem holds</p>

    <script type="math/tex; mode=display">\begin{align}
      n\sum\limits_{j=1}^{K}\dfrac{\left( \mathbf{\hat{p}_j} - \mathbf{p^0_j} \right)^2}{\mathbf{p^0_j}} \xrightarrow[n \to \infty]{(d)} \chi^2 _ {K-1}
  \end{align}</script>

    <p>There are some subtleties here because of the constraint that all the individual probabilities need to sum to 1. The fisher information matrix now becomes non invertible, so a lot of the previous results based on the fact that the covariance matrix is the inverse Fisher information now fails to hold. In fact, the fact, if we consider the quantity <script type="math/tex">(\mathbf{\hat{p}} - \mathbf{p^0})^T\mathbf{1}</script>, where <script type="math/tex">\mathbf{1}</script> is the all ones vector, this dot product turns out to be precisely zero. There is no variance in this unit vector direction and hence the fisher information matrix becomes non invertible. Another way to think about this is that the asymptotic Gaussian vector lies only in <script type="math/tex">K-1</script> dimensions because of the added dependency, and hence the degree of the <script type="math/tex">\chi^2</script> distribution.</p>
  </li>
  <li>
    <p>A more general version of the <script type="math/tex">\chi^2</script> distribution for a discrete distribution. To test if some distribution <script type="math/tex">\mathbf{P}</script> is described some family of discrete distributions <script type="math/tex">\{\mathbf{P}\} _ {\theta \in \Theta \subset \mathbb{R^d}}</script>, where <script type="math/tex">\Theta \subset \mathbb{R}^d</script> is a $d$ - dimensional vector with support <script type="math/tex">\{0, 1, \ldots, K \}</script> and pmf <script type="math/tex">f_\theta</script>, i.e., to test the hypothesis</p>

    <script type="math/tex; mode=display">\begin{align}
      H_0: \mathbf{P} \in \{ \mathbf{P} _ \theta\} _ {\theta \in \Theta} \\
      H_1: \mathbf{P} \notin \{ \mathbf{P} _ \theta\} _ {\theta \in \Theta} \\
  \end{align}</script>

    <p>we define the test statistic to be</p>

    <script type="math/tex; mode=display">\begin{align}
      T_n:=n\sum\limits_{j=0}^K \dfrac{\left(\frac{N_j}{n} - f_{\hat{\theta}}(j)\right)^2}{f_{\hat{\theta}}(j)} \xrightarrow[n \to \infty]{(d)} \chi_{(K+1) -d - 1}^2
  \end{align}</script>

    <p>Note that <script type="math/tex">K+1</script> is the support size of <script type="math/tex">\mathbb{P} _ {\theta \in \Theta}</script> with <script type="math/tex">\Theta \subset \mathbb{R}^d</script>, i.e., the null hypothesis <script type="math/tex">H_0</script> holds, and if in addition some technical conditions hold, then the above test statistic allows us to define tests on the pivotal distribution. Essentially we are using proportions to define this test because in the discrete case, the MLE estimator for a parameter turns out to be the proportion of samples that was observed for each case.</p>
  </li>
</ul>

<h3 id="goodness-of-fit-tests-for-continuous-distributions">Goodness of fit tests for continuous distributions</h3>

<ul>
  <li>
    <p>The CDF of a probability distribution can be written as <script type="math/tex">F(t) = \mathbf{P}(X \leq t) = \mathbf{E}[\mathbf{1}\{ X\leq t \}]</script>. Because I see the expected value, I can now use my hammer of replacing expectations with sample means and hence I have an estimator for the CDF:</p>

    <script type="math/tex; mode=display">\begin{align}
      F_n(t) = \dfrac{1}{n}\sum\limits_{i=1}^n \mathbf{1} \{ X_i \leq t \}
  \end{align}</script>

    <p>This quantity is called the empirical CDF or sample CDF. It just finds the proportion of outcomes that are less than or equal to <script type="math/tex">t</script>.</p>
  </li>
  <li>
    <p><strong>The fundamental theorem of statistics (Glivenko-Cantelli theorem)</strong>:</p>

    <script type="math/tex; mode=display">\begin{align}
      \underset{t \in \mathbb{R}}{\sup} \lvert F_n(t) - F(t) \rvert \xrightarrow[n \to \infty]{\textrm{a.s.}} 0
  \end{align}</script>

    <p>The subtlety here is that Glivenko-Cantelli tells us that the convergence is uniform as opposed to the convergence being pointwise. The latter is a situation where given <script type="math/tex">t</script>, we let <script type="math/tex">n \to \infty</script>, while for uniform convergence, we pick the worst <script type="math/tex">t</script> and then let <script type="math/tex">n \to \infty</script>.</p>
  </li>
  <li>
    <p>The advantage of defining the empirical CDF is that because it is an average, the central limit theorem applies and hence</p>

    <script type="math/tex; mode=display">\begin{align}
      \sqrt{n}(F_n(t) - F(t)) \xrightarrow[n \to \infty]{(d)}\mathcal{N}(0, F(t)(1-F(t)))
  \end{align}</script>

    <p>The variance is easy to calculate because each term is a Bernoulli, and the parameter of that Bernoulli is <script type="math/tex">\mathbf{P}(X_i \leq t) = F(t)</script>.</p>
  </li>
  <li>
    <p><strong>Donsker’s Theorem</strong></p>

    <p>The theorem states that</p>

    <script type="math/tex; mode=display">\begin{align}
          \sqrt{n} \underset{t \in \mathbb{R}}{\sup}{\lvert F_n(t) - F(t)\rvert} \xrightarrow[n \to \infty]{(d)} \underset{0 \leq t \leq 1}{\sup}\lvert \mathbb{B}(t) \rvert
  \end{align}</script>

    <p>Here, the quantity <script type="math/tex">\mathbb{B}(t)</script> is called the Brownian bridge. This is a distribution that simulates at a random walk between 0 and 1 under the constraint that the two ends are pinned at 0. Each step is itself a standard Gaussian. The Kolmogorov-Smirnov test is a natural extension of this theorem because the Brownian bridge is a pivotal distribution.</p>
  </li>
  <li>
    <p><strong>Kolmogorov-Smirnov test</strong></p>

    <p>Assume that we want to test the null hypothesis <script type="math/tex">H_0: F = F^0</script> against the alternative hypothesis <script type="math/tex">H_1: F \neq F^0</script>. Define the test statistic based on the theorem above</p>

    <script type="math/tex; mode=display">\begin{align}
      T_n := \sqrt{n} \underset{t \in \mathbb{R}}{\sup}{\lvert F_n(t) - F^0(t)\rvert}
  \end{align}</script>

    <p>then we know from Donsker’s theorem that</p>

    <script type="math/tex; mode=display">\begin{align}
      T_n \xrightarrow[n \to \infty]{(d)} Z
  \end{align}</script>

    <p>where <script type="math/tex">Z</script> has a known distribution, i.e., the supremum of the absolute value of a Brownian bridge. Now, for the KS test with asymptotic level <script type="math/tex">\alpha</script>, we define</p>

    <script type="math/tex; mode=display">\begin{align}
      \delta _ \alpha ^ {KS} = \mathbf{1} \{ T_n > q _ {\alpha} \}
  \end{align}</script>

    <p>where <script type="math/tex">q_\alpha</script> is the <script type="math/tex">1-\alpha</script> quantile of the supremum of the absolute value of a Brownian bridge. The p-value can then be calculated as <script type="math/tex">\mathbf{P}(Z > T_n \vert T_n)</script>, where <script type="math/tex">Z</script> is a random variable described by the supremum of the absolute value of a Brownian bridge.</p>
  </li>
  <li>
    <p>How do I actually calculate this supremum in practice? Let us say we have a set of observations <script type="math/tex">X_1, \ldots, X_n</script>. We reason that given the step wise nature of the empirical CDF, we should see the largest discrepancy between <script type="math/tex">F_n(t)</script> and <script type="math/tex">F(t)</script> at the points of observation. Therefore, the first thing we do is to order all the observation points in ascending order. Such ordered statistics are notated by <script type="math/tex">X _ {(1)}, \ldots, X _ {(n)}</script>. We can then simplify the calculation of the supremum as</p>

    <script type="math/tex; mode=display">\begin{align}
      T_n = \sqrt{n}\underset{i = 1, \ldots, n}{\max} \left( \max\left( \left\lvert \dfrac{i-1}{n} - F^0(X _ {(i)}) \right\rvert, \left\lvert \dfrac{i}{n} - F^0(X _ {(i)}) \right\rvert \right) \right)
  \end{align}</script>

    <p>where there are a total of <script type="math/tex">n</script> points. Note that we have used the fact that the value of the empirical CDF at <script type="math/tex">X _ {(i)}</script> just counts the proportion of points less than or equal to <script type="math/tex">X _ {(i)}</script>.</p>
  </li>
  <li>
    <p>There still remains the issue of how I actually go about calculating the supremum of the absolute value of the Brownian bridge. There is something remarkable that happens here that allows us to calculate this distribution in a non-asymptotic manner. Let <script type="math/tex">X _ 1, \ldots, X _ n</script> de distributed with CDF <script type="math/tex">F</script>. Define a new random variable <script type="math/tex">Y _i = F(X _ i)</script>. Then the CDF of <script type="math/tex">Y _ i</script> is given by</p>

    <script type="math/tex; mode=display">% <![CDATA[
\begin{align}
      F _ {Y _ i} (t) & = \mathbf{P}(Y _ i \leq t) \\
                      & = \mathbf{P}(F(X _ i) \leq t) \\
                      & = \mathbf{P}(X _ i \leq F ^ {-1} (t))\\
                      & = F(F ^ {-1} (t)) = t
  \end{align} %]]></script>

    <p>Note that <script type="math/tex">t \in [0,1]</script> because <script type="math/tex">Y _ i</script> is a CDF. This means that the CDF of <script type="math/tex">Y _ i</script> is the same as a uniform random variable and <script type="math/tex">Y _ i = F(X_i) \sim \textrm{Unif}([0,1])</script>. Therefore, no matter what the actual CDF of <script type="math/tex">X _ i</script> is, I can rewrite in non asymptotic form that</p>

    <script type="math/tex; mode=display">% <![CDATA[
\begin{align}
      T_n & := \sqrt{n} \underset{t \in \mathbb{R}}{\sup}{\lvert F_n(t) - F^0(t)\rvert} \\
          & = \sqrt{n}\underset{0 \leq x \leq 1}{\sup} \left\lvert G_n(x) - x \right\rvert \\
          & = \sqrt{n}\underset{0 \leq x \leq 1}{\sup} \left\lvert \dfrac{1}{n} \sum\limits_{i=1}^n \mathbf{1}\{ X_i \leq x \} - x \right\rvert
  \end{align} %]]></script>

    <p>where <script type="math/tex">G_n(x)</script> is the empirical CDF of a uniform distribution. Therefore, to actually calculate the supremum of the absolute value of the Brownian bridge, I first discretize my domain of <script type="math/tex">0 \leq x \leq 1</script> into a fine mesh, and then for each one of those $x_i$, I simulate <script type="math/tex">n</script> uniform random variables and find <script type="math/tex">T_n ^ l</script>. I then simulate <script type="math/tex">T_n</script> like this <script type="math/tex">M</script> times, where <script type="math/tex">M</script> is very large. I then know that <script type="math/tex">T_n ^ l \to T_n</script> as <script type="math/tex">M</script> grows large, and I use this value as the supremum of the Brownian bridge.</p>

    <p>In any case, there are tables that I can look up to find quantiles of the Kolmogorov-Smirnov test (i.e. for the supremum of the absolute value of a Brownian bridge), so I don’t have to calculate this thing each time, but this is just a useful technique to generate samples of any given distribution starting from being able to generate samples for a uniform distribution.</p>

    <p>Here is some python code below that generates the distribution for the supremum of the absolute value of the Brownian bridge using a uniform distribution:</p>

    <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code>  <span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="n">np</span>
  <span class="kn">from</span> <span class="nn">scipy</span> <span class="kn">import</span> <span class="n">stats</span>
  <span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="n">plt</span>

  <span class="c"># Basic constants</span>
  <span class="n">x</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1000</span><span class="p">)</span>
  <span class="n">n</span> <span class="o">=</span> <span class="mi">1000</span>

  <span class="k">def</span> <span class="nf">statistic_calculator</span><span class="p">(</span><span class="n">x</span><span class="p">):</span>
      <span class="s">"""
      Given x, this function generates n iid copies of uniform 
      random variables, and then calculates the value in the abs
       signs in equation 1.
      input: x
      returns: value of quantity in absolute value in equation 1
      """</span>
      <span class="c"># generate n rvs</span>
      <span class="n">nrvs</span> <span class="o">=</span> <span class="n">stats</span><span class="o">.</span><span class="n">uniform</span><span class="o">.</span><span class="n">rvs</span><span class="p">(</span><span class="n">loc</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">scale</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">size</span><span class="o">=</span><span class="n">n</span><span class="p">)</span>
      <span class="n">emp_cdf</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="nb">sum</span><span class="p">((</span><span class="n">nrvs</span> <span class="o">&lt;</span> <span class="n">x</span><span class="p">)</span><span class="o">*</span><span class="mi">1</span><span class="p">)</span> <span class="o">/</span> <span class="n">n</span>
      <span class="k">return</span> <span class="nb">abs</span><span class="p">(</span><span class="n">emp_cdf</span> <span class="o">-</span> <span class="n">x</span><span class="p">)</span>
    
  <span class="c"># vectorize the function</span>
  <span class="n">vec_statistic_calculator</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">vectorize</span><span class="p">(</span><span class="n">statistic_calculator</span><span class="p">)</span>

  <span class="k">def</span> <span class="nf">tlcalculator</span><span class="p">():</span>
      <span class="s">"""
      Does the optimization to find the supremum over the domain 
      0 &lt;= x &lt;= 1
      """</span>
      <span class="k">return</span> <span class="n">np</span><span class="o">.</span><span class="nb">max</span><span class="p">(</span><span class="n">vec_statistic_calculator</span><span class="p">(</span><span class="n">x</span><span class="p">))</span>
    
  <span class="c"># Do this a whole bunch of times for large M to find the </span>
  <span class="c"># distribution</span>
  <span class="n">M</span> <span class="o">=</span> <span class="mi">22000</span>
  <span class="n">m_all</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="n">tlcalculator</span><span class="p">()</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">M</span><span class="p">)])</span>
  <span class="n">plt</span><span class="o">.</span><span class="n">hist</span><span class="p">(</span><span class="n">m_all</span><span class="p">,</span> <span class="n">bins</span><span class="o">=</span><span class="mi">100</span><span class="p">);</span>
</code></pre></div>    </div>
  </li>
  <li>
    <p><strong>Kolmogorov-Lilliefors test</strong>: The Kolmogorov-Smirnov test is useful in the case that I am given a certain Gaussian distribution of known parameters, and I would like to test if my data agrees with the hypothesis that it is distributed according to this Gaussian. I can the for example use the student t-test to proceed further if the goodness of fit test determines this to be true (i.e. I fail to reject). However, what about the case where I do not know what the parameters of the Gaussian distribution is? It would be natural to assume that I can use estimators</p>

    <script type="math/tex; mode=display">% <![CDATA[
\begin{align}
      \hat{\mu} & = \bar{X} _ n \\
      \hat{\sigma^2} & = S _ n
  \end{align} %]]></script>

    <p>But by doing this, I am already making my data look more Gaussian because I am using the data to check something about the data. I am estimating the unknown parameters based on the data and this is dangerous. In this case, Donsker’s theorem does not apply! Instead I have to correct for the fact that I am less likely to reject the hypothesis by decreasing the values of the quantiles $q _ \alpha$ (compared to the Kolmogorov-Smirnov test). These tables exist.</p>

    <script type="math/tex; mode=display">\begin{align}
      \sqrt{n}\underset{t \in \mathbb{R}}{\sup} \left\lvert F _ n(t) - \Phi _ {\hat{\mu}, \hat{\sigma^2}}(t) \right\rvert \xrightarrow[n \to \infty]{(d)} Z
  \end{align}</script>

    <p>where <script type="math/tex">Z</script> is a random variable whose distribution is given by the Kolmogorov-Lilliefors tables.</p>
  </li>
  <li>
    <p>Quantile-Quantile plots are an informal but visual way to test goodness of fits. It’s often hard to draw two CDFs and then check to see if they look close. So we flip it by plotting quantiles (i.e. the inverse of the CDF) against each other. If the two distributions being compared are exactly the same, we would expect a line that is parallel to <script type="math/tex">y=x</script>. In reality, we will see deviations from this line to different extents, either on the left axis, or on the right axis or both. More formally, we are plotting the points</p>

    <script type="math/tex; mode=display">\begin{align}
      \left( F ^ {-1} (i/n), F _ n ^{-1}(i/n) \right)
  \end{align}</script>

    <p>The inverse CDF is somewhat confusing to think about. In essence, what we need to find for the empirical CDFis the <script type="math/tex">i</script>-th ordered observation <script type="math/tex">X _ {(i)}</script>. For the continuous CDF, we are interested in finding that value of <script type="math/tex">x _ i</script> such that <script type="math/tex">F(x _ i) = i/n</script>. One can intuit how this quantifies the distance between the empirical CDF and the true CDF. The Q-Q plot needs to be interpreted very carefully. For example, we know that the student distribution has heavier tails compared to the Gaussian distribution. So we would expect the quantiles for the student t on the right to be larger (i.e. more to the right) than that of the Gaussian distribution. Similarly, the quantiles on the left would be more to the left than those of the Gaussian. However, if I don’t have enough data, I would be very tempted to conclude that the student distribution is in fact Gaussian because I wouldn’t have enough data to see these small deviations at the tails. Try and think about what you would expect the deviations on the QQ plots to be for some standard distributions such  as uniform, exponential, etc. compared to the standard normal.</p>
  </li>
</ul>

<h2 id="unit-5-bayesian-statistics">Unit 5: Bayesian Statistics</h2>

<h3 id="lecture-17-introduction-to-bayesian-statistics">Lecture 17: Introduction to Bayesian Statistics</h3>

<p>This unit doesn’t have that much new in terms of Bayesian thinking over the probability class so just see those notes. Below are just some brief notes on new material.</p>

<ul>
  <li>
    <p>Remind yourself of frequentist and Bayesian approaches based on the notes in the probability class.</p>
  </li>
  <li>
    <p>When we have a Bernoulli parameter to estimate, we often use the Beta distribution (with some chosen values for <script type="math/tex">a, b</script>) as a flexible example of the prior.</p>
  </li>
  <li>
    <p>A prior is called a conjugate prior if the posterior probability falls in the same family as the prior itself. For example, a Gaussian prior and data that is distributed as a Gaussian, or a Gamma prior and data that is distributed through a Gamma distribution, etc.</p>
  </li>
  <li>
    <p>An improper prior is one that does not integrate to 1. For example, assuming a uniform prior when the data is distributed as a Gaussian. It of course doesn’t matter because depending on the data, the posterior can still be proper and well-behaved.</p>
  </li>
  <li>
    <p><strong>Jeffrey’s prior</strong>: This is an important new thing learned in this class. The idea is that given the nature of the distribution of the observation <script type="math/tex">X_1, \ldots, X_n</script>, we can find the Fisher information matrix <script type="math/tex">I(\theta)</script> of the probability model (assuming that it exists). Jeffrey’s prior is then defined as</p>
  </li>
</ul>

<script type="math/tex; mode=display">\begin{align}
    \pi_J(\theta) \propto \sqrt{ \det{I(\theta)} }
\end{align}</script>

<ul>
  <li>The idea behind the Jeffrey’s prior is quite interesting. We know that the Fisher information gives us some information about how much we know about the distribution i.e. the variance of the distribution is the inverse of the Fisher information. The Fisher information gives us the curvature of the likelihood function (can you reason why?). So by making the Jeffrey’s prior proportional to the Fisher information, we, are giving weight to those points where we know a lot about the distribution (lots of information and low variance)</li>
</ul>

<h2 id="unit-6-linear-regression">Unit 6: Linear Regression</h2>

<h2 id="unit-7-generalized-linear-models">Unit 7: Generalized Linear Models</h2>

<h3 id="lecture-21-introduction-exponential-families">Lecture 21: Introduction, Exponential Families</h3>

        
      </section>

      <footer class="page__meta">
        
        


        
          <p class="page__date"><strong><i class="fas fa-fw fa-calendar-alt" aria-hidden="true"></i> Updated:</strong> <time datetime="2019-02-01T00:00:00-04:00">February 01, 2019</time></p>
        
      </footer>

      

      
  <nav class="pagination">
    
      <a href="/coursenotes/python_notes/" class="pagination--pager" title="Miscellaneous notes: python, numpy, pandas, sklearn
">Previous</a>
    
    
      <a href="#" class="pagination--pager disabled">Next</a>
    
  </nav>

    </div>

    
  </article>

  
  
</div>

    </div>

    
      <div class="search-content">
        <div class="search-content__inner-wrap"><input type="text" id="search" class="search-input" tabindex="-1" placeholder="Enter your search term..." />
    <div id="results" class="results"></div></div>

      </div>
    

    <div class="page__footer">
      <footer>
        <!-- start custom footer snippets -->

<!-- end custom footer snippets -->
        <div class="page__footer-follow">
  <ul class="social-icons">
    
      <li><strong>Follow:</strong></li>
    

    
      
        
      
    

    <li><a href="/feed.xml"><i class="fas fa-fw fa-rss-square" aria-hidden="true"></i> Feed</a></li>
  </ul>
</div>

<div class="page__footer-copyright">&copy; 2019 Aditya Jaishankar. Powered by <a href="https://jekyllrb.com" rel="nofollow">Jekyll</a> &amp; <a href="https://mademistakes.com/work/minimal-mistakes-jekyll-theme/" rel="nofollow">Minimal Mistakes</a>.</div>

      </footer>
    </div>

    
  <script src="/assets/js/main.min.js"></script>
  <script defer src="https://use.fontawesome.com/releases/v5.6.0/js/all.js" integrity="sha384-z9ZOvGHHo21RqN5De4rfJMoAxYpaVoiYhuJXPyVmSs8yn20IE3PmBM534CffwSJI" crossorigin="anonymous"></script>




<script src="/assets/js/lunr/lunr.min.js"></script>
<script src="/assets/js/lunr/lunr-store.js"></script>
<script src="/assets/js/lunr/lunr-en.js"></script>








<script type="text/javascript"
        src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.0/MathJax.js?config=TeX-AMS_CHTML"></script>

<script type="text/x-mathjax-config">
MathJax.Hub.Config({
tex2jax: {
inlineMath: [['$','$'], ['\\(','\\)']],
processEscapes: true},
jax: ["input/TeX","input/MathML","input/AsciiMath","output/CommonHTML"],
extensions: ["tex2jax.js","mml2jax.js","asciimath2jax.js","MathMenu.js","MathZoom.js","AssistiveMML.js", "[Contrib]/a11y/accessibility-menu.js"],
TeX: {
extensions: ["AMSmath.js","AMSsymbols.js","noErrors.js","noUndefined.js"],
equationNumbers: {
autoNumber: "AMS"
}
}
});
</script>


  </body>
</html>
